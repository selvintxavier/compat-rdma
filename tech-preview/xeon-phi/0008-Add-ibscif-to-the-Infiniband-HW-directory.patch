diff -ruN a/drivers/infiniband/hw/scif/ibscif_ah.c b/drivers/infiniband/hw/scif/ibscif_ah.c
--- a/drivers/infiniband/hw/scif/ibscif_ah.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_ah.c	2016-04-14 13:33:08.858411346 -0700
@@ -0,0 +1,50 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+struct ib_ah *ibscif_create_ah(struct ib_pd *ibpd, struct ib_ah_attr *attr)
+{
+	struct ibscif_ah *ah;
+
+	ah = kzalloc(sizeof *ah, GFP_KERNEL);
+	if (!ah) 
+		return ERR_PTR(-ENOMEM);
+
+	ah->dlid = cpu_to_be16(attr->dlid);
+
+	return &ah->ibah;
+}
+
+int ibscif_destroy_ah(struct ib_ah *ibah)
+{
+	kfree(to_ah(ibah));
+	return 0;
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_cm.c b/drivers/infiniband/hw/scif/ibscif_cm.c
--- a/drivers/infiniband/hw/scif/ibscif_cm.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_cm.c	2016-04-14 13:33:08.858411346 -0700
@@ -0,0 +1,515 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+static LIST_HEAD(listen_list);
+DEFINE_SPINLOCK(listen_list_lock);
+
+static int sockaddr_in_to_node_id( struct sockaddr_in addr )
+{
+	u8 *p = (u8 *)&addr.sin_addr.s_addr;
+
+	if (p[0]==192 && p[1]==0 && p[2]==2 && p[3]>=100 && p[3]<100+IBSCIF_MAX_DEVICES)
+		return (int)(p[3]-100);
+
+	else
+		return -EINVAL;
+}
+
+static struct sockaddr_in node_id_to_sockaddr_in( int node_id )
+{
+	struct sockaddr_in addr;
+	u8 *p = (u8 *)&addr.sin_addr.s_addr;
+
+	addr.sin_family = AF_INET;
+	addr.sin_addr.s_addr = 0;
+	addr.sin_port = 0;
+
+	p[0] = 192;
+	p[1] = 0;
+	p[2] = 2;
+	p[3] = 100 + node_id;
+
+	return addr;
+}
+
+void free_cm(struct kref *kref)
+{
+	struct ibscif_cm *cm_ctx;
+	cm_ctx = container_of(kref, struct ibscif_cm, kref);
+	if (cm_ctx->conn)
+		ibscif_put_conn(cm_ctx->conn);
+	kfree(cm_ctx);
+}
+
+static inline void get_cm(struct ibscif_cm *cm_ctx)
+{        
+        kref_get(&cm_ctx->kref);
+}
+ 
+static inline void put_cm(struct ibscif_cm *cm_ctx)
+{        
+        kref_put(&cm_ctx->kref, free_cm);
+}
+
+void free_listen(struct kref *kref)
+{
+	struct ibscif_listen *listen;
+	listen = container_of(kref, struct ibscif_listen, kref);
+	kfree(listen);
+}
+
+static inline void get_listen(struct ibscif_listen *listen)
+{        
+        kref_get(&listen->kref);
+}
+ 
+static inline void put_listen(struct ibscif_listen *listen)
+{        
+        kref_put(&listen->kref, free_listen);
+}
+
+static int connect_qp(struct ibscif_cm *cm_ctx)
+{
+        struct ibscif_qp *qp;
+        struct ib_qp_attr qp_attr;
+        int qp_attr_mask;
+        int err;
+
+        qp = ibscif_get_qp(cm_ctx->qpn);
+        if (IS_ERR(qp)) {
+                printk(KERN_ERR PFX "%s: invalid QP number: %d\n", __func__, cm_ctx->qpn);
+                return -EINVAL; 
+        }
+         
+        qp_attr_mask =  IB_QP_STATE |
+                        IB_QP_AV |  
+                        IB_QP_DEST_QPN |
+                        IB_QP_ACCESS_FLAGS |
+                        IB_QP_MAX_QP_RD_ATOMIC |
+                        IB_QP_MAX_DEST_RD_ATOMIC;
+ 
+        qp_attr.ah_attr.ah_flags = 0;
+        qp_attr.ah_attr.dlid = IBSCIF_NODE_ID_TO_LID(cm_ctx->remote_node_id);
+        qp_attr.dest_qp_num = cm_ctx->remote_qpn;
+        qp_attr.qp_state = IB_QPS_RTS;
+        qp_attr.qp_access_flags = IB_ACCESS_LOCAL_WRITE |
+                                  IB_ACCESS_REMOTE_WRITE |
+                                  IB_ACCESS_REMOTE_READ |
+                                  IB_ACCESS_REMOTE_ATOMIC;
+        qp_attr.max_rd_atomic = 16;     /* 8-bit value, don't use MAX_OR */
+        qp_attr.max_dest_rd_atomic = 16;/* 8-bit value, don't use MAX_IR */
+ 
+        err = ib_modify_qp(&qp->ibqp, &qp_attr, qp_attr_mask);
+ 
+        if (!err) {
+                qp->cm_context = cm_ctx;
+		get_cm(cm_ctx);
+	}
+ 
+        ibscif_put_qp(qp);
+ 
+        return err;
+}
+
+static void event_connection_close(struct ibscif_cm *cm_ctx)
+{
+        struct iw_cm_event event;
+ 
+        memset(&event, 0, sizeof(event));
+        event.event = IW_CM_EVENT_CLOSE;
+        event.status = -ECONNRESET;
+        if (cm_ctx->cm_id) {
+                cm_ctx->cm_id->event_handler(cm_ctx->cm_id, &event);
+                cm_ctx->cm_id->rem_ref(cm_ctx->cm_id);
+                cm_ctx->cm_id = NULL;
+        }
+}
+
+static void event_connection_reply(struct ibscif_cm *cm_ctx, int status)
+{
+        struct iw_cm_event event;
+ 
+        memset(&event, 0, sizeof(event));
+        event.event             = IW_CM_EVENT_CONNECT_REPLY;
+        event.status            = status;
+        event.local_addr        = *(struct sockaddr_storage *) &cm_ctx->local_addr;
+        event.remote_addr       = *(struct sockaddr_storage *) &cm_ctx->remote_addr;
+
+        if ((status == 0) || (status == -ECONNREFUSED)) {
+                event.private_data_len = cm_ctx->plen;
+                event.private_data = cm_ctx->pdata;
+        }
+        if (cm_ctx->cm_id) {
+                cm_ctx->cm_id->event_handler(cm_ctx->cm_id, &event);
+		if (status == -ECONNREFUSED) {
+			cm_ctx->cm_id->rem_ref(cm_ctx->cm_id);
+			cm_ctx->cm_id = NULL;
+		}
+	}
+}
+
+static void event_connection_request(struct ibscif_cm *cm_ctx)
+{
+        struct iw_cm_event event;
+ 
+        memset(&event, 0, sizeof(event));
+        event.event             = IW_CM_EVENT_CONNECT_REQUEST;
+        event.local_addr        = *(struct sockaddr_storage *) &cm_ctx->local_addr;
+        event.remote_addr       = *(struct sockaddr_storage *) &cm_ctx->remote_addr;
+        event.private_data_len  = cm_ctx->plen;
+        event.private_data      = cm_ctx->pdata;
+        event.provider_data     = cm_ctx;
+	event.ird = 16;
+	event.ord = 16;
+
+        if (cm_ctx->listen) {
+                cm_ctx->listen->cm_id->event_handler( cm_ctx->listen->cm_id, &event);
+		put_listen(cm_ctx->listen);
+		cm_ctx->listen = NULL;
+        }
+}
+
+static void event_connection_established( struct ibscif_cm *cm_ctx )
+{
+        struct iw_cm_event event;
+ 
+        memset(&event, 0, sizeof(event));
+        event.event = IW_CM_EVENT_ESTABLISHED;
+	event.ird = 16;
+	event.ord = 16;
+        if (cm_ctx->cm_id) {
+                cm_ctx->cm_id->event_handler(cm_ctx->cm_id, &event);
+        }
+}
+
+void ibscif_cm_async_callback(void *cm_context)
+{
+        struct ibscif_cm *cm_ctx = cm_context;
+ 
+        if (cm_ctx) {
+                event_connection_close(cm_ctx);
+                put_cm(cm_ctx);
+        }
+}
+
+int ibscif_cm_connect(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param)
+{
+	struct ibscif_cm *cm_ctx;
+	struct sockaddr_in *local_addr = (struct sockaddr_in *) &cm_id->local_addr;
+	struct sockaddr_in *remote_addr = (struct sockaddr_in *) &cm_id->remote_addr;
+	int node_id;
+	int remote_node_id;
+	int err = 0;
+
+	cm_ctx = kzalloc(sizeof *cm_ctx, GFP_KERNEL);
+	if (!cm_ctx) {
+		printk(KERN_ALERT PFX "%s: cannot allocate cm_ctx\n", __func__);
+		return -ENOMEM;
+	}
+
+	kref_init(&cm_ctx->kref); /* refcnt <- 1 */
+	spin_lock_init(&cm_ctx->lock);
+
+	node_id = sockaddr_in_to_node_id(*local_addr);
+	remote_node_id = sockaddr_in_to_node_id(*remote_addr);
+	if (node_id<0 || remote_node_id<0) {
+		printk(KERN_ALERT PFX "%s: invalid address, local_addr=%8x, remote_addr=%8x, node_id=%d, remote_node_id=%d\n",
+				__func__, local_addr->sin_addr.s_addr, remote_addr->sin_addr.s_addr,
+				node_id, remote_node_id);
+		err = -EINVAL;
+		goto out_free;
+	}
+
+	cm_ctx->conn = ibscif_get_conn( node_id, remote_node_id, 0 );
+	if (!cm_ctx->conn) {
+		printk(KERN_ALERT PFX "%s: failed to get connection %d-->%d\n", __func__, node_id, remote_node_id);
+		err = -EINVAL;
+		goto out_free;
+	}
+
+	cm_id->add_ref(cm_id);
+	cm_id->provider_data = cm_ctx;
+
+	cm_ctx->cm_id = cm_id;
+	cm_ctx->node_id = node_id;
+	cm_ctx->remote_node_id = remote_node_id;
+	cm_ctx->local_addr = *local_addr;
+	cm_ctx->remote_addr = *remote_addr;
+	cm_ctx->qpn = conn_param->qpn;
+	cm_ctx->plen = conn_param->private_data_len;
+	if (cm_ctx->plen > IBSCIF_MAX_PDATA_SIZE) {
+		printk(KERN_ALERT PFX "%s: plen (%d) exceeds the limit (%d), truncated.\n",
+				__func__, cm_ctx->plen, IBSCIF_MAX_PDATA_SIZE);
+		cm_ctx->plen = IBSCIF_MAX_PDATA_SIZE;
+	}
+	if (cm_ctx->plen)
+		memcpy(cm_ctx->pdata, conn_param->private_data, cm_ctx->plen);
+
+	err = ibscif_send_cm_req( cm_ctx );
+
+	return err;
+
+out_free:
+	kfree(cm_ctx);
+	return err;
+}
+
+int ibscif_cm_accept(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param)
+{
+	struct ibscif_cm *cm_ctx = cm_id->provider_data;
+	int err = 0;
+
+	cm_id->add_ref(cm_id);
+	cm_ctx->cm_id = cm_id;
+	cm_ctx->qpn = conn_param->qpn;
+	cm_ctx->plen = conn_param->private_data_len;
+	if (cm_ctx->plen > IBSCIF_MAX_PDATA_SIZE) {
+		printk(KERN_ALERT PFX "%s: plen (%d) exceeds the limit (%d), truncated.\n",
+				__func__, cm_ctx->plen, IBSCIF_MAX_PDATA_SIZE);
+		cm_ctx->plen = IBSCIF_MAX_PDATA_SIZE;
+	}
+	if (cm_ctx->plen)
+		memcpy(cm_ctx->pdata, conn_param->private_data, cm_ctx->plen);
+
+	err = connect_qp( cm_ctx );
+	if (err) {
+		printk(KERN_ALERT PFX "%s: failed to modify QP into connected state\n", __func__);
+		goto err_out;
+	}
+
+	err = ibscif_send_cm_rep( cm_ctx );
+	if (err) {
+		printk(KERN_ALERT PFX "%s: failed to send REP\n", __func__);
+		goto err_out;
+	}
+
+	return 0;
+
+err_out:
+	cm_id->rem_ref(cm_id);
+	cm_ctx->cm_id = NULL;
+	put_cm(cm_ctx);
+	return err;
+}
+
+int ibscif_cm_reject(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len)
+{
+	struct ibscif_cm *cm_ctx = cm_id->provider_data;
+	int err = 0;
+
+	err = ibscif_send_cm_rej( cm_ctx, pdata, pdata_len );
+
+	put_cm(cm_ctx);
+	return err;
+}
+
+int ibscif_cm_create_listen(struct iw_cm_id *cm_id, int backlog)
+{
+	struct ibscif_listen *listen;
+	struct sockaddr_in *local_addr = (struct sockaddr_in *) &cm_id->local_addr;
+
+	listen = kzalloc(sizeof *listen, GFP_KERNEL);
+	if (!listen) {
+		printk(KERN_ALERT PFX "%s: cannot allocate listen object\n", __func__);
+		return -ENOMEM;
+	}
+	
+	kref_init(&listen->kref); /* refcnt <- 1 */
+
+	listen->cm_id = cm_id;
+	listen->port = local_addr->sin_port;
+	cm_id->provider_data = listen;
+	cm_id->add_ref(cm_id);
+
+	spin_lock_bh(&listen_list_lock);
+	list_add(&listen->entry, &listen_list);
+	spin_unlock_bh(&listen_list_lock);
+
+	return 0;
+}
+
+int ibscif_cm_destroy_listen(struct iw_cm_id *cm_id)
+{
+	struct ibscif_listen *listen = cm_id->provider_data;
+
+	spin_lock_bh(&listen_list_lock);
+	list_del(&listen->entry);
+	spin_unlock_bh(&listen_list_lock);
+	cm_id->rem_ref(cm_id);
+	put_listen(listen);
+
+	return 0;
+}
+
+/* similar to ibscif_get_qp(), but differs in:
+ * (1) use the "irqsave" version of the lock functions to avoid the
+ *     kernel warnings about "local_bh_enable_ip";
+ * (2) don't hold the reference on success;
+ * (3) return NULL instead of error code on failure.
+ */
+struct ib_qp *ibscif_cm_get_qp(struct ib_device *ibdev, int qpn)
+{
+	struct ibscif_qp *qp;
+	unsigned long flags;
+
+	read_lock_irqsave(&wiremap_lock, flags);
+	qp = idr_find(&wiremap, qpn);
+	if (likely(qp) && unlikely(qp->magic != QP_MAGIC))
+		qp = NULL;
+	read_unlock_irqrestore(&wiremap_lock,flags);
+
+	return qp ? &qp->ibqp : NULL;
+}
+
+void ibscif_cm_add_ref(struct ib_qp *ibqp)
+{
+	struct ibscif_qp *qp;
+
+	if (likely(ibqp)) {
+		qp = to_qp(ibqp);
+		kref_get(&qp->ref);
+	}
+}
+
+void ibscif_cm_rem_ref(struct ib_qp *ibqp)
+{
+	struct ibscif_qp *qp;
+
+	if (likely(ibqp)) {
+		qp = to_qp(ibqp);
+		ibscif_put_qp(qp);
+	}
+}
+
+int ibscif_process_cm_skb(struct sk_buff *skb, struct ibscif_conn *conn)
+{
+	union ibscif_pdu *pdu = (union ibscif_pdu *)skb->data;
+	struct ibscif_cm *cm_ctx;
+	struct ibscif_listen *listen;
+	int cmd, qpn, status, plen, err, port;
+	u64 req_ctx, rep_ctx;
+
+	req_ctx	= __be64_to_cpu(pdu->cm.req_ctx);
+	rep_ctx	= __be64_to_cpu(pdu->cm.rep_ctx);
+	cmd	= __be32_to_cpu(pdu->cm.cmd);
+	port	= __be32_to_cpu(pdu->cm.port);
+	qpn	= __be32_to_cpu(pdu->cm.qpn);
+	status	= __be32_to_cpu(pdu->cm.status);
+	plen	= __be32_to_cpu(pdu->cm.plen);
+
+	switch (cmd) {
+	  case IBSCIF_CM_REQ:
+		cm_ctx = kzalloc(sizeof *cm_ctx, GFP_KERNEL);
+		if (!cm_ctx) {
+			printk(KERN_ALERT PFX "%s: cannot allocate cm_ctx\n", __func__);
+			return -ENOMEM;
+		}
+		kref_init(&cm_ctx->kref); /* refcnt <- 1 */
+		spin_lock_init(&cm_ctx->lock);
+
+		spin_lock_bh(&listen_list_lock);
+		list_for_each_entry(listen, &listen_list, entry) {
+			if (listen->port == port) {
+				cm_ctx->listen = listen;
+				get_listen(listen);
+			}
+		}
+		spin_unlock_bh(&listen_list_lock);
+
+		if (!cm_ctx->listen) {
+			printk(KERN_ALERT PFX "%s: no matching listener for connection request, port=%d\n", __func__, port);
+			put_cm(cm_ctx);
+			/* fix me: send CM_REJ */
+			return -EINVAL;
+		}
+
+		cm_ctx->cm_id = NULL;
+		cm_ctx->node_id = conn->dev->node_id;
+		cm_ctx->remote_node_id = conn->remote_node_id;
+		cm_ctx->local_addr = node_id_to_sockaddr_in(cm_ctx->node_id);
+		if (cm_ctx->listen)
+			cm_ctx->local_addr.sin_port = cm_ctx->listen->port;
+		cm_ctx->remote_addr = node_id_to_sockaddr_in(cm_ctx->remote_node_id);
+		cm_ctx->remote_qpn = qpn;
+		cm_ctx->plen = plen;
+		if (cm_ctx->plen > IBSCIF_MAX_PDATA_SIZE) {
+			printk(KERN_ALERT PFX "%s: plen (%d) exceeds the limit (%d), truncated.\n",
+					__func__, cm_ctx->plen, IBSCIF_MAX_PDATA_SIZE);
+			cm_ctx->plen = IBSCIF_MAX_PDATA_SIZE;
+		}
+		if (cm_ctx->plen)
+			memcpy(cm_ctx->pdata, pdu->cm.pdata, cm_ctx->plen);
+
+		cm_ctx->peer_context = req_ctx;
+		cm_ctx->conn = conn;
+		atomic_inc(&conn->refcnt);
+
+		event_connection_request(cm_ctx);
+		break;
+
+	  case IBSCIF_CM_REP:
+		cm_ctx = (struct ibscif_cm *)req_ctx;
+		cm_ctx->plen = plen;
+		memcpy(cm_ctx->pdata, pdu->cm.pdata, plen);
+		cm_ctx->remote_qpn = qpn;
+		cm_ctx->peer_context = rep_ctx;
+		err = connect_qp( cm_ctx );
+		if (!err) 
+			err = ibscif_send_cm_rtu(cm_ctx);
+		if (err)
+			printk(KERN_ALERT PFX "%s: failed to modify QP into connected state\n", __func__);
+		event_connection_reply(cm_ctx, err);
+		put_cm(cm_ctx);
+		break;
+
+	  case IBSCIF_CM_REJ:
+		cm_ctx = (struct ibscif_cm *)req_ctx;
+		cm_ctx->plen = plen;
+		memcpy(cm_ctx->pdata, pdu->cm.pdata, plen);
+		event_connection_reply(cm_ctx, status);
+		put_cm(cm_ctx);
+		break;
+
+	  case IBSCIF_CM_RTU:
+		cm_ctx = (struct ibscif_cm *)rep_ctx;
+		event_connection_established( cm_ctx );
+		put_cm(cm_ctx);
+		break;
+
+	  default:
+		printk(KERN_ALERT PFX "%s: invalid CM cmd: %d\n", __func__, pdu->cm.cmd);
+		break;
+	}
+
+	return 0;
+}
+
diff -ruN a/drivers/infiniband/hw/scif/ibscif_cq.c b/drivers/infiniband/hw/scif/ibscif_cq.c
--- a/drivers/infiniband/hw/scif/ibscif_cq.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_cq.c	2016-04-14 13:33:08.859411305 -0700
@@ -0,0 +1,313 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+static void ibscif_cq_tasklet(unsigned long cq_ptr)
+{
+	struct ibscif_cq *cq = (struct ibscif_cq *)cq_ptr;
+	cq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);
+}
+
+#ifdef	MOFED
+struct ib_cq *ibscif_create_cq(struct ib_device *ibdev, struct ib_cq_init_attr *attr,
+			      struct ib_ucontext *context, struct ib_udata *udata)
+#else
+struct ib_cq *ibscif_create_cq(struct ib_device *ibdev, int entries, int comp_vector,
+			      struct ib_ucontext *context, struct ib_udata *udata)
+#endif
+{
+	struct ibscif_dev *dev = to_dev(ibdev);
+	struct ibscif_cq *cq;
+	int nbytes, npages;
+	int err;
+#ifdef	MOFED
+	int entries = attr->cqe;
+#endif
+
+	if (entries < 1 || entries > MAX_CQ_SIZE)
+		return ERR_PTR(-EINVAL);
+
+	if (!atomic_add_unless(&dev->cq_cnt, 1, MAX_CQS))
+		return ERR_PTR(-EAGAIN);
+
+	cq = kzalloc(sizeof *cq, GFP_KERNEL);
+	if (!cq) {
+		atomic_dec(&dev->cq_cnt);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	spin_lock_init(&cq->lock);
+	tasklet_init(&cq->tasklet, ibscif_cq_tasklet, (unsigned long)cq);
+	cq->state = CQ_READY;
+
+	nbytes = PAGE_ALIGN(entries * sizeof *cq->wc);
+	npages = nbytes >> PAGE_SHIFT;
+
+	err = ibscif_reserve_quota(&npages);
+	if (err)
+		goto out;
+
+	cq->wc = vzalloc(nbytes); /* Consider using vmalloc_user */
+	if (!cq->wc) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	cq->ibcq.cqe = nbytes / sizeof *cq->wc;
+
+	return &cq->ibcq;
+out:
+	ibscif_destroy_cq(&cq->ibcq);
+	return ERR_PTR(err);
+}
+
+int ibscif_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata)
+{
+	struct ibscif_cq *cq = to_cq(ibcq);
+	struct ibscif_wc *old_wc, *new_wc;
+	int nbytes, old_npages, new_npages, i, err;
+
+	if (cqe < 1 || cqe > MAX_CQ_SIZE)
+		return -EINVAL;
+
+	nbytes = PAGE_ALIGN(cqe * sizeof *cq->wc);
+	new_npages = nbytes >> PAGE_SHIFT;
+	old_npages = PAGE_ALIGN(ibcq->cqe * sizeof *cq->wc) >> PAGE_SHIFT;
+	new_npages -= old_npages;
+
+	if (new_npages == 0)
+		return 0;
+
+	if (new_npages > 0) {
+		err = ibscif_reserve_quota(&new_npages);
+		if (err)
+			return err;
+	}
+
+	new_wc = vzalloc(nbytes); /* Consider using vmalloc_user */
+	if (!new_wc) {
+		err = -ENOMEM;
+		goto out1;
+	}
+	cqe = nbytes / sizeof *cq->wc;
+	old_wc = cq->wc;
+
+	spin_lock_bh(&cq->lock);
+
+	if (cqe < cq->depth) {
+		err = -EBUSY;
+		goto out2;
+	}
+
+	for (i = 0; i < cq->depth; i++) {
+		new_wc[i] = old_wc[cq->head];
+		cq->head = (cq->head + 1) % ibcq->cqe;
+	}
+
+	cq->wc	  = new_wc;
+	cq->head  = 0;
+	cq->tail  = cq->depth;
+	ibcq->cqe = cqe;
+
+	spin_unlock_bh(&cq->lock);
+
+	if (old_wc)
+		vfree(old_wc);
+	if (new_npages < 0)
+		ibscif_release_quota(-new_npages);
+
+	return 0;
+out2:
+	spin_unlock_bh(&cq->lock);
+	vfree(new_wc);
+out1:
+	if (new_npages > 0)
+		ibscif_release_quota(new_npages);
+	return err;
+}
+
+int ibscif_destroy_cq(struct ib_cq *ibcq)
+{
+	struct ibscif_dev *dev = to_dev(ibcq->device);
+	struct ibscif_cq *cq = to_cq(ibcq);
+
+	tasklet_kill(&cq->tasklet);
+
+	if (cq->wc)
+		vfree(cq->wc);
+
+	ibscif_release_quota(PAGE_ALIGN(ibcq->cqe * sizeof *cq->wc) >> PAGE_SHIFT);
+
+	atomic_dec(&dev->cq_cnt);
+
+	kfree(cq);
+	return 0;
+}
+
+int ibscif_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *entry)
+{
+	struct ibscif_cq *cq = to_cq(ibcq);
+	struct ibscif_wq *wq;
+	int i, reap;
+
+	/*
+	 * The protocol layer holds WQ lock while processing a packet and acquires
+	 * the CQ lock to append a work completion.  To avoid a deadly embrace, do
+	 * not hold the CQ lock when adjusting the WQ reap count.
+	 */
+	for (i = 0; (i < num_entries) && cq->depth; i++) {
+
+		spin_lock_bh(&cq->lock);
+		entry[i] = cq->wc[cq->head].ibwc;
+		reap = cq->wc[cq->head].reap;
+		cq->depth--;
+		wq = cq->wc[cq->head].wq;
+		cq->head = (cq->head + 1) % ibcq->cqe;
+		spin_unlock_bh(&cq->lock);
+
+		/* WQ may no longer exist or has been flushed. */ 
+		if (wq) {
+			spin_lock_bh(&wq->lock);
+			wq->head = (wq->head + reap) % wq->size;
+			wq->depth -= reap;
+			wq->completions -= reap;
+			spin_unlock_bh(&wq->lock);
+		}
+	}
+
+	return i;
+}
+
+int ibscif_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags notify)
+{
+	struct ibscif_cq *cq = to_cq(ibcq);
+	int ret;
+
+	spin_lock_bh(&cq->lock);
+
+	cq->arm |= notify & IB_CQ_SOLICITED_MASK;
+
+	if (notify & IB_CQ_SOLICITED)
+		cq->solicited = 0;
+
+	ret = (notify & IB_CQ_REPORT_MISSED_EVENTS) && cq->depth;
+
+	spin_unlock_bh(&cq->lock);
+
+	return ret;
+}
+
+void ibscif_notify_cq(struct ibscif_cq *cq)
+{
+	if (!cq->arm || !cq->depth)
+		return;
+
+	spin_lock_bh(&cq->lock);
+	if ((cq->arm & IB_CQ_NEXT_COMP) || ((cq->arm & IB_CQ_SOLICITED) && cq->solicited)) {
+		cq->arm = 0;	/* Disarm the CQ */
+		spin_unlock_bh(&cq->lock);
+		tasklet_hi_schedule(&cq->tasklet);
+	} else
+		spin_unlock_bh(&cq->lock);
+}
+
+void ibscif_clear_cqes(struct ibscif_cq *cq, struct ibscif_wq *wq)
+{
+	struct ibscif_wc *wc;
+	int i, j;
+
+	if (!cq)
+		return;
+
+	/*
+	 * Walk the CQ work completions and clear pointers to the
+	 * given WQ to prevent retiring WQEs when CQEs are polled.
+	 */
+	spin_lock_bh(&cq->lock);
+	j = cq->head;
+	for (i = 0; i < cq->depth; i++) {
+		wc = &cq->wc[j];
+		if (wc->wq == wq)
+			wc->wq = NULL;
+		j = (j + 1) % cq->ibcq.cqe;
+	}
+	spin_unlock_bh(&cq->lock);
+}
+
+/*
+ * Acquire lock and reserve a completion queue entry.
+ * Note that cq->lock is held upon successful completion of this call.
+ * On error, WQs affiliated with this CQ should generate an event and
+ * transition to the error state; refer to IB Spec r1.2 C11-39 and C11-40.
+ */
+int ibscif_reserve_cqe(struct ibscif_cq *cq, struct ibscif_wc **wc)
+{
+	spin_lock_bh(&cq->lock);
+
+	if (cq->state != CQ_READY) {
+		spin_unlock_bh(&cq->lock);
+		return -EIO;
+	}
+	if (!cq->ibcq.cqe) {
+		spin_unlock_bh(&cq->lock);
+		return -ENOSPC;
+	}
+	if (cq->depth == cq->ibcq.cqe) {
+		cq->state = CQ_ERROR;
+		spin_unlock_bh(&cq->lock);
+
+		if (cq->ibcq.event_handler) {
+			struct ib_event record;
+			record.event	  = IB_EVENT_CQ_ERR;
+			record.device	  = cq->ibcq.device;
+			record.element.cq = &cq->ibcq;
+			cq->ibcq.event_handler(&record, cq->ibcq.cq_context);
+		}
+		return -ENOBUFS;
+	}
+
+	*wc = &cq->wc[cq->tail];
+
+	return 0;
+}
+
+/*
+ * Append a completion queue entry and release lock.
+ * Note that this function assumes that the cq->lock is currently held.
+ */
+void ibscif_append_cqe(struct ibscif_cq *cq, struct ibscif_wc *wc, int solicited)
+{
+	cq->solicited = !!(solicited || (wc->ibwc.status != IB_WC_SUCCESS));
+	cq->tail = (cq->tail + 1) % cq->ibcq.cqe;
+	cq->depth++;
+
+	spin_unlock_bh(&cq->lock);
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_driver.h b/drivers/infiniband/hw/scif/ibscif_driver.h
--- a/drivers/infiniband/hw/scif/ibscif_driver.h	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_driver.h	2016-04-14 13:33:08.860411266 -0700
@@ -0,0 +1,787 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef IBSCIF_DRIVER_H
+#define IBSCIF_DRIVER_H
+
+#include <linux/module.h>
+#include <linux/idr.h>		/* for idr routines	*/
+#include <linux/kthread.h>	/* for kthread routines	*/
+#include <linux/highmem.h>	/* for kmap_atomic	*/
+#include <linux/pkt_sched.h>	/* for TC_PRIO_CONTROL	*/
+#include <linux/if_arp.h>	/* for ARPHRD_ETHER	*/
+#include <linux/swap.h>		/* for totalram_pages	*/
+#include <linux/proc_fs.h>	/* for proc_mkdir	*/
+#include <linux/version.h>	/* for LINUX_VERSION_CODE */
+#include <linux/poll.h>
+#include <linux/workqueue.h>
+#include <linux/semaphore.h>
+
+/* these macros are defined in "linux/semaphore.h".
+ * however, they may be missing on older systems.
+ */
+#ifndef DECLARE_MUTEX
+#define DECLARE_MUTEX(name) \
+	struct semaphore name = __SEMAPHORE_INITIALIZER(name, 1)
+#endif
+
+#ifndef init_MUTEX
+#define init_MUTEX(sem)         sema_init(sem, 1)
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+  #include <linux/interrupt.h>
+
+  #define KMAP_ATOMIC(x,y) kmap_atomic(x)
+  #define KUNMAP_ATOMIC(x,y) kunmap_atomic(x)
+#else
+  #define KMAP_ATOMIC(x,y) kmap_atomic(x, y)
+  #define KUNMAP_ATOMIC(x,y) kunmap_atomic(x, y)
+#endif
+
+#include <rdma/ib_umem.h>
+#include <rdma/ib_verbs.h>
+#include <rdma/ib_user_verbs.h>
+#include <rdma/iw_cm.h>
+
+#include <scif.h>
+#include "ibscif_protocol.h"
+
+#define IBSCIF_MTU	4096
+
+#define IBSCIF_EP_TYPE_LISTEN	0
+#define IBSCIF_EP_TYPE_COMM	1
+
+#define DRV_NAME	"ibscif"
+#define PFX		DRV_NAME ": "
+#define	IBDEV_PFX	DRV_NAME ""
+#define DRV_DESC	"OpenFabrics IBSCIF Driver"
+#define DRV_VERSION	"0.1"
+#define DRV_SIGNON	DRV_DESC " v" DRV_VERSION
+#define DRV_BUILD	" built " __DATE__ " " __TIME__
+
+#define UVERBS_ABI_VER	6
+#define VENDOR_ID	0x8086	/* Intel Corporation */
+#define DEVICE_ID	0
+#define HW_REV		1
+#define FW_REV		IBSCIF_PROTOCOL_VER
+
+/*
+ * Attribute limits.
+ * These limits are imposed on client requests, however, the actual values
+ * returned may be larger than these limits on some objects due to rounding.
+ * The definitions are intended to show the thinking behind the values.
+ * E.g., MAX_PDS defined as MAX_QPS is intended to allow each QP to be
+ * on a separate PD, although that is not a usage requirement.
+ */
+#define	MAX_QPS		(64 * 1024)
+#define	MAX_QP_SIZE	(16 * 1024)
+#define	MAX_CQS		(MAX_QPS * 2)	  /* x2:send queues + recv queues */
+#define	MAX_CQ_SIZE	(MAX_QP_SIZE * 4) /* or combined		  */ 
+#define	MAX_PDS		MAX_QPS		  /* 1 per QP			  */
+#if 0
+#define	MAX_MRS		(MAX_QPS * 4)	  /* x4:local/remote,read/write	  */
+#else
+#define	MAX_MRS		16383	  	  /* limited by IBSCIF_MR_MAX_KEY */
+#endif
+#define	MAX_MR_SIZE	(2U * 1024 * 1024 * 1024)
+#define	MAX_SGES	(PAGE_SIZE / sizeof(struct ib_sge))
+#define	MAX_OR		(MAX_QP_SIZE / 2) /* half outbound reqs		  */
+#define	MAX_IR		MAX_OR	/* balance inbound with outbound	  */
+
+extern int window_size;
+#define MIN_WINDOW_SIZE	4	/* Ack every window_size/MIN_WINDOW_SIZE packets */
+
+extern int rma_threshold;
+extern int fast_rdma;
+extern int blocking_send;
+extern int blocking_recv;
+extern int scif_loopback;
+extern int host_proxy;
+extern int new_ib_type;
+extern int verbose;
+extern int check_grh;
+
+extern struct list_head devlist;
+extern struct semaphore devlist_mutex;
+
+extern struct idr wiremap;
+extern rwlock_t wiremap_lock;
+
+extern struct ib_dma_mapping_ops ibscif_dma_mapping_ops;
+
+/* Match IB opcodes for copy in post_send; append driver specific values. */
+enum ibscif_wr_opcode {
+	WR_SEND			= IB_WR_SEND,
+	WR_SEND_WITH_IMM	= IB_WR_SEND_WITH_IMM,
+	WR_RDMA_WRITE		= IB_WR_RDMA_WRITE,
+	WR_RDMA_WRITE_WITH_IMM	= IB_WR_RDMA_WRITE_WITH_IMM,
+	WR_RDMA_READ		= IB_WR_RDMA_READ,
+	WR_ATOMIC_CMP_AND_SWP	= IB_WR_ATOMIC_CMP_AND_SWP,
+	WR_ATOMIC_FETCH_AND_ADD = IB_WR_ATOMIC_FETCH_AND_ADD,
+	WR_RDMA_READ_RSP,
+	WR_ATOMIC_RSP,
+	WR_RMA_RSP,
+	WR_UD,
+	NR_WR_OPCODES		/* Must be last (for stats) */
+};
+
+struct ibscif_stats {
+	unsigned long	packets_sent;
+	unsigned long	packets_rcvd;
+	unsigned long	bytes_sent;
+	unsigned long	bytes_rcvd;
+	unsigned long	duplicates;
+	unsigned long	tx_errors;
+	unsigned long	sched_exhaust;
+	unsigned long	unavailable;
+	unsigned long	loopback;
+	unsigned long	recv;
+	unsigned long	recv_imm;
+	unsigned long	wr_opcode[NR_WR_OPCODES];
+	unsigned long	fast_rdma_write;
+	unsigned long	fast_rdma_read;
+	unsigned long	fast_rdma_unavailable;
+	unsigned long	fast_rdma_fallback;
+	unsigned long	fast_rdma_force_ack;
+	unsigned long	fast_rdma_tail_write;
+};
+
+#define	DEV_STAT(dev, counter)	dev->stats.counter
+
+#define IBSCIF_MAX_DEVICES	16
+#define IBSCIF_NAME_SIZE	12
+
+#define IBSCIF_NODE_ID_TO_LID(node_id)	(node_id+1000)
+#define IBSCIF_LID_TO_NODE_ID(lid)	(lid-1000)
+
+struct ibscif_conn {
+	struct list_head	entry;
+	atomic_t		refcnt;
+	scif_epd_t 		ep;
+	unsigned short		remote_node_id;
+	union ib_gid		remote_gid;
+	struct ibscif_dev	*dev;
+	int			local_close;
+	int			remote_close;
+};
+
+struct ibscif_listen {
+	struct iw_cm_id		*cm_id;
+	struct list_head	entry;
+	struct kref		kref;
+	int			port;
+};
+
+#define IBSCIF_MAX_PDATA_SIZE	256
+struct ibscif_cm {
+	struct iw_cm_id		*cm_id;
+	struct ibscif_conn	*conn;
+	struct ibscif_listen	*listen;
+	struct kref		kref;
+	spinlock_t		lock;
+	struct sockaddr_in	local_addr;
+	struct sockaddr_in	remote_addr;
+	unsigned short		node_id;
+	unsigned short		remote_node_id;
+	u32			qpn;
+	u32			remote_qpn;
+	int			plen;
+	u8			pdata[IBSCIF_MAX_PDATA_SIZE];
+	u64			peer_context;
+};
+
+struct ibscif_dev {
+	struct ib_device	ibdev;
+	struct net_device	*netdev; 	/* for RDMA CM support */
+	struct list_head	entry;
+
+	char			name[IBSCIF_NAME_SIZE];
+	union ib_gid		gid;
+	unsigned short		node_id;
+	atomic_t		refcnt;
+	scif_epd_t 		listen_ep;
+	struct list_head 	conn_list;
+	struct list_head	mr_list;
+	struct semaphore	mr_list_mutex;
+
+	struct proc_dir_entry	*procfs;
+	struct ibscif_stats	stats;
+
+	atomic_t		pd_cnt;
+	atomic_t		cq_cnt;
+	atomic_t		qp_cnt;
+	atomic_t		mr_cnt;
+
+	atomic_t		available;
+	atomic_t		was_new;
+
+	spinlock_t		atomic_op;
+
+	struct semaphore	mutex;
+	struct list_head	wq_list;	/* List of WQ's on this device */
+};
+
+struct ibscif_pd {
+	struct ib_pd		ibpd;
+};
+
+struct ibscif_ah {
+	struct ib_ah		ibah;
+	__be16			dlid;
+};
+
+struct ibscif_wc {
+	struct ib_wc		ibwc;
+	int			reap;
+	struct ibscif_wq	*wq;
+};
+
+enum ibscif_cq_state {
+	CQ_READY,
+	CQ_ERROR
+};
+
+struct ibscif_cq {
+	struct ib_cq		ibcq;
+	spinlock_t		lock;
+	struct tasklet_struct	tasklet;
+	enum ibscif_cq_state	state;
+	enum ib_cq_notify_flags	arm;
+	int			solicited;
+	int			head;
+	int			tail;
+	int			depth;
+	struct ibscif_wc	*wc;
+};
+
+struct ibscif_ds {
+	struct ibscif_mr	*mr;
+	u32			offset;
+	u32			length;
+	u32			lkey;
+	u32			in_use;
+	struct ibscif_mreg_info	*current_mreg;
+};
+
+struct ibscif_segmentation {
+	struct ibscif_ds	*current_ds;
+	u32			current_page_index;
+	u32			current_page_offset;
+	u32			wr_length_remaining;
+	u32			ds_length_remaining;
+	u32			starting_seq;
+	u32			next_seq;
+	u32			ending_seq;
+};
+
+struct ibscif_reassembly {
+	struct ibscif_ds	*current_ds;
+	u32			current_ds_offset;
+	u32			last_packet_seq;
+	u32			last_seen_seq;
+	__be32			immediate_data;
+	int			final_length;
+	u16			opcode;
+};
+
+struct ibscif_sar {
+	struct ibscif_segmentation seg;
+	struct ibscif_reassembly  rea;
+};
+
+enum ibscif_wr_state {
+	WR_WAITING,
+	WR_STARTED,
+	WR_WAITING_FOR_ACK,
+	WR_WAITING_FOR_RSP,
+	WR_LAST_SEEN,
+	WR_COMPLETED
+};
+
+struct ibscif_wr {
+	u64			id;
+	enum ibscif_wr_opcode	opcode;
+	int			length;
+	enum ib_send_flags	flags;
+
+	u32			msg_id;
+	enum ibscif_wr_state	state;
+	struct ibscif_sar	sar;
+	u32			use_rma;
+	u32			rma_id;
+
+	union {
+		struct ibscif_send {
+			u32		immediate_data;
+		} send;
+
+		struct ibscif_ud {
+			u16		remote_node_id;
+			u32		remote_qpn;
+		} ud;
+
+		struct ibscif_read {
+			u64		remote_address;
+			int		remote_length;
+			u32		rkey;
+		} read;
+
+		struct ibscif_write {
+			u64		remote_address;
+			u32		rkey;
+			u32		immediate_data;
+		} write;
+
+		struct ibscif_cmp_swp {
+			u64		cmp_operand;
+			u64		swp_operand;
+			u64		remote_address;
+			u32		rkey;
+		} cmp_swp;
+
+		struct ibscif_fetch_add {
+			u64		add_operand;
+			u64		remote_address;
+			u32		rkey;
+		} fetch_add;
+
+		struct ibscif_atomic_rsp {
+			u64		orig_data;
+			u16		opcode;
+		} atomic_rsp;
+
+		struct ibscif_rma_rsp {
+			u32		xfer_length;
+			u32		error;
+		} rma_rsp;
+	};
+
+	u32			num_ds;
+	struct ibscif_ds		ds_list[0];	/* Must be last */
+};
+
+struct ibscif_tx_state {
+	u32			next_seq;
+	u32			last_ack_seq_recvd;
+	u32			next_msg_id;
+};
+
+struct ibscif_rx_state {
+	u32			last_in_seq;
+	u32			last_seq_acked;
+	int			defer_in_process;
+};
+
+struct ibscif_wirestate {
+	struct ibscif_tx_state	tx;
+	struct ibscif_rx_state	rx;
+};
+
+struct ibscif_wire {
+	struct ibscif_wirestate	sq;
+	struct ibscif_wirestate	iq;
+};
+
+struct ibscif_wq {
+	struct list_head	entry;
+	struct ibscif_qp	*qp;
+	spinlock_t		lock;
+	struct ibscif_wr	*wr;
+	int			head;
+	int			tail;
+	int			depth;
+	int			size;
+	int			max_sge;
+	int			wr_size;
+	int			completions;
+	int			reap;
+	int			next_wr;
+	int			next_msg_id;
+	struct ibscif_wirestate	*wirestate;
+	int			fast_rdma_completions;
+	int			ud_msg_id;
+};
+
+enum ibscif_qp_state {
+	QP_IDLE,
+	QP_CONNECTED,
+	QP_DISCONNECT,
+	QP_ERROR,
+	QP_RESET,
+	QP_IGNORE,
+	NR_QP_STATES		/* Must be last */
+};
+
+enum ibscif_schedule {
+	SCHEDULE_RESUME	 = 1 << 0,
+	SCHEDULE_RETRY	 = 1 << 1,
+	SCHEDULE_TIMEOUT = 1 << 2,
+	SCHEDULE_SQ	 = 1 << 6,
+	SCHEDULE_IQ	 = 1 << 7
+};
+
+struct ibscif_qp {
+	int			magic;		/* Must be first */
+#	define QP_MAGIC		0x5b51505d	/*    "[QP]"     */
+	struct kref		ref;
+	struct completion	done;
+	struct ib_qp		ibqp;
+	struct ibscif_dev	*dev;
+	enum ib_access_flags	access;
+	enum ib_sig_type	sq_policy;
+	enum ibscif_schedule	schedule;
+	struct ibscif_wire	wire;
+	int			mtu;
+
+	int			max_or;
+	atomic_t		or_depth;
+	atomic_t		or_posted;
+
+	struct semaphore	modify_mutex;
+	spinlock_t		lock;
+	enum ibscif_qp_state	state;
+	u16			local_node_id;
+	u16			remote_node_id;
+	struct ibscif_conn	*conn;
+	u32			remote_qpn;
+	int			loopback;
+	struct ibscif_wq	sq;
+	struct ibscif_wq	rq;
+	struct ibscif_wq	iq;
+	int			in_scheduler;
+
+	struct ibscif_conn	*ud_conn[IBSCIF_MAX_DEVICES];
+	struct ibscif_cm	*cm_context;
+};
+
+#define	is_sq(wq)		(wq == &wq->qp->sq)
+#define	is_rq(wq)		(wq == &wq->qp->rq)
+#define	is_iq(wq)		(wq == &wq->qp->iq)
+
+/* Info about MR registered via SCIF API */
+struct ibscif_mreg_info {
+	struct list_head	entry;
+	struct ibscif_conn	*conn;
+	u64			offset;
+	u64			aligned_offset;
+	u32			aligned_length;
+};
+
+struct ibscif_mr {
+	int			magic;		/* Must be first */
+#	define MR_MAGIC		0x5b4d525d	/*    "[MR]"     */
+	struct list_head	entry;
+	struct kref		ref;
+	struct completion	done;
+	struct ib_mr		ibmr;
+	struct ib_umem		*umem;
+	enum ib_access_flags	access;
+	u64			addr;
+	u32			length;
+	int			npages;
+	struct page		**page;
+	scif_pinned_pages_t 	pinned_pages;
+	struct list_head	mreg_list;
+};
+
+/* Canonical virtual address on X86_64 falls in the range 0x0000000000000000-0x00007fffffffffff
+ * and 0xffff800000000000-0xffffffffffffffff. The range 0x0000800000000000-0xffff7fffffffffff
+ * are unused. This basically means only 48 bits are used and the highest 16 bits are just sign
+ * extensions. We can put rkey into these 16 bits and use the result as the "offset" of SCIF's 
+ * registered address space. By doing this, the SCIF_MAP_FIXED flag can be used so that the offset
+ * can be calculated directly from rkey and virtual address w/o using the "remote registration cache" 
+ * mechanism.
+ *
+ * SCIF reserve the top 2 bits of the offset for internal uses, leaving 14 bits for rkey. 
+ */
+#define IBSCIF_MR_MAX_KEY	(0x3FFF)
+#define IBSCIF_MR_VADDR_MASK	(0x0000FFFFFFFFFFFFUL)
+#define IBSCIF_MR_SIGN_MASK	(0x0000800000000000UL)
+#define IBSCIF_MR_SIGN_EXT	(0xFFFF000000000000UL)
+#define IBSCIF_MR_RKEY_MASK	(0x3FFF000000000000UL)
+
+#define IBSCIF_MR_VADDR_TO_OFFSET(rkey, vaddr)	((((unsigned long)rkey) << 48) | \
+						 (vaddr & IBSCIF_MR_VADDR_MASK))
+
+#define IBSCIF_MR_OFFSET_TO_VADDR(offset)	((offset & IBSCIF_MR_SIGN_MASK) ? \
+						 (offset | IBSCIF_MR_SIGN_EXT) : \
+						 (offset & IBSCIF_MR_VADDR_MASK))
+
+#define IBSCIF_MR_OFFSET_TO_RKEY(offset)	((offset & IBSCIF_MR_RKEY_MASK) >> 48)
+
+#define	TO_OBJ(name, src, dst, field)				\
+static inline struct dst *name(struct src *field)		\
+{								\
+	return container_of(field, struct dst, field);		\
+}
+TO_OBJ(to_dev, ib_device, ibscif_dev, ibdev)
+TO_OBJ(to_pd, ib_pd, ibscif_pd, ibpd)
+TO_OBJ(to_cq, ib_cq, ibscif_cq, ibcq)
+TO_OBJ(to_qp, ib_qp, ibscif_qp, ibqp)
+TO_OBJ(to_mr, ib_mr, ibscif_mr, ibmr)
+TO_OBJ(to_ah, ib_ah, ibscif_ah, ibah)
+
+#define OBJ_GET(obj, type)					\
+static inline struct ibscif_##obj *ibscif_get_##obj(int id)	\
+{								\
+	struct ibscif_##obj *obj;				\
+	read_lock_bh(&wiremap_lock);				\
+	obj = idr_find(&wiremap, id);				\
+	if (likely(obj)) {					\
+		if (likely(obj->magic == type))			\
+			kref_get(&obj->ref);			\
+		else						\
+			obj = ERR_PTR(-ENXIO);			\
+	} else							\
+		obj = ERR_PTR(-ENOENT);				\
+	read_unlock_bh(&wiremap_lock);				\
+	return obj;						\
+}
+OBJ_GET(mr, MR_MAGIC)
+OBJ_GET(qp, QP_MAGIC)
+
+void ibscif_complete_mr(struct kref *kref);
+void ibscif_complete_qp(struct kref *kref);
+
+#define OBJ_PUT(obj)						\
+static inline void ibscif_put_##obj(struct ibscif_##obj *obj)	\
+{								\
+	if (likely(obj))					\
+		kref_put(&obj->ref, ibscif_complete_##obj);	\
+}
+OBJ_PUT(mr)
+OBJ_PUT(qp)
+
+#define RHEL61_AND_ABOVE 0
+#if defined(RHEL_MAJOR) && defined(RHEL_MINOR)
+#if (RHEL_MAJOR==6) && (RHEL_MINOR>0)
+#undef RHEL61_AND_ABOVE
+#define RHEL61_AND_ABOVE 1
+#endif
+#endif
+
+#if (LINUX_VERSION_CODE<KERNEL_VERSION(2,6,37)) && ! RHEL61_AND_ABOVE
+static inline void *vzalloc(unsigned long size)
+{
+	void *addr = vmalloc(size);
+	if (addr)
+		memset(addr, 0, size);
+	return addr;
+}
+#endif
+
+/* This function assumes the WQ is protected by a lock. */
+static inline struct ibscif_wr *ibscif_get_wr(struct ibscif_wq *wq, int index)
+{
+	/* Must calculate because WQ array elements are variable sized. */
+	return (struct ibscif_wr *)((void *)wq->wr + (wq->wr_size * index));
+}
+
+/* This function assumes the WQ is protected by a lock. */
+static inline void ibscif_append_wq(struct ibscif_wq *wq)
+{
+	wq->tail = (wq->tail + 1) % wq->size;
+	wq->depth++;
+	wq->next_msg_id++;
+}
+
+static inline void ibscif_clear_ds_ref(struct ibscif_ds *ds)
+{
+	if (ds->in_use) {
+		ds->in_use = 0;
+		ibscif_put_mr(ds->mr);
+	}
+}
+
+static inline void ibscif_clear_ds_refs(struct ibscif_ds *ds, int num_ds)
+{
+	while(num_ds--)
+		ibscif_clear_ds_ref(ds++);
+}
+
+static inline enum ib_wc_opcode to_ib_wc_opcode(enum ib_wr_opcode opcode)
+{
+	/* SQ only - RQ is either IB_WC_RECV or IB_WC_RECV_RDMA_WITH_IMM. */
+	switch (opcode) {
+	case IB_WR_RDMA_WRITE:		 return IB_WC_RDMA_WRITE;
+	case IB_WR_RDMA_WRITE_WITH_IMM:	 return IB_WC_RDMA_WRITE;
+	case IB_WR_SEND:		 return IB_WC_SEND;
+	case IB_WR_SEND_WITH_IMM:	 return IB_WC_SEND;
+	case IB_WR_RDMA_READ:		 return IB_WC_RDMA_READ;
+	case IB_WR_ATOMIC_CMP_AND_SWP:	 return IB_WC_COMP_SWAP;
+	case IB_WR_ATOMIC_FETCH_AND_ADD: return IB_WC_FETCH_ADD;
+	default:			 return -1;
+	}
+}
+
+static inline void *ibscif_map_src(struct page *page)
+{
+	return KMAP_ATOMIC(page, KM_SOFTIRQ0);
+}
+
+static inline void *ibscif_map_dst(struct page *page)
+{
+	return KMAP_ATOMIC(page, KM_SOFTIRQ1);
+}
+
+static inline void ibscif_unmap_src(struct page *page, void *addr)
+{
+	if (likely(addr))
+		KUNMAP_ATOMIC(addr, KM_SOFTIRQ0);
+}
+
+static inline void ibscif_unmap_dst(struct page *page, void *addr)
+{
+	if (likely(addr))
+		KUNMAP_ATOMIC(addr, KM_SOFTIRQ1);
+	if (likely(page)) {
+		flush_dcache_page(page);
+		if (!PageReserved(page))
+			set_page_dirty(page);
+	}
+}
+
+#ifdef IBSCIF_PERF_TEST
+#define IBSCIF_PERF_SAMPLE(counter,next) ibscif_perf_sample(counter,next)
+#else
+#define IBSCIF_PERF_SAMPLE(counter,next)
+#endif
+
+int ibscif_atomic_copy(void *dst_addr, void *src_addr, u32 copy_len, int head_copied);
+
+int ibscif_wiremap_add(void *obj, int *id);
+void ibscif_wiremap_del(int id);
+
+int ibscif_dev_init(void);
+void ibscif_protocol_init_pre(void);
+void ibscif_protocol_init_post(void);
+
+void ibscif_dev_cleanup(void);
+void ibscif_protocol_cleanup(void);
+
+int ibscif_procfs_add_dev(struct ibscif_dev *dev);
+void ibscif_procfs_remove_dev(struct ibscif_dev *dev);
+
+int ibscif_reserve_quota(int *npages);
+void ibscif_release_quota(int npages);
+
+void ibscif_scheduler_add_qp(struct ibscif_qp *qp);
+void ibscif_scheduler_remove_qp(struct ibscif_qp *qp);
+void ibscif_schedule(struct ibscif_wq *wq);
+
+struct ib_ah *ibscif_create_ah(struct ib_pd *ibpd, struct ib_ah_attr *attr);
+int ibscif_destroy_ah(struct ib_ah *ibah);
+
+struct ib_pd *ibscif_alloc_pd(struct ib_device *ibdev, struct ib_ucontext *context, struct ib_udata *udata);
+int ibscif_dealloc_pd(struct ib_pd *ibpd);
+
+struct ib_qp *ibscif_create_qp(struct ib_pd *ibpd, struct ib_qp_init_attr *attr, struct ib_udata *udata);
+int ibscif_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr, int attr_mask, struct ib_qp_init_attr *init_attr);
+int ibscif_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr, int attr_mask, struct ib_udata *udata);
+int ibscif_destroy_qp(struct ib_qp *ibqp);
+void ibscif_qp_internal_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason);
+void ibscif_qp_remote_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason);
+void ibscif_qp_add_ud_conn(struct ibscif_qp *qp, struct ibscif_conn *conn);
+
+#ifdef	MOFED
+struct ib_cq *ibscif_create_cq(struct ib_device *ibdev, struct ib_cq_init_attr *attr,
+			      struct ib_ucontext *context, struct ib_udata *udata);
+#else
+struct ib_cq *ibscif_create_cq(struct ib_device *ibdev, int entries, int comp_vector,
+			      struct ib_ucontext *context, struct ib_udata *udata);
+#endif
+int ibscif_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata);
+int ibscif_destroy_cq(struct ib_cq *ibcq);
+int ibscif_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *entry);
+int ibscif_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags notify);
+void ibscif_notify_cq(struct ibscif_cq *cq);
+void ibscif_clear_cqes(struct ibscif_cq *cq, struct ibscif_wq *wq);
+int ibscif_reserve_cqe(struct ibscif_cq *cq, struct ibscif_wc **wc);
+void ibscif_append_cqe(struct ibscif_cq *cq, struct ibscif_wc *wc, int solicited);
+
+struct ib_mr *ibscif_get_dma_mr(struct ib_pd *ibpd, int access);
+struct ib_mr *ibscif_reg_phys_mr(struct ib_pd *ibpd, struct ib_phys_buf *phys_buf_array,
+				int num_phys_buf, int access, u64 *iova_start);
+#ifdef	MOFED
+struct ib_mr *ibscif_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 length,
+				u64 virt_addr, int access, struct ib_udata *udata, int mr_id);
+#else
+struct ib_mr *ibscif_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 length,
+				u64 virt_addr, int access, struct ib_udata *udata);
+#endif
+int ibscif_dereg_mr(struct ib_mr *ibmr);
+struct ibscif_mr *ibscif_validate_mr(u32 key, u64 addr, int length,
+				   struct ib_pd *ibpd, enum ib_access_flags access);
+struct ibscif_mreg_info *ibscif_mr_get_mreg(struct ibscif_mr *mr, struct ibscif_conn *conn);
+void ibscif_refresh_mreg( struct ibscif_conn *conn );
+
+int ibscif_post_send(struct ib_qp *ibqp, struct ib_send_wr *ibwr, struct ib_send_wr **bad_wr);
+int ibscif_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *ibwr, struct ib_recv_wr **bad_wr);
+
+void ibscif_send_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason);
+void ibscif_send_close(struct ibscif_conn *conn);
+void ibscif_send_reopen(struct ibscif_conn *conn);
+
+void ibscif_loopback_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason);
+void ibscif_loopback(struct ibscif_wq *sq);
+
+int ibscif_xmit_wr(struct ibscif_wq *wq, struct ibscif_wr *wr, int tx_limit, int retransmit,
+		  u32 from_seq, u32 *posted);
+int ibscif_process_sq_completions(struct ibscif_qp *qp);
+
+struct ibscif_conn *ibscif_get_conn( int node_id, int remote_node_id, int find_local_peer );
+void ibscif_put_conn( struct ibscif_conn *conn );
+void ibscif_do_accept(struct ibscif_dev *dev);
+void ibscif_get_pollep_list(struct scif_pollepd *polleps, struct ibscif_dev **devs,
+			  int *types, struct ibscif_conn **conns, int *count);
+void ibscif_refresh_pollep_list(void);
+void ibscif_get_ep_list(scif_epd_t *eps, int *count);
+void ibscif_remove_ep(struct ibscif_dev *dev, scif_epd_t ep);
+void ibscif_free_conn(struct ibscif_conn *conn);
+int  ibscif_cleanup_idle_conn( void );
+void ibscif_perf_sample(int counter, int next);
+
+int ibscif_cm_connect(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param);
+int ibscif_cm_accept(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param);
+int ibscif_cm_reject(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len);
+int ibscif_cm_create_listen(struct iw_cm_id *cm_id, int backlog);
+int ibscif_cm_destroy_listen(struct iw_cm_id *cm_id);
+struct ib_qp *ibscif_cm_get_qp(struct ib_device *ibdev, int qpn);
+void ibscif_cm_add_ref(struct ib_qp *ibqp);
+void ibscif_cm_rem_ref(struct ib_qp *ibqp);
+void ibscif_cm_async_callback(void *cm_context);
+int ibscif_process_cm_skb(struct sk_buff *skb, struct ibscif_conn *conn);
+int ibscif_send_cm_req(struct ibscif_cm *cm_ctx);
+int ibscif_send_cm_rep(struct ibscif_cm *cm_ctx);
+int ibscif_send_cm_rej(struct ibscif_cm *cm_ctx, const void *pdata, u8 plen);
+int ibscif_send_cm_rtu(struct ibscif_cm *cm_ctx);
+
+#endif /* IBSCIF_DRIVER_H */
diff -ruN a/drivers/infiniband/hw/scif/ibscif_loopback.c b/drivers/infiniband/hw/scif/ibscif_loopback.c
--- a/drivers/infiniband/hw/scif/ibscif_loopback.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_loopback.c	2016-04-14 13:33:08.861411229 -0700
@@ -0,0 +1,582 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+struct ibscif_seg {
+	enum ib_access_flags	access;
+	struct ibscif_ds		*ds;
+	struct ibscif_mr		*mr;
+	struct page		**page;
+	void			*addr;
+	u32			offset;
+	u32			ds_len;
+	u32			pg_len;
+	void			*(*map)(struct page *page);
+	void			(*unmap)(struct page *page, void *addr);
+};
+
+static void ibscif_seg_init(struct ibscif_seg *seg, struct ibscif_ds *ds,
+			   void *(*map)(struct page *page), void (*unmap)(struct page *page, void *addr),
+			   enum ib_access_flags access)
+{
+	memset(seg, 0, sizeof *seg);
+	seg->ds	    = ds;
+	seg->map    = map;
+	seg->unmap  = unmap;
+	seg->access = access;
+}
+
+static void ibscif_seg_fini(struct ibscif_seg *seg)
+{
+	seg->unmap(*seg->page, seg->addr);
+	if (likely(seg->mr))
+		ibscif_put_mr(seg->mr);
+}
+
+static int ibscif_seg_set(struct ibscif_seg *seg, u32 length, u32 copy_len)
+{
+	struct page **prev_page;
+
+	if (!seg->ds_len) {
+
+		if (seg->mr)
+			ibscif_put_mr(seg->mr);
+
+		seg->mr = ibscif_get_mr(seg->ds->lkey);
+		if (unlikely(IS_ERR(seg->mr)))
+			return PTR_ERR(seg->mr);
+
+		if (unlikely(seg->access && !(seg->mr->access & seg->access)))
+			return -EACCES;
+
+		prev_page    = seg->page;
+		seg->offset  = seg->ds->offset + (seg->mr->addr & ~PAGE_MASK);
+		seg->page    = &seg->mr->page[seg->offset >> PAGE_SHIFT];
+		seg->offset &= ~PAGE_MASK;
+		seg->ds_len  = seg->ds->length;
+		seg->pg_len  = min(seg->ds_len, (u32)PAGE_SIZE - seg->offset);
+		seg->pg_len  = min(seg->pg_len, length);
+
+		if (seg->page != prev_page)
+			seg->addr = seg->map(*seg->page) + seg->offset;
+
+		seg->ds++;
+
+	} else if (!seg->pg_len) {
+
+		seg->unmap(*seg->page, seg->addr);
+
+		seg->page++;
+		seg->addr   = seg->map(*seg->page);
+		seg->pg_len = min(seg->ds_len, (u32)PAGE_SIZE);
+		seg->pg_len = min(seg->pg_len, length);
+	} else
+		seg->addr += copy_len;
+
+	return 0;
+}
+
+static inline int ibscif_seg_copy(struct ibscif_seg *dst, struct ibscif_seg *src, u32 length, int head_copied)
+{
+	src->ds_len -= length;
+	src->pg_len -= length;
+
+	dst->ds_len -= length;
+	dst->pg_len -= length;
+
+	return ibscif_atomic_copy(dst->addr, src->addr, length, head_copied);
+}
+
+/*
+ * Copy data from the source to the destination data segment list.
+ * This is a bit complicated since we must map and copy each page
+ * individually and because each data segment can be split across
+ * multiple pages within the memory region as illustrated below:
+ *
+ *	+---page---+   +---page---+   +---page---+
+ *	|  .~~mr~~~|~~~|~~~~~~~~~~|~~~|~~~~~~.   |
+ *	|  |       |   |  [==ds===|===|====] |   |
+ *	|  '~~~~~~~|~~~|~~~~~~~~~~|~~~|~~~~~~'   |
+ *	+----------+   +----------+   +----------+
+ *
+ * For example, due to different buffer page offsets, copying data
+ * between the following buffers will result in five separate copy
+ * operations as shown by the numeric labels below:
+ *
+ *	       +----------+     +----------+
+ *	       |          |     |          |
+ *	       |1111111111|     |          |
+ *	       |2222222222|     |1111111111|
+ *	       +----------+     +----------+
+ *
+ *	       +----------+     +----------+
+ *	       |3333333333|     |2222222222|
+ *	       |3333333333|     |3333333333|
+ *	       |4444444444|     |3333333333|
+ *	       +----------+     +----------+
+ *
+ *	       +----------+     +----------+
+ *	       |5555555555|     |4444444444|
+ *	       |          |     |5555555555|
+ *	       |          |     |          |
+ *	       +----------+     +----------+
+ *
+ * The source and destination data segment list lengths are
+ * assumed to have been validated outside of this function.
+ */
+static int ibscif_dscopy(struct ibscif_ds *dst_ds, struct ibscif_ds *src_ds, u32 length)
+{
+	struct ibscif_seg src, dst;
+	int head_copied;
+	u32 copy_len;
+	int err = 0;
+
+	ibscif_seg_init(&src, src_ds, ibscif_map_src, ibscif_unmap_src, 0);
+	ibscif_seg_init(&dst, dst_ds, ibscif_map_dst, ibscif_unmap_dst, IB_ACCESS_LOCAL_WRITE);
+
+	head_copied = 0;
+	for (copy_len = 0; length; length -= copy_len) {
+
+		err = ibscif_seg_set(&src, length, copy_len);
+		if (unlikely(err))
+			break;
+		err = ibscif_seg_set(&dst, length, copy_len);
+		if (unlikely(err))
+			break;
+
+		copy_len = min(src.pg_len, dst.pg_len);
+		head_copied = ibscif_seg_copy(&dst, &src, copy_len, head_copied);
+	}
+
+	ibscif_seg_fini(&src);
+	ibscif_seg_fini(&dst);
+
+	return err;
+}
+
+/* Hold sq->lock during this call for synchronization. */
+static int ibscif_complete_sq_wr(struct ibscif_wq *sq, struct ibscif_wr *send_wr, enum ib_wc_status status)
+{
+	struct ibscif_qp *qp = sq->qp;
+	struct ibscif_wc *wc;
+	int err;
+
+	ibscif_clear_ds_refs(send_wr->ds_list, send_wr->num_ds);
+	sq->completions++;
+	sq->reap++;
+
+	if (send_wr->flags & IB_SEND_SIGNALED) {
+		struct ibscif_cq *cq = to_cq(qp->ibqp.send_cq);
+
+		err = ibscif_reserve_cqe(cq, &wc);
+		if (unlikely(err))
+			return err;
+
+		wc->ibwc.qp	  = &qp->ibqp;
+		wc->ibwc.src_qp	  = qp->remote_qpn;
+		wc->ibwc.wr_id	  = send_wr->id;
+		wc->ibwc.opcode	  = to_ib_wc_opcode(send_wr->opcode);
+		wc->ibwc.status	  = status;
+		wc->ibwc.ex.imm_data = 0;
+		wc->ibwc.port_num = 1;
+
+		if ((enum ib_wr_opcode)send_wr->opcode == IB_WR_RDMA_READ)
+			wc->ibwc.byte_len = send_wr->read.remote_length;
+		else if (((enum ib_wr_opcode)send_wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP) ||
+			 ((enum ib_wr_opcode)send_wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD))
+			 wc->ibwc.byte_len = sizeof send_wr->atomic_rsp.orig_data;
+		else
+			wc->ibwc.byte_len = send_wr->length;
+
+		wc->wq	 = sq;
+		wc->reap = sq->reap;
+		sq->reap = 0;
+
+		ibscif_append_cqe(cq, wc, 0);
+	}
+
+	return 0;
+}
+
+/* Hold rq->lock during this call for synchronization. */
+static int ibscif_complete_rq_wr(struct ibscif_wq *rq, struct ibscif_wr *recv_wr,
+				struct ibscif_wr *send_wr, enum ib_wc_status status)
+{
+	struct ibscif_qp *qp = rq->qp;
+	struct ibscif_cq *cq = to_cq(qp->ibqp.recv_cq);
+	struct ibscif_wc *wc;
+	int err;
+
+	ibscif_clear_ds_refs(recv_wr->ds_list, recv_wr->num_ds);
+
+	err = ibscif_reserve_cqe(cq, &wc);
+	if (unlikely(err))
+		return err;
+
+	wc->ibwc.qp	  = &qp->ibqp;
+	wc->ibwc.src_qp	  = qp->remote_qpn;
+	wc->ibwc.wr_id	  = recv_wr->id;
+	wc->ibwc.status	  = status;
+	wc->ibwc.byte_len = send_wr->length;
+	wc->ibwc.port_num = 1;
+
+	if ((enum ib_wr_opcode)send_wr->opcode == IB_WR_SEND_WITH_IMM) {
+		DEV_STAT(qp->dev, recv_imm++);
+		wc->ibwc.opcode	  = IB_WC_RECV_RDMA_WITH_IMM;
+		wc->ibwc.ex.imm_data = cpu_to_be32(send_wr->send.immediate_data);
+	} else if ((enum ib_wr_opcode)send_wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM) {
+		DEV_STAT(qp->dev, recv_imm++);
+		wc->ibwc.opcode	  = IB_WC_RECV_RDMA_WITH_IMM;
+		wc->ibwc.ex.imm_data = cpu_to_be32(send_wr->write.immediate_data);
+	} else {
+		DEV_STAT(qp->dev, recv++);
+		wc->ibwc.opcode	  = IB_WC_RECV;
+		wc->ibwc.ex.imm_data = 0;
+	}
+
+	wc->wq	 = rq;
+	wc->reap = 1;
+	rq->completions++;
+
+	ibscif_append_cqe(cq, wc, !!(send_wr->flags & IB_SEND_SOLICITED));
+
+	return 0;
+}
+
+/* Hold wq lock during this call for synchronization. */
+static int ibscif_validate_wq(struct ibscif_wq *wq, struct ibscif_wr **wr, enum ib_access_flags access)
+{
+	if (unlikely(wq->qp->state != QP_CONNECTED))
+		return -ENOTCONN;
+
+	if (unlikely(access && !(wq->qp->access & access)))
+		return -EACCES;
+
+	if (wr) {
+		int next;
+
+		if (unlikely(!wq->size))
+			return -ENOSPC;
+
+		next = (wq->head + wq->completions) % wq->size;
+
+		if (unlikely(next == wq->tail))
+			return -ENOBUFS;
+
+		*wr = ibscif_get_wr(wq, next);
+	}
+
+	return 0;
+}
+
+static int ibscif_loopback_send(struct ibscif_wq *sq, struct ibscif_wq *rq, struct ibscif_wr *send_wr)
+{
+	struct ibscif_wr *recv_wr;
+	int err;
+
+	spin_lock_bh(&rq->lock);
+
+	err = ibscif_validate_wq(rq, &recv_wr, 0);
+	if (unlikely(err))
+		goto out;
+
+	if (likely(send_wr->length)) {
+		if (unlikely(send_wr->length > recv_wr->length)) {
+			err = -EMSGSIZE;
+			goto out;
+		}
+
+		err = ibscif_dscopy(recv_wr->ds_list, send_wr->ds_list, send_wr->length);
+		if (unlikely(err))
+			goto out;
+	}
+
+	err = ibscif_complete_rq_wr(rq, recv_wr, send_wr, IB_WC_SUCCESS);
+out:
+	spin_unlock_bh(&rq->lock);
+
+	return err;
+}
+
+static int ibscif_loopback_write(struct ibscif_wq *sq, struct ibscif_wq *rq, struct ibscif_wr *write_wr)
+{
+	struct ibscif_wr *recv_wr = NULL;
+	struct ibscif_mr *dst_mr	 = ERR_PTR(-ENOENT);
+	int err;
+
+	spin_lock_bh(&rq->lock);
+
+	err = ibscif_validate_wq(rq, ((enum ib_wr_opcode)write_wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM) ?
+					&recv_wr : NULL, IB_ACCESS_REMOTE_WRITE);
+	if (unlikely(err))
+		goto out;
+
+	if (likely(write_wr->length)) {
+		struct ibscif_ds dst_ds;
+
+		dst_mr = ibscif_validate_mr(write_wr->write.rkey, write_wr->write.remote_address,
+					   write_wr->length, rq->qp->ibqp.pd, IB_ACCESS_REMOTE_WRITE);
+		if (unlikely(IS_ERR(dst_mr))) {
+			err = PTR_ERR(dst_mr);
+			goto out;
+		}
+
+		dst_ds.mr     = dst_mr;
+		dst_ds.offset = write_wr->write.remote_address - dst_mr->addr;
+		dst_ds.length = write_wr->length;
+		dst_ds.lkey   = dst_mr->ibmr.lkey;
+
+		err = ibscif_dscopy(&dst_ds, write_wr->ds_list, dst_ds.length);
+		if (unlikely(err))
+			goto out;
+	} else
+		err = 0;
+
+	if (recv_wr)
+		err = ibscif_complete_rq_wr(rq, recv_wr, write_wr, IB_WC_SUCCESS);
+out:
+	if (likely(!IS_ERR(dst_mr)))
+		ibscif_put_mr(dst_mr);
+
+	spin_unlock_bh(&rq->lock);
+
+	return err;
+}
+
+static int ibscif_loopback_read(struct ibscif_wq *sq, struct ibscif_wq *iq, struct ibscif_wr *read_wr)
+{
+	struct ibscif_mr *src_mr = ERR_PTR(-ENOENT);
+	int err;
+
+	spin_lock_bh(&iq->lock);
+
+	err = ibscif_validate_wq(iq, NULL, IB_ACCESS_REMOTE_READ);
+	if (unlikely(err))
+		goto out;
+
+	if (!iq->size) {
+		err = -ENOBUFS;
+		goto out;
+	}
+
+	if (likely(read_wr->read.remote_length)) {
+		struct ibscif_ds src_ds;
+
+		src_mr = ibscif_validate_mr(read_wr->read.rkey, read_wr->read.remote_address,
+					   read_wr->read.remote_length, iq->qp->ibqp.pd,
+					   IB_ACCESS_REMOTE_READ);
+		if (unlikely(IS_ERR(src_mr))) {
+			err = PTR_ERR(src_mr);
+			goto out;
+		}
+
+		src_ds.mr     = src_mr;
+		src_ds.offset = read_wr->read.remote_address - src_mr->addr;
+		src_ds.length = read_wr->read.remote_length;
+		src_ds.lkey   = src_mr->ibmr.lkey;
+
+		err = ibscif_dscopy(read_wr->ds_list, &src_ds, src_ds.length);
+	} else
+		err = 0;
+out:
+	if (likely(!IS_ERR(src_mr)))
+		ibscif_put_mr(src_mr);
+
+	spin_unlock_bh(&iq->lock);
+
+	atomic_dec(&sq->qp->or_posted);
+
+	return err;
+}
+
+static int ibscif_loopback_atomic(struct ibscif_wq *sq, struct ibscif_wq *iq, struct ibscif_wr *atomic_wr)
+{
+	struct ibscif_mr *src_mr = ERR_PTR(-ENOENT);
+	struct ibscif_ds  src_ds;
+	struct page *src_page;
+	u64 *src_addr, addr;
+	u32 src_offset, rkey;
+	int err;
+
+	if ((enum ib_wr_opcode)atomic_wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP) {
+		addr = atomic_wr->cmp_swp.remote_address;
+		rkey = atomic_wr->cmp_swp.rkey;
+	} else {
+		addr = atomic_wr->fetch_add.remote_address;
+		rkey = atomic_wr->fetch_add.rkey;
+	}
+
+	spin_lock_bh(&iq->lock);
+
+	err = ibscif_validate_wq(iq, NULL, IB_ACCESS_REMOTE_ATOMIC);
+	if (unlikely(err))
+		goto out;
+
+	if (!iq->size) {
+		err = -ENOBUFS;
+		goto out;
+	}
+
+	src_mr = ibscif_validate_mr(rkey, addr, sizeof atomic_wr->atomic_rsp.orig_data,
+				   iq->qp->ibqp.pd, IB_ACCESS_REMOTE_ATOMIC);
+	if (unlikely(IS_ERR(src_mr))) {
+		err = PTR_ERR(src_mr);
+		goto out;
+	}
+
+	/* Build a source data segment to copy the original data. */
+	src_ds.mr     = src_mr;
+	src_ds.offset = addr - src_mr->addr;
+	src_ds.length = sizeof atomic_wr->atomic_rsp.orig_data;
+	src_ds.lkey   = src_mr->ibmr.lkey;
+
+	/* Determine which page to map. */
+	src_offset  = src_ds.offset + (src_mr->addr & ~PAGE_MASK);
+	src_page    = src_mr->page[src_offset >> PAGE_SHIFT];
+	src_offset &= ~PAGE_MASK;
+
+	/* Lock to perform the atomic operation atomically. */
+	spin_lock_bh(&iq->qp->dev->atomic_op);
+
+	/* Copy the original data; this handles any ds_list crossing. */
+	err = ibscif_dscopy(atomic_wr->ds_list, &src_ds, sizeof atomic_wr->atomic_rsp.orig_data);
+	if (likely(!err)) {
+		src_addr = ibscif_map_src(src_page) + src_offset;
+		if ((enum ib_wr_opcode)atomic_wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD)
+			 *src_addr += atomic_wr->fetch_add.add_operand;
+		else if (*src_addr == atomic_wr->cmp_swp.cmp_operand)
+			 *src_addr  = atomic_wr->cmp_swp.swp_operand;
+		ibscif_unmap_src(src_page, src_addr);
+	}
+
+	/* Atomic operation is complete. */
+	spin_unlock_bh(&iq->qp->dev->atomic_op);
+out:
+	if (likely(!IS_ERR(src_mr)))
+		ibscif_put_mr(src_mr);
+
+	spin_unlock_bh(&iq->lock);
+
+	atomic_dec(&sq->qp->or_posted);
+
+	return err;
+}
+
+void ibscif_loopback_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason)
+{
+	struct ibscif_qp *remote_qp;
+
+	remote_qp = ibscif_get_qp(qp->remote_qpn);
+	if (unlikely(IS_ERR(remote_qp)))
+		return;
+
+	/* Don't bother if the SQ is connected to the RQ on the same QP. */
+	if (remote_qp != qp)
+		ibscif_qp_remote_disconnect(remote_qp, reason);
+
+	ibscif_put_qp(remote_qp);
+}
+
+/*
+ * Loopback QPs connected through the same MAC address.
+ * This includes an SQ connected to the RQ on the same QP.
+ */
+void ibscif_loopback(struct ibscif_wq *sq)
+{
+	struct ibscif_wq *rq, *iq;
+	struct ibscif_qp *remote_qp;
+	struct ibscif_wr *wr;
+	int status = 0, err = 0;
+
+	BUG_ON(!is_sq(sq));
+
+again:
+	remote_qp = ibscif_get_qp(sq->qp->remote_qpn);
+	if (unlikely(IS_ERR(remote_qp))) {
+		ibscif_qp_remote_disconnect(sq->qp, IBSCIF_REASON_INVALID_QP);
+		return;
+	}
+	rq = &remote_qp->rq;
+	iq = &remote_qp->iq;
+
+	DEV_STAT(sq->qp->dev, loopback++);
+
+	spin_lock_bh(&sq->lock);
+	for (wr = ibscif_get_wr(sq, sq->next_wr);
+	     (sq->next_wr != sq->tail) && !err;
+	     sq->next_wr = (sq->next_wr + 1) % sq->size) {
+
+		switch (wr->opcode) {
+
+		case WR_SEND:
+		case WR_SEND_WITH_IMM:
+			status = ibscif_loopback_send(sq, rq, wr);
+			break;
+		case WR_RDMA_WRITE:
+		case WR_RDMA_WRITE_WITH_IMM:
+			status = ibscif_loopback_write(sq, rq, wr);
+			break;
+		case WR_RDMA_READ:
+			status = ibscif_loopback_read(sq, iq, wr);
+			break;
+		case WR_ATOMIC_CMP_AND_SWP:
+		case WR_ATOMIC_FETCH_AND_ADD:
+			status = ibscif_loopback_atomic(sq, iq, wr);
+			break;
+		default:
+			status = -ENOSYS;
+			break;
+		}
+
+		if (likely(!status)) {
+			err = ibscif_complete_sq_wr(sq, wr, IB_WC_SUCCESS);
+
+			spin_unlock_bh(&sq->lock);
+			ibscif_notify_cq(to_cq(sq->qp->ibqp.send_cq));
+			ibscif_notify_cq(to_cq(remote_qp->ibqp.recv_cq));
+			spin_lock_bh(&sq->lock);
+		} else
+			break;
+	}
+	spin_unlock_bh(&sq->lock);
+
+	if (unlikely(status) && status != -ENOBUFS)
+		ibscif_qp_remote_disconnect(sq->qp, IBSCIF_REASON_QP_FATAL);
+	else if (unlikely(err))
+		ibscif_qp_internal_disconnect(sq->qp, IBSCIF_REASON_QP_FATAL);
+
+	ibscif_put_qp(remote_qp);
+
+	if (status == -ENOBUFS) {
+		schedule();
+		goto again;
+	}
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_main.c b/drivers/infiniband/hw/scif/ibscif_main.c
--- a/drivers/infiniband/hw/scif/ibscif_main.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_main.c	2016-04-14 13:33:08.861411229 -0700
@@ -0,0 +1,379 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+static const char ibscif_signon[] = DRV_SIGNON DRV_BUILD;
+
+MODULE_AUTHOR("Intel Corporation");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION(DRV_DESC);
+MODULE_VERSION(DRV_VERSION);
+
+#define MODULE_PARAM(type, name, value, desc)			\
+	type name = value;					\
+	module_param(name, type, 0664);				\
+	MODULE_PARM_DESC(name, desc)
+
+#define MODULE_ARRAY(name, size, value, desc)			\
+	unsigned int name##_argc;				\
+	char *name[size] = { [0 ... size-1] = value };		\
+	module_param_array(name, charp, &name##_argc, 0644);	\
+	MODULE_PARM_DESC(name, desc)
+
+#define DEFAULT_MAX_PINNED	50
+MODULE_PARAM(int, max_pinned, DEFAULT_MAX_PINNED,
+	     "Maximum percent of physical memory that may be pinned");
+
+#define DEFAULT_WINDOW_SIZE	40
+MODULE_PARAM(int, window_size, DEFAULT_WINDOW_SIZE,
+	     "Maximum number of outstanding unacknowledged packets");
+
+#define DEFAULT_RMA_THRESHOLD	1024
+MODULE_PARAM(int, rma_threshold, DEFAULT_RMA_THRESHOLD,
+	     "Maximum message size sent through scif_send()");
+
+MODULE_PARAM(int, fast_rdma, 1,
+	     "Use scif_writeto()/scif_readfrom() directly for RDMA write/read");
+
+MODULE_PARAM(int, blocking_send, 0,
+	     "Use blocking version of scif_send()");
+
+MODULE_PARAM(int, blocking_recv, 1,
+	     "Use blocking version of scif_recv()");
+
+MODULE_PARAM(int, scif_loopback, 1,
+	     "Use SCIF lookback instead of kernel copy based loopback");
+
+MODULE_PARAM(int, host_proxy, 0,
+	     "Proxy card side RDMA operations to host");
+
+MODULE_PARAM(int, new_ib_type, 1,
+	     "Use new transport type dedicated to IBSCIF");
+
+MODULE_PARAM(int, verbose, 0,
+	     "Produce more log info for debugging purpose");
+
+MODULE_PARAM(int, check_grh, 1,
+	     "Detect outside-box connection by checking the global routing header");
+
+static atomic_t avail_pages; /* Calculated from max_pinned and totalram_pages */
+
+LIST_HEAD(devlist);
+DECLARE_MUTEX(devlist_mutex);
+
+DEFINE_IDR(wiremap);
+DEFINE_RWLOCK(wiremap_lock);
+static u32 reserved_0 = 0;
+
+void ibscif_dump(char *str, unsigned char* buf, int len)
+{
+	unsigned char *p, tmp[(16*3)+1];
+	int i;
+	return;
+	len = len > 64 ? 64 : len;
+	while (len) {
+		p = tmp;
+		for (i = len > 16 ? 16 : len; i; i--, len--) 
+			p += sprintf(p, "%2x ", *buf++);
+		printk("(%d)%s: %s\n", smp_processor_id(), str, tmp);
+	}
+}
+
+int ibscif_reserve_quota(int *npages)
+{
+	int c, old, err;
+
+	if (!*npages)
+		return 0;
+
+	err = 0;
+	c = atomic_read(&avail_pages);
+	for (;;) {
+		if (unlikely(c < *npages))
+			break;
+		old = atomic_cmpxchg(&avail_pages, c, c - *npages);
+		if (likely(old == c))
+			break;
+		c = old;
+	}
+
+	if (c < *npages) {
+		*npages = 0;
+		err = -EDQUOT;
+	}
+
+	return err;
+}
+
+void ibscif_release_quota(int npages)
+{
+	if (npages)
+		atomic_add(npages, &avail_pages);
+}
+
+/*
+ * To work around MPI's assumptions that data is written atomically in their
+ * header structures, write the first 16 integers of a transfer atomically.
+ *
+ * Update: the assumption of MPI's ofa module is different in that the last 
+ * four bytes needs to be written last and atomically. The buffers used in
+ * this case is always aligned.
+ */
+int ibscif_atomic_copy(void *dst_addr, void *src_addr, u32 copy_len, int head_copied)
+{
+	volatile int *src_x = (int *)src_addr;
+	volatile int *dst_x = (int *)dst_addr;
+	volatile u8  *src_c, *dst_c;
+	int head_aligned, tail_aligned;
+
+	if (unlikely(!copy_len))
+		return head_copied;
+
+	head_aligned =	!((unsigned long)src_addr & (sizeof(int)-1)) &&
+		  	!((unsigned long)dst_addr & (sizeof(int)-1)); 
+
+
+	tail_aligned =	!((unsigned long)(src_addr+copy_len) & (sizeof(int)-1)) &&
+		  	!((unsigned long)(dst_addr+copy_len) & (sizeof(int)-1)); 
+
+	if (!head_copied && head_aligned) {
+
+		switch (copy_len) {
+		case sizeof(int):
+			*dst_x = *src_x;
+			goto done;
+		case sizeof(int)*2:
+			*dst_x++ = *src_x++;
+			*dst_x	 = *src_x;
+			goto done;
+		case sizeof(int)*3:
+			*dst_x++ = *src_x++;
+			*dst_x++ = *src_x++;
+			*dst_x	 = *src_x;
+			goto done;
+		default:
+			if (copy_len >= (sizeof(int)*4)) {
+				/* We have at least a whole header to copy. */
+				head_copied = 1;
+				copy_len -= sizeof(int)*4;
+
+				*dst_x++ = *src_x++;
+				*dst_x++ = *src_x++;
+				*dst_x++ = *src_x++;
+
+				if (copy_len == 0) {
+					*dst_x = *src_x;
+					goto done;
+				}
+				*dst_x++ = *src_x++;
+			}
+			break;
+		}
+	}
+
+        /* The last integer is aligned. Copy all but the last int, then the last int */
+        if (tail_aligned && copy_len >= sizeof(int)) {
+                copy_len -= sizeof(int);
+                if (copy_len)
+                        memcpy((void *)dst_x, (void *)src_x, copy_len);
+                smp_wmb();
+                src_x = (volatile int *)((char *)src_x + copy_len);
+                dst_x = (volatile int *)((char *)dst_x + copy_len);
+                *dst_x = *src_x;
+                goto done;
+        }
+ 
+	/* Bad alignment. Copy all but the last byte, then the last byte */
+	if (--copy_len)
+		memcpy((void *)dst_x, (void *)src_x, copy_len);
+
+	src_c = ((volatile u8 *)src_x) + copy_len;
+	dst_c = ((volatile u8 *)dst_x) + copy_len;
+	smp_wmb();
+	*dst_c = *src_c;
+done:
+	return head_copied;
+}
+
+#if LINUX_VERSION_CODE>=KERNEL_VERSION(3,10,0)
+int ibscif_wiremap_add(void *obj, int *id)
+{
+	int ret;
+
+	write_lock_bh(&wiremap_lock);
+	ret = idr_alloc(&wiremap, obj, 0, 0, GFP_ATOMIC);
+	write_unlock_bh(&wiremap_lock);
+
+	if (ret < 0)
+		return ret;
+
+	*id = ret;
+
+	return 0;
+}
+#else
+/*
+ * Because idr_pre_get acquires the same internal spinlock used by idr_pre_get/idr_remove
+ * calls under a write_lock_bh, we need to call idr_pre_get with bottom half disabled.
+ * We cannot simply take the write_lock_bh(&wiremap_lock) because idr_pre_get does a
+ * blocking memory allocation call.  Since bh is disabled, mask must be GFP_ATOMIC.
+ */
+static inline int ibscif_wiremap_pre_get(void)
+{
+	int ret;
+
+	local_bh_disable();
+	ret = idr_pre_get(&wiremap, GFP_ATOMIC);
+	local_bh_enable();
+
+	return ret;
+}
+ 
+int ibscif_wiremap_add(void *obj, int *id)
+{
+	int ret;
+
+	do {
+		if (!ibscif_wiremap_pre_get())
+			return -ENOMEM;
+
+		write_lock_bh(&wiremap_lock);
+		ret = idr_get_new(&wiremap, obj, id);
+		write_unlock_bh(&wiremap_lock);
+	} while (ret == -EAGAIN);
+
+	return ret;
+}
+#endif
+
+void ibscif_wiremap_del(int id)
+{
+	write_lock_bh(&wiremap_lock);
+	idr_remove(&wiremap, id);
+	write_unlock_bh(&wiremap_lock);
+}
+
+static int ibscif_init_wiremap(void)
+{
+	int ret;
+	/*
+	 * Instead of treating them as opaque, some applications assert that returned key
+	 * values are non-zero.  As a work-around, reserve the first key from the wiremap.
+	 */
+#if LINUX_VERSION_CODE>=KERNEL_VERSION(3,10,0)
+	ret = idr_alloc(&wiremap, &reserved_0, 0, 1, GFP_KERNEL);
+#else
+	ret = ibscif_wiremap_add(&reserved_0, &reserved_0);
+#endif
+	BUG_ON(reserved_0 != 0);
+	return ret;
+}
+
+static void ibscif_free_wiremap(void)
+{
+#if LINUX_VERSION_CODE>=KERNEL_VERSION(3,10,0)
+	idr_destroy(&wiremap);
+#else
+	write_lock_bh(&wiremap_lock);
+	idr_remove_all(&wiremap);
+	idr_destroy(&wiremap);
+	write_unlock_bh(&wiremap_lock);
+#endif
+}
+
+static void ibscif_init_params(void)
+{
+	if ((max_pinned <= 0) || (max_pinned > 100)) {
+		max_pinned = DEFAULT_MAX_PINNED;
+		printk(KERN_WARNING PFX "Corrected max_pinned module parameter to %d.\n",
+		       max_pinned);
+	}
+	if (window_size < MIN_WINDOW_SIZE) {
+		window_size = MIN_WINDOW_SIZE;
+		printk(KERN_WARNING PFX "Corrected window_size module parameter to %d.\n",
+		       window_size);
+	}
+	if (rma_threshold < 0) {
+		rma_threshold = 0x7FFFFFFF;
+		printk(KERN_WARNING PFX "Corrected rma_threshold module parameter to %d.\n",
+		       rma_threshold);
+	}
+
+	/*
+	 * Hardware RDMA devices have built-in limits on the number of registered pages.
+	 * The avail_pages variable provides a limit for this software device.
+	 */
+	atomic_set(&avail_pages, max_pinned * (totalram_pages / 100));
+}
+
+static int __init ibscif_init(void)
+{
+	int err;
+
+	printk(KERN_INFO PFX "%s\n", ibscif_signon);
+	printk(KERN_INFO PFX "max_pinned=%d, window_size=%d, "
+			"blocking_send=%d, blocking_recv=%d, "
+			"fast_rdma=%d, "
+			"host_proxy=%d, "
+			"rma_threshold=%d, scif_loopback=%d, "
+			"new_ib_type=%d, verbose=%d, "
+			"check_grh=%d\n",
+			max_pinned, window_size,
+			blocking_send, blocking_recv,
+			fast_rdma,
+			host_proxy,
+			rma_threshold, scif_loopback,
+			new_ib_type, verbose,
+			check_grh);
+
+	ibscif_init_params();
+
+	err = ibscif_init_wiremap();
+	if (err)
+		return err;
+
+	err = ibscif_dev_init();
+	if (!err)
+		return 0;
+
+	ibscif_free_wiremap();
+	return err;
+}
+
+static void __exit ibscif_exit(void)
+{
+	ibscif_dev_cleanup();
+	ibscif_free_wiremap();
+	printk(KERN_INFO PFX "unloaded\n");
+}
+
+module_init(ibscif_init);
+module_exit(ibscif_exit);
diff -ruN a/drivers/infiniband/hw/scif/ibscif_mr.c b/drivers/infiniband/hw/scif/ibscif_mr.c
--- a/drivers/infiniband/hw/scif/ibscif_mr.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_mr.c	2016-04-14 13:33:08.862411195 -0700
@@ -0,0 +1,552 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+static int ibscif_mr_init_mreg(struct ibscif_mr *mr);
+
+struct ib_mr *ibscif_get_dma_mr(struct ib_pd *ibpd, int access)
+{
+	struct ibscif_dev *dev = to_dev(ibpd->device);
+	struct ibscif_mr *mr;
+	int err;
+
+	if (!atomic_add_unless(&dev->mr_cnt, 1, MAX_MRS))
+		return ERR_PTR(-EAGAIN);
+
+	mr = kzalloc(sizeof *mr, GFP_KERNEL);
+	if (!mr) {
+		err = -ENOMEM;
+		printk(KERN_ALERT PFX "%s: unable to allocate mr.\n", __func__); 
+		goto out1;
+	}
+
+	kref_init(&mr->ref);
+	init_completion(&mr->done);
+
+	err = ibscif_wiremap_add(mr, &mr->ibmr.lkey);
+	if (err) {
+		printk(KERN_ALERT PFX "%s: unable to allocate lkey.\n", __func__); 
+		goto out2;
+	}
+
+	if (mr->ibmr.lkey > IBSCIF_MR_MAX_KEY) {
+		err = -ENOSPC;
+		printk(KERN_ALERT PFX "%s: lkey (%x) out of range.\n", __func__, mr->ibmr.lkey); 
+		goto out3;
+	}
+
+	mr->ibmr.device = ibpd->device;		/* For ibscif_dereg_mr() calls below. */
+	mr->ibmr.rkey	= mr->ibmr.lkey;
+	mr->access	= access;
+	mr->magic	= MR_MAGIC;
+	INIT_LIST_HEAD(&mr->mreg_list);
+
+	return &mr->ibmr;
+
+out3:
+	ibscif_wiremap_del(mr->ibmr.lkey);
+out2:
+	kfree(mr);
+out1:
+	atomic_dec(&dev->mr_cnt);
+	return ERR_PTR(err);
+}
+
+struct ib_mr *ibscif_reg_phys_mr(struct ib_pd *ibpd, struct ib_phys_buf *phys_buf_array,
+				int num_phys_buf, int access, u64 *iova_start)
+{
+	struct ibscif_mr *mr;
+	struct ib_mr *ibmr;
+	int i, j, k, err;
+	u64 mask;
+
+	ibmr = ibscif_get_dma_mr(ibpd, access);
+	if (IS_ERR(ibmr))
+		return ibmr;
+
+	mr = to_mr(ibmr);
+	mr->addr = *iova_start;
+
+	mask = 0;
+	for (i = 0; i < num_phys_buf; i++) {
+		if (i != 0)
+			mask |= phys_buf_array[i].addr;				 /* All but 1st are aligned    */
+		if (i != num_phys_buf - 1)
+			mask |= phys_buf_array[i].addr + phys_buf_array[i].size; /* Middle bufs are full pages */
+
+		mr->length += phys_buf_array[i].size;
+	}
+	if ((mask & ~PAGE_MASK) || (mr->length > MAX_MR_SIZE)) {
+		err = -EINVAL;
+		goto out;
+	}
+	if (mr->length && ((mr->addr + mr->length - 1) < mr->addr)) {
+		err = -EOVERFLOW;
+		goto out;
+	}
+
+	phys_buf_array[0].size += phys_buf_array[0].addr & ~PAGE_MASK;	/* Adjust 1st buf size by page offset */
+	phys_buf_array[0].addr &= PAGE_MASK;				/* Truncate 1st buf to start of page  */
+
+	for (i = 0; i < num_phys_buf; i++)
+		mr->npages += PAGE_ALIGN(phys_buf_array[i].size) >> PAGE_SHIFT;
+
+	if (!mr->npages)
+		return &mr->ibmr;
+
+	err = ibscif_reserve_quota(&mr->npages);
+	if (err)
+		goto out;
+
+	mr->page = vzalloc(mr->npages * sizeof *mr->page);
+	if (!mr->page) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	k = 0;
+	for (i = 0; i < num_phys_buf; i++)
+		for (j = 0; j < PAGE_ALIGN(phys_buf_array[i].size) >> PAGE_SHIFT; j++)
+			mr->page[k++] = pfn_to_page((phys_buf_array[i].addr >> PAGE_SHIFT) + j);
+
+	return &mr->ibmr;
+out:
+	ibscif_dereg_mr(ibmr);
+	return ERR_PTR(err);
+}
+
+#ifdef	MOFED
+struct ib_mr *ibscif_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 length,
+				u64 virt_addr, int access, struct ib_udata *udata, int mr_id)
+#else
+struct ib_mr *ibscif_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 length,
+				u64 virt_addr, int access, struct ib_udata *udata)
+#endif
+{
+	struct ib_mr *ibmr;
+	struct ibscif_mr *mr;
+	struct scatterlist *sg;
+	struct ibscif_dev *dev;
+	int i, k, err;
+
+	if (length && ((start + length - 1) < start))
+		return ERR_PTR(-EOVERFLOW);
+
+	ibmr = ibscif_get_dma_mr(ibpd, access);
+	if (IS_ERR(ibmr))
+		return ibmr;
+
+	mr = to_mr(ibmr);
+	mr->addr = start;
+
+	mr->umem = ib_umem_get(ibpd->uobject->context, start, length, access, 0/*dma_sync*/);
+	if (IS_ERR(mr->umem)) {
+		err = PTR_ERR(mr->umem);
+		printk(KERN_ALERT PFX "%s: ib_umem_get returns %d.\n", __func__, err); 
+		goto out;
+	}
+
+	mr->npages = ib_umem_page_count(mr->umem);
+	if (!mr->npages)
+		return &mr->ibmr;
+
+	mr->length = mr->umem->length;
+
+	err = ibscif_reserve_quota(&mr->npages);
+	if (err)
+		goto out;
+
+	mr->page = vzalloc(mr->npages * sizeof *mr->page);
+	if (!mr->page) {
+		err = -ENOMEM;
+		printk(KERN_ALERT PFX "%s: unable to allocate mr->page.\n", __func__); 
+		goto out;
+	}
+
+	k = 0;
+	for_each_sg(mr->umem->sg_head.sgl, sg, mr->umem->nmap, i)
+		mr->page[k++] = sg_page(sg);
+
+	err = ibscif_mr_init_mreg(mr);
+	if (err) 
+		goto out;
+
+ 	dev = to_dev(mr->ibmr.device);
+	down(&dev->mr_list_mutex);
+	list_add_tail(&mr->entry, &dev->mr_list);
+	up(&dev->mr_list_mutex);
+
+	return &mr->ibmr;
+out:
+	ibscif_dereg_mr(ibmr);
+	return ERR_PTR(err);
+}
+
+void ibscif_complete_mr(struct kref *ref)
+{
+	struct ibscif_mr *mr = container_of(ref, struct ibscif_mr, ref);
+	complete(&mr->done);
+}
+
+int ibscif_dereg_mr(struct ib_mr *ibmr)
+{
+	struct ibscif_dev *dev = to_dev(ibmr->device);
+	struct ibscif_mr *mr = to_mr(ibmr);
+	struct ibscif_mreg_info *mreg, *next;
+	struct ibscif_mr *mr0, *next0;
+	int ret;
+
+	ibscif_put_mr(mr);
+	wait_for_completion(&mr->done);
+
+	list_for_each_entry_safe(mreg, next, &mr->mreg_list, entry) {
+		do {
+			ret = scif_unregister(mreg->conn->ep, mreg->aligned_offset, mreg->aligned_length);
+		}
+		while (ret == -ERESTARTSYS);
+
+		if (ret && ret != -ENOTCONN) 
+			printk(KERN_ALERT PFX "%s: scif_unregister returns %d. ep=%p, offset=%llx, length=%x\n", 
+				__func__, ret, mreg->conn->ep, mreg->aligned_offset, mreg->aligned_length);
+
+		ibscif_put_conn(mreg->conn);
+		list_del(&mreg->entry);
+		kfree(mreg);
+	}
+
+	down(&dev->mr_list_mutex);
+	list_for_each_entry_safe(mr0, next0, &dev->mr_list, entry) {
+		if (mr0 == mr) {
+			list_del(&mr0->entry);
+			break;
+		}
+	}
+	up(&dev->mr_list_mutex);
+
+	if (mr->pinned_pages)
+		scif_unpin_pages(mr->pinned_pages);
+
+	if (mr->umem && !IS_ERR(mr->umem))
+		ib_umem_release(mr->umem);
+	if (mr->page)
+		vfree(mr->page);
+
+	ibscif_release_quota(mr->npages);
+	atomic_dec(&dev->mr_cnt);
+
+	ibscif_wiremap_del(mr->ibmr.lkey);
+
+	kfree(mr);
+	return 0;
+}
+
+/*
+ * Lookup and validate the given memory region access.  A reference is held on success.
+ */
+struct ibscif_mr *ibscif_validate_mr(u32 key, u64 addr, int length,
+				   struct ib_pd *ibpd, enum ib_access_flags access)
+{
+	struct ibscif_mr *mr;
+	int err;
+
+	mr = ibscif_get_mr(key);
+	if (unlikely(IS_ERR(mr)))
+		return mr;
+
+	if (unlikely(mr->ibmr.pd != ibpd)) {
+		err = -EPERM;
+		goto out;
+	}
+	if (unlikely(access && !(mr->access & access))) {
+		err = -EACCES;
+		goto out;
+	}
+	if (unlikely((addr < mr->addr) || ((addr + length) > (mr->addr + mr->length)))) {
+		err = -ERANGE;
+		goto out;
+	}
+
+	return mr;
+out:
+	ibscif_put_mr(mr);
+	return ERR_PTR(err);
+}
+
+static void ibscif_dma_nop(struct ib_device *ibdev, u64 addr, size_t size, enum dma_data_direction direction)
+{
+}
+
+static int ibscif_mapping_error(struct ib_device *ibdev, u64 dma_addr)
+{
+	return !dma_addr;
+}
+
+static u64 ibscif_dma_map_single(struct ib_device *ibdev, void *cpu_addr, size_t size,
+			        enum dma_data_direction direction)
+{
+	return (u64)cpu_addr;
+}
+
+static u64 ibscif_dma_map_page(struct ib_device *ibdev, struct page *page, unsigned long offset, size_t size,
+			      enum dma_data_direction direction)
+{
+	u64 addr;
+
+	if (offset + size > PAGE_SIZE)
+		return 0;
+
+	addr = (u64)page_address(page);
+	if (addr)
+		addr += offset;
+
+	return addr;
+}
+
+static int ibscif_map_sg(struct ib_device *ibdev, struct scatterlist *sg, int nents,
+			enum dma_data_direction direction)
+{
+	u64 addr;
+	int i, ret = nents;
+
+	for (i = 0; i < nents; i++, sg++) {
+		addr = (u64)page_address(sg_page(sg));
+		if (!addr) {
+			ret = 0;
+			break;
+		}
+
+		sg->dma_address = sg->offset + addr;
+		sg->dma_length  = sg->length;
+	}
+	return ret;
+}
+
+static void ibscif_unmap_sg(struct ib_device *ibdev, struct scatterlist *sg, int nents,
+			   enum dma_data_direction direction)
+{
+}
+
+static void ibscif_sync_single(struct ib_device *ibdev, u64 dma, size_t size,
+			       enum dma_data_direction direction)
+{
+}
+
+static void *ibscif_dma_alloc_coherent(struct ib_device *ibdev, size_t size, u64 *dma_handle, gfp_t flag)
+{
+	struct page *p = alloc_pages(flag, get_order(size));
+	void *addr = p ? page_address(p) : NULL;
+
+	if (dma_handle)
+		*dma_handle = (u64)addr;
+
+	return addr;
+}
+
+static void ibscif_dma_free_coherent(struct ib_device *ibdev, size_t size, void *cpu_addr, u64 dma_handle)
+{
+	free_pages((unsigned long)cpu_addr, get_order(size));
+}
+
+struct ib_dma_mapping_ops ibscif_dma_mapping_ops = {
+	ibscif_mapping_error,
+	ibscif_dma_map_single,
+	ibscif_dma_nop,
+	ibscif_dma_map_page,
+	ibscif_dma_nop,
+	ibscif_map_sg,
+	ibscif_unmap_sg,
+	ibscif_sync_single,
+	ibscif_sync_single,
+	ibscif_dma_alloc_coherent,
+	ibscif_dma_free_coherent
+};
+
+static void ibscif_dump_mr_list( struct ibscif_dev *dev )
+{
+	struct ibscif_mr *mr;
+
+	list_for_each_entry(mr, &dev->mr_list, entry){
+		printk(KERN_ALERT PFX "%s: mr=%p [%llx, %x, %x]\n", __func__, mr, mr->addr, mr->length, mr->ibmr.rkey);
+	}
+}
+
+static int ibscif_mr_reg_with_conn(struct ibscif_mr *mr, struct ibscif_conn *conn, struct ibscif_mreg_info **new_mreg)
+{
+	struct ibscif_mreg_info *mreg;
+	off_t offset, aligned_offset;
+	u64 aligned_addr;
+	int aligned_length;
+	int offset_in_page;
+	int err;
+
+	aligned_addr = mr->addr & PAGE_MASK;
+	offset_in_page = (int)(mr->addr & ~PAGE_MASK);
+	aligned_length = (mr->length + offset_in_page + PAGE_SIZE - 1) & PAGE_MASK;
+	aligned_offset = IBSCIF_MR_VADDR_TO_OFFSET(mr->ibmr.rkey, aligned_addr);
+
+	offset = scif_register_pinned_pages(conn->ep, mr->pinned_pages, aligned_offset, SCIF_MAP_FIXED);
+
+	if (IS_ERR_VALUE(offset)) {
+		printk(KERN_ALERT PFX "%s: scif_register_pinned_pages returns %d\n", __func__, (int)offset);
+		printk(KERN_ALERT PFX "%s: conn=%p, ep=%p, mr=%p, addr=%llx, length=%x, rkey=%x, "
+			"aligned_addr=%llx, aligned_length=%x, aligned_offset=%llx\n", 
+			__func__, conn, conn->ep, mr, mr->addr, mr->length, mr->ibmr.rkey,
+			aligned_addr, aligned_length, (uint64_t)aligned_offset);
+		ibscif_dump_mr_list(conn->dev);
+		return (int)offset;
+	}
+
+	BUG_ON(offset != aligned_offset);
+
+	offset += offset_in_page;
+
+	mreg = kzalloc(sizeof(struct ibscif_mreg_info), GFP_KERNEL);
+	if (!mreg) {
+		do {
+			err = scif_unregister(conn->ep, aligned_offset, aligned_length);
+		}
+		while (err == -ERESTARTSYS);
+
+		if (err && err != -ENOTCONN) 
+			printk(KERN_ALERT PFX "%s: scif_unregister returns %d. ep=%p, offset=%llx, length=%x\n",
+				__func__, err, conn->ep, (uint64_t)aligned_offset, aligned_length);
+
+		return -ENOMEM;
+	}
+	mreg->conn = conn;
+	mreg->offset = (u64)offset;
+	mreg->aligned_offset = aligned_offset;
+	mreg->aligned_length = aligned_length;
+	list_add_tail(&mreg->entry, &mr->mreg_list);
+
+	atomic_inc(&conn->refcnt);
+	if (conn->local_close) {
+		conn->local_close = 0;
+		ibscif_send_reopen(conn);
+	}
+
+	if (new_mreg) 
+		*new_mreg = mreg;
+
+	return 0;
+}
+
+struct ibscif_mreg_info *ibscif_mr_get_mreg(struct ibscif_mr *mr, struct ibscif_conn *conn)
+{
+	struct ibscif_mreg_info *mreg;
+	int err;
+	int i;
+
+	if (unlikely(!conn)) {
+		printk(KERN_ALERT PFX "%s: conn==NULL\n", __func__);
+		return NULL;
+	}
+
+	list_for_each_entry(mreg, &mr->mreg_list, entry){
+		if (mreg->conn == conn)
+			return mreg;
+	}
+
+	mreg = NULL;
+	err = ibscif_mr_reg_with_conn(mr, conn, &mreg);
+	if (err != -EADDRINUSE)
+		return mreg;
+		
+	/* another thread is performing the registration */
+	if (verbose)
+		printk(KERN_INFO PFX "%s: mr is being registered by another thread. mr=%p, conn=%p.\n", __func__, mr, conn);
+	for (i=0; i<10000; i++) {
+		list_for_each_entry(mreg, &mr->mreg_list, entry){
+			if (mreg->conn == conn) {
+				if (verbose)
+					printk(KERN_INFO PFX "%s: got mreg after %d retries.\n", __func__, i+1);
+				return mreg;
+			}
+		}
+		schedule();
+	}
+	if (verbose)
+		printk(KERN_INFO PFX "%s: failed to get mreg after %d retries.\n", __func__, i);
+	return NULL;
+}
+
+static int ibscif_mr_init_mreg(struct ibscif_mr *mr)
+{
+	struct ibscif_dev *dev = to_dev(mr->ibmr.device);
+	struct ibscif_conn *conn;
+	int prot;
+	u64 aligned_addr;
+	int aligned_length;
+	int offset_in_page;
+	int err;
+
+	aligned_addr = mr->addr & PAGE_MASK;
+	offset_in_page = (int)(mr->addr & ~PAGE_MASK);
+	aligned_length = (mr->length + offset_in_page + PAGE_SIZE - 1) & PAGE_MASK;
+
+#if 0
+	prot =  ((mr->access & IB_ACCESS_REMOTE_READ)?SCIF_PROT_READ:0) |
+		((mr->access & IB_ACCESS_REMOTE_WRITE)?SCIF_PROT_WRITE:0);
+#else
+  	// In IB, the same buffer can be registered multiple times with different access rights. 
+  	// SCIF doesn't have mechanism to support that. So we just turn on all the access rights.
+  	// Otherwise we may end up with protection error.
+	prot = SCIF_PROT_READ | SCIF_PROT_WRITE;
+#endif
+
+	err = scif_pin_pages((void *)aligned_addr, aligned_length, prot, 0/*user addr*/, &mr->pinned_pages);
+	if (err) {
+		printk(KERN_ALERT PFX "%s: scif_pin_pages returns %d\n", __func__, err);
+		return err;
+	}
+
+	down(&dev->mutex);
+	list_for_each_entry(conn, &dev->conn_list, entry) {
+		err = ibscif_mr_reg_with_conn(mr, conn, NULL);
+		if (err)
+			break;
+	}
+	up(&dev->mutex);
+
+	return err;
+}
+
+void ibscif_refresh_mreg( struct ibscif_conn *conn )
+{
+	struct ibscif_mr *mr;
+
+	down(&conn->dev->mr_list_mutex);
+	list_for_each_entry(mr, &conn->dev->mr_list, entry){
+		ibscif_mr_get_mreg(mr, conn);
+	}
+	up(&conn->dev->mr_list_mutex);
+}
+
diff -ruN a/drivers/infiniband/hw/scif/ibscif_pd.c b/drivers/infiniband/hw/scif/ibscif_pd.c
--- a/drivers/infiniband/hw/scif/ibscif_pd.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_pd.c	2016-04-14 13:33:08.862411195 -0700
@@ -0,0 +1,56 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+struct ib_pd *ibscif_alloc_pd(struct ib_device *ibdev, struct ib_ucontext *context, struct ib_udata *udata)
+{
+	struct ibscif_dev *dev = to_dev(ibdev);
+	struct ibscif_pd *pd;
+
+	if (!atomic_add_unless(&dev->pd_cnt, 1, MAX_PDS))
+		return ERR_PTR(-EAGAIN);
+
+	pd = kzalloc(sizeof *pd, GFP_KERNEL);
+	if (!pd) {
+		atomic_dec(&dev->pd_cnt);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	return &pd->ibpd;
+}
+
+int ibscif_dealloc_pd(struct ib_pd *ibpd)
+{
+	struct ibscif_dev *dev = to_dev(ibpd->device);
+	atomic_dec(&dev->pd_cnt);
+	kfree(to_pd(ibpd));
+	return 0;
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_post.c b/drivers/infiniband/hw/scif/ibscif_post.c
--- a/drivers/infiniband/hw/scif/ibscif_post.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_post.c	2016-04-14 13:34:27.551410310 -0700
@@ -0,0 +1,313 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+void ibscif_dump_sg(char *str, struct ib_sge *sge, int num)
+{
+	extern void ibscif_dump(char*, void*, int);
+	if (!sge)
+		return;
+	while (num--) {
+		ibscif_dump(str, (void*)sge->addr, sge->length);
+		sge++;
+	}
+}
+
+/*
+ * Build and validate the wr->ds_list from the given sg_list.
+ * If successful, a reference is held on each mr in the wr->ds_list.
+ */
+static int ibscif_wr_ds(struct ib_pd *ibpd, struct ib_sge *sg_list, int num_sge,
+		       struct ibscif_wr *wr, int *total_length, enum ib_access_flags access)
+{
+	struct ibscif_ds *ds_list = wr->ds_list;
+	int err;
+
+	*total_length = 0;
+	for (wr->num_ds = 0; wr->num_ds < num_sge; sg_list++, ds_list++) {
+
+		ds_list->mr = ibscif_validate_mr(sg_list->lkey, sg_list->addr, sg_list->length, ibpd, access);
+		if (unlikely(IS_ERR(ds_list->mr))) {
+			err = PTR_ERR(ds_list->mr);
+			goto out;
+		}
+
+		ds_list->in_use = 1;
+		wr->num_ds++;
+
+		if (unlikely((*total_length + sg_list->length) < *total_length)) {
+			err = -EOVERFLOW;
+			goto out;
+		}
+
+		ds_list->offset = sg_list->addr - ds_list->mr->addr;
+		ds_list->length = sg_list->length;
+		ds_list->lkey   = sg_list->lkey;
+		ds_list->current_mreg = NULL;
+
+		*total_length += ds_list->length;
+	}
+
+	return 0;
+out:
+	ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+	return err;
+}
+
+int ibscif_post_send(struct ib_qp *ibqp, struct ib_send_wr *ibwr, struct ib_send_wr **bad_wr)
+{
+	struct ibscif_qp *qp = to_qp(ibqp);
+	struct ibscif_wq *sq = &qp->sq;
+	struct ibscif_wr *wr;
+	int nreq = 0, err;
+
+	IBSCIF_PERF_SAMPLE(0, 0);
+
+	spin_lock_bh(&sq->lock);
+
+	if (unlikely(ibqp->qp_type != IB_QPT_UD && qp->state != QP_CONNECTED)) {
+		err = -ENOTCONN;
+		goto out;
+	}
+	if (unlikely(!sq->size)) {
+		err = -ENOSPC;
+		goto out;
+	}
+
+	for (err = 0; ibwr; ibwr = ibwr->next, nreq++) {
+
+		if (unlikely(sq->depth == sq->size)) {
+			err = -ENOBUFS;
+			goto out;
+		}
+		if (unlikely(ibwr->num_sge > sq->max_sge)) {
+			err = -E2BIG;
+			goto out;
+		}
+
+		wr = ibscif_get_wr(sq, sq->tail);
+
+		memset(&wr->sar, 0, sizeof wr->sar);
+
+		wr->id	   = ibwr->wr_id;
+		wr->opcode = ibwr->opcode;
+		wr->flags  = ibwr->send_flags | ((qp->sq_policy == IB_SIGNAL_ALL_WR) ? IB_SEND_SIGNALED : 0);
+		wr->state  = WR_WAITING;
+		wr->use_rma = 0;
+		wr->rma_id = 0;
+
+		if (ibqp->qp_type == IB_QPT_UD) {
+			wr->opcode = WR_UD;
+			wr->ud.remote_node_id = IBSCIF_LID_TO_NODE_ID(be16_to_cpu(to_ah(ibwr->wr.ud.ah)->dlid));
+			wr->ud.remote_qpn = ibwr->wr.ud.remote_qpn;
+
+			/* the remainings are the same as IB_WR_SEND */
+			err = ibscif_wr_ds(ibqp->pd, ibwr->sg_list, ibwr->num_sge, wr, &wr->length, 0);
+			if (unlikely(err))
+				goto out;
+
+			if (wr->length > IBSCIF_MTU) {
+				ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+				err = -EMSGSIZE;
+				goto out;
+			}
+
+			wr->msg_id = sq->wirestate->tx.next_msg_id++;
+		}
+
+		else switch (ibwr->opcode) {
+
+		case IB_WR_SEND_WITH_IMM:
+			wr->send.immediate_data = ibwr->ex.imm_data;
+		case IB_WR_SEND:
+			err = ibscif_wr_ds(ibqp->pd, ibwr->sg_list, ibwr->num_sge, wr, &wr->length, 0);
+			if (unlikely(err))
+				goto out;
+			wr->msg_id = sq->wirestate->tx.next_msg_id++;
+			if (wr->length > rma_threshold) {
+				wr->use_rma = 1;
+				wr->rma_id = sq->next_msg_id;
+			}
+			break;
+
+		case IB_WR_RDMA_WRITE_WITH_IMM:
+			wr->msg_id = sq->wirestate->tx.next_msg_id++;
+			wr->write.immediate_data = ibwr->ex.imm_data;
+		case IB_WR_RDMA_WRITE:
+			err = ibscif_wr_ds(ibqp->pd, ibwr->sg_list, ibwr->num_sge, wr, &wr->length, 0);
+			if (unlikely(err))
+				goto out;
+			if (wr->length &&
+			    ((ibwr->wr.rdma.remote_addr + wr->length - 1) < ibwr->wr.rdma.remote_addr)) {
+				err = -EOVERFLOW;
+				goto out;
+			}
+			wr->write.remote_address = ibwr->wr.rdma.remote_addr;
+			wr->write.rkey		 = ibwr->wr.rdma.rkey;
+			if (ibwr->opcode == IB_WR_RDMA_WRITE)
+				wr->msg_id = 0;
+			if (wr->length > rma_threshold) {
+				wr->use_rma = 1;
+				wr->rma_id = sq->next_msg_id;
+			}
+			break;
+
+		case IB_WR_RDMA_READ:
+			if (unlikely(!qp->max_or)) {
+				err = -ENOBUFS;
+				goto out;
+			}
+			err = ibscif_wr_ds(ibqp->pd, ibwr->sg_list, ibwr->num_sge, wr, &wr->length, IB_ACCESS_LOCAL_WRITE);
+			if (unlikely(err))
+				goto out;
+			if (wr->length &&
+			    ((ibwr->wr.rdma.remote_addr + wr->length - 1) < ibwr->wr.rdma.remote_addr)) {
+				err = -EOVERFLOW;
+				goto out;
+			}
+			wr->read.remote_address = ibwr->wr.rdma.remote_addr;
+			wr->read.remote_length	= wr->length;
+			wr->read.rkey		= ibwr->wr.rdma.rkey;
+			wr->length		= 0;	  /* No tx data with this opcode */
+			wr->msg_id		= sq->next_msg_id;
+			atomic_inc(&qp->or_posted);
+			if (wr->read.remote_length > rma_threshold) {
+				wr->use_rma = 1;
+				wr->rma_id = wr->msg_id;
+			}
+			break;
+
+		case IB_WR_ATOMIC_CMP_AND_SWP:
+		case IB_WR_ATOMIC_FETCH_AND_ADD:
+			if (unlikely(!qp->max_or)) {
+				err = -ENOBUFS;
+				goto out;
+			}
+			if (unlikely(ibwr->wr.atomic.remote_addr & (sizeof wr->atomic_rsp.orig_data - 1))) {
+				err = -EADDRNOTAVAIL;
+				goto out;
+			}
+			err = ibscif_wr_ds(ibqp->pd, ibwr->sg_list, ibwr->num_sge, wr, &wr->length, IB_ACCESS_LOCAL_WRITE);
+			if (unlikely(err))
+				goto out;
+			if (unlikely(wr->length < sizeof wr->atomic_rsp.orig_data)) {
+				err = -EINVAL;
+				goto out;
+			}
+			if (ibwr->opcode == IB_WR_ATOMIC_CMP_AND_SWP) {
+				wr->cmp_swp.cmp_operand    = ibwr->wr.atomic.compare_add;
+				wr->cmp_swp.swp_operand    = ibwr->wr.atomic.swap;
+				wr->cmp_swp.remote_address = ibwr->wr.atomic.remote_addr;
+				wr->cmp_swp.rkey	   = ibwr->wr.atomic.rkey;
+			} else {
+				wr->fetch_add.add_operand    = ibwr->wr.atomic.compare_add;
+				wr->fetch_add.remote_address = ibwr->wr.atomic.remote_addr;
+				wr->fetch_add.rkey	     = ibwr->wr.atomic.rkey;
+			}
+			wr->length = 0; /* No tx data with these opcodes */
+			wr->msg_id = sq->next_msg_id;
+			atomic_inc(&qp->or_posted);
+			break;
+
+		default:
+			err = -ENOMSG;
+			goto out;
+		}
+
+		DEV_STAT(qp->dev, wr_opcode[wr->opcode]++);
+		ibscif_append_wq(sq);
+	}
+out:
+	spin_unlock_bh(&sq->lock);
+
+	IBSCIF_PERF_SAMPLE(1, 0);
+
+	if (err)
+		*bad_wr = ibwr;
+	if (nreq)
+		ibscif_schedule(sq);
+
+	IBSCIF_PERF_SAMPLE(9, 1);
+
+	return err;
+}
+
+int ibscif_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *ibwr, struct ib_recv_wr **bad_wr)
+{
+	struct ibscif_qp *qp = to_qp(ibqp);
+	struct ibscif_wq *rq = &qp->rq;
+	struct ibscif_wr *wr;
+	int err;
+
+	spin_lock_bh(&rq->lock);
+
+	if ((qp->state != QP_IDLE) && (qp->state != QP_CONNECTED)) {
+		err = -ENOTCONN;
+		goto out;
+	}
+	if (unlikely(!rq->size)) {
+		err = -ENOSPC;
+		goto out;
+	}
+
+	for (err = 0; ibwr; ibwr = ibwr->next) {
+ 
+		if (unlikely(rq->depth == rq->size)) {
+			err = -ENOBUFS;
+			goto out;
+		}
+		if (unlikely(ibwr->num_sge > rq->max_sge)) {
+			err = -E2BIG;
+			goto out;
+		}
+
+		wr = ibscif_get_wr(rq, rq->tail);
+
+		memset(&wr->sar, 0, sizeof wr->sar);
+
+		wr->id	   = ibwr->wr_id;
+		wr->msg_id = rq->next_msg_id;
+		wr->state  = WR_WAITING;
+
+		err = ibscif_wr_ds(ibqp->pd, ibwr->sg_list, ibwr->num_sge, wr, &wr->length, IB_ACCESS_LOCAL_WRITE);
+		ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+		if (unlikely(err))
+			goto out;
+
+		ibscif_append_wq(rq);
+	}
+out:
+	spin_unlock_bh(&rq->lock);
+	if (err)
+		*bad_wr = ibwr;
+
+	return err;
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_procfs.c b/drivers/infiniband/hw/scif/ibscif_procfs.c
--- a/drivers/infiniband/hw/scif/ibscif_procfs.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_procfs.c	2016-04-14 13:33:08.863411163 -0700
@@ -0,0 +1,180 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0))
+static int ibscif_stats_show(struct seq_file *m, void *v)
+#else
+static int ibscif_stats_read(char *page, char **start, off_t offset,
+			     int count, int *eof, void *data)
+#endif
+{
+	int l = 0;
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0))
+	struct ibscif_dev *dev = m->private;
+#else
+	struct ibscif_dev *dev = data;
+	char *m = page;
+
+	if (offset)
+		return 0;
+
+	*eof = 1;
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0))
+	seq_printf
+#else
+	l += sprintf
+#endif
+		(m,
+		"%s statistics:\n"
+		"    tx_bytes %lu rx_bytes %lu\n"
+		"    tx_pkts %lu rx_pkts %lu loopback_pkts %lu\n"
+		"    sched_exhaust %lu unavailable %lu\n"
+		"    tx_errors %lu duplicates %lu\n"
+		"    total wr %lu :\n"
+		"        send %lu send_imm %lu write %lu write_imm %lu\n"
+		"        recv %lu recv_imm %lu read %lu comp %lu fetch %lu\n"
+		"        read_rsp %lu atomic_rsp %lu ud %lu\n"
+		"    fast_rdma :\n"
+		"        write %lu read %lu unavailable %lu fallback %lu force_ack %lu tail_write %lu\n",
+		dev->ibdev.name,
+		DEV_STAT(dev, bytes_sent),
+		DEV_STAT(dev, bytes_rcvd),
+		DEV_STAT(dev, packets_sent),
+		DEV_STAT(dev, packets_rcvd),
+		DEV_STAT(dev, loopback),
+		DEV_STAT(dev, sched_exhaust),
+		DEV_STAT(dev, unavailable),
+		DEV_STAT(dev, tx_errors),
+		DEV_STAT(dev, duplicates),
+		DEV_STAT(dev, wr_opcode[WR_SEND])			+
+		DEV_STAT(dev, wr_opcode[WR_SEND_WITH_IMM])		+
+		DEV_STAT(dev, wr_opcode[WR_RDMA_WRITE])			+
+		DEV_STAT(dev, wr_opcode[WR_RDMA_WRITE_WITH_IMM])	+
+		DEV_STAT(dev, recv)					+
+		DEV_STAT(dev, recv_imm)					+
+		DEV_STAT(dev, wr_opcode[WR_RDMA_READ])			+
+		DEV_STAT(dev, wr_opcode[WR_ATOMIC_CMP_AND_SWP])		+
+		DEV_STAT(dev, wr_opcode[WR_ATOMIC_FETCH_AND_ADD])	+
+		DEV_STAT(dev, wr_opcode[WR_RDMA_READ_RSP])		+
+		DEV_STAT(dev, wr_opcode[WR_ATOMIC_RSP]),
+		DEV_STAT(dev, wr_opcode[WR_SEND]),
+		DEV_STAT(dev, wr_opcode[WR_SEND_WITH_IMM]),
+		DEV_STAT(dev, wr_opcode[WR_RDMA_WRITE]),
+		DEV_STAT(dev, wr_opcode[WR_RDMA_WRITE_WITH_IMM]),
+		DEV_STAT(dev, recv),
+		DEV_STAT(dev, recv_imm),
+		DEV_STAT(dev, wr_opcode[WR_RDMA_READ]),
+		DEV_STAT(dev, wr_opcode[WR_ATOMIC_CMP_AND_SWP]),
+		DEV_STAT(dev, wr_opcode[WR_ATOMIC_FETCH_AND_ADD]),
+		DEV_STAT(dev, wr_opcode[WR_RDMA_READ_RSP]),
+		DEV_STAT(dev, wr_opcode[WR_ATOMIC_RSP]),
+		DEV_STAT(dev, wr_opcode[WR_UD]),
+		DEV_STAT(dev, fast_rdma_write),
+		DEV_STAT(dev, fast_rdma_read),
+		DEV_STAT(dev, fast_rdma_unavailable),
+		DEV_STAT(dev, fast_rdma_fallback),
+		DEV_STAT(dev, fast_rdma_force_ack),
+		DEV_STAT(dev, fast_rdma_tail_write)
+		);
+
+	return l;
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0))
+static ssize_t ibscif_stats_write(struct file *file, const char __user *buffer,
+				 size_t count, loff_t *ppos)
+{
+       struct ibscif_dev *dev = PDE_DATA(file_inode(file));
+       memset(&dev->stats, 0, sizeof dev->stats);
+       return count;
+}
+
+static int ibscif_stats_open(struct inode *inode, struct file *file)
+{
+       return single_open(file, ibscif_stats_show, PDE_DATA(inode));
+}
+
+struct file_operations ibscif_fops = {
+       .owner = THIS_MODULE,
+       .open = ibscif_stats_open,
+       .read = seq_read,
+       .write = ibscif_stats_write,
+       .llseek = seq_lseek,
+       .release = seq_release,
+};
+
+int ibscif_procfs_add_dev(struct ibscif_dev *dev)
+{
+       dev->procfs = proc_mkdir(dev->ibdev.name, init_net.proc_net);
+       if (!dev->procfs)
+	       return -ENOENT;
+
+       if (proc_create_data("stats", S_IRUGO | S_IWUGO, dev->procfs,
+			    &ibscif_fops ,dev))
+	       return -ENOENT;
+
+       return 0;
+}
+#else /* (LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)) */
+static int ibscif_stats_write(struct file *file, const char __user *buffer, unsigned long count, void *data)
+{
+	struct ibscif_dev *dev = data;
+	memset(&dev->stats, 0, sizeof dev->stats);
+	return count;
+}
+
+int ibscif_procfs_add_dev(struct ibscif_dev *dev)
+{
+	struct proc_dir_entry *entry;
+
+	dev->procfs = proc_mkdir(dev->ibdev.name, init_net.proc_net);
+	if (!dev->procfs)
+		return -ENOENT;
+
+	entry = create_proc_read_entry("stats", S_IRUGO | S_IWUGO, dev->procfs, ibscif_stats_read, dev);
+	if (!entry)
+		return -ENOENT;
+	entry->write_proc = ibscif_stats_write;
+
+	return 0;
+}
+#endif
+
+void ibscif_procfs_remove_dev(struct ibscif_dev *dev)
+{
+	if (dev->procfs)
+		remove_proc_entry("stats", dev->procfs);
+	remove_proc_entry(dev->ibdev.name, init_net.proc_net);
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_protocol.c b/drivers/infiniband/hw/scif/ibscif_protocol.c
--- a/drivers/infiniband/hw/scif/ibscif_protocol.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_protocol.c	2016-04-14 13:34:27.552410331 -0700
@@ -0,0 +1,2819 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+#include <linux/sched.h>
+/* dev/wr/qp backpointers overlayed in skb cb[] */
+struct ibscif_skb_cb {
+	struct ibscif_dev	*dev;
+	struct ibscif_wr	*wr;
+	scif_epd_t 		scif_ep;
+	struct ibscif_qp	*qp;	/* for UD only */
+};
+
+#define SET_SKB_DEV(skb,dev0)	((struct ibscif_skb_cb *)&skb->cb)->dev = dev0
+#define SET_SKB_WR(skb,wr0)	((struct ibscif_skb_cb *)&skb->cb)->wr = wr0
+#define SET_SKB_EP(skb,ep0)	((struct ibscif_skb_cb *)&skb->cb)->scif_ep = ep0
+#define SET_SKB_QP(skb,qp0)	((struct ibscif_skb_cb *)&skb->cb)->qp = qp0
+
+#define GET_SKB_DEV(skb)	((struct ibscif_skb_cb *)&skb->cb)->dev
+#define GET_SKB_WR(skb)		((struct ibscif_skb_cb *)&skb->cb)->wr
+#define GET_SKB_EP(skb)		((struct ibscif_skb_cb *)&skb->cb)->scif_ep
+#define GET_SKB_QP(skb)		((struct ibscif_skb_cb *)&skb->cb)->qp
+
+#define hw_addr_equal(h1, h2)	(!memcmp(h1, h2, ETH_ALEN))
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,2,0)
+  #define KMAP(x) kmap(x->page)
+  #define KUNMAP(x) kunmap(x->page)
+  #define SET_PAGE(x,y) x->page = y
+  #define GET_PAGE(x) get_page(x->page)
+#else
+  #define KMAP(x) kmap(skb_frag_page(x))
+  #define KUNMAP(x) kunmap(skb_frag_page(x))
+  #define SET_PAGE(x,y) __skb_frag_set_page(x, y)
+  #define GET_PAGE(x) __skb_frag_ref(x)
+#endif
+
+void ibscif_skb_destructor(struct sk_buff *skb)
+{
+	struct ibscif_dev *dev = GET_SKB_DEV(skb);
+
+	/* A sk_buff is now available. */
+	if (atomic_inc_return(&dev->available) == 1)
+		; /* Could invoke the scheduler here. */
+
+	/* Release the module reference held for this sk_buff. */
+	module_put(THIS_MODULE);
+}
+
+static struct sk_buff *ibscif_alloc_tx_skb(struct ibscif_dev *dev, int hdr_size, int payload_size)
+{
+	struct sk_buff *skb;
+
+	skb = dev_alloc_skb(hdr_size);
+	if (unlikely(!skb))
+		return NULL;
+
+	skb_reset_mac_header(skb);
+	skb_reset_network_header(skb);
+
+	skb->protocol  = IBSCIF_PACKET_TYPE;
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+	skb->priority  = TC_PRIO_CONTROL;	/* highest defined priority */
+	skb->dev       = (void *) dev;
+	skb->len       = hdr_size + payload_size;
+	skb->data_len  = payload_size;
+	skb->tail     += hdr_size;
+
+	return skb;
+}
+
+static struct	sk_buff_head xmit_queue;
+static void	ibscif_xmit_work_handler( struct work_struct *context );
+static DECLARE_WORK(ibscif_xmit_work, ibscif_xmit_work_handler);
+static atomic_t	xmit_busy = ATOMIC_INIT(0);
+
+static void ibscif_xmit_work_handler( struct work_struct *context )
+{
+	struct sk_buff *skb;
+	scif_epd_t scif_ep;
+	int num_frags;
+	skb_frag_t *frag;
+	void *vaddr;
+	int ret;
+	int hdr_size;
+	int i;
+	struct ibscif_qp *qp;
+
+again:
+	while ((skb = skb_dequeue(&xmit_queue))) {
+		scif_ep = GET_SKB_EP(skb);
+		if (!scif_ep) {
+			printk(KERN_ALERT PFX "%s: NULL scif_ep, skb=%p\n", __func__, skb);
+			goto next;
+		}
+
+		hdr_size = skb->len - skb->data_len;
+		for (i=0; i<hdr_size; ) {
+			ret = scif_send(scif_ep, skb->data+i, hdr_size-i,
+					 blocking_send ? SCIF_SEND_BLOCK : 0); 
+			if (ret < 0) {
+				printk(KERN_ALERT PFX "%s: fail to send header, hdr_size=%d, ret=%d\n", __func__, hdr_size, ret);
+				goto next;
+			}
+			i += ret;
+		}
+
+		num_frags = skb_shinfo(skb)->nr_frags;
+		frag = skb_shinfo(skb)->frags;
+		while (num_frags--) {
+			vaddr = KMAP(frag); /* because scif_send() may cause scheduling */
+			for (i=0; i<frag->size; ) {
+				ret = scif_send(scif_ep, vaddr + frag->page_offset + i, 
+						frag->size - i, 
+						blocking_send ? SCIF_SEND_BLOCK : 0); 
+				if (ret < 0) {
+					printk(KERN_ALERT PFX "%s: scif_send returns %d, frag_size=%d\n", __func__, ret, frag->size);
+					break;
+				}
+				i += ret;
+			}
+			KUNMAP(frag);
+			frag++;
+		}
+next:
+		qp = GET_SKB_QP(skb);
+		if (qp && qp->ibqp.qp_type == IB_QPT_UD) {
+			struct ibscif_full_frame *pdu = (struct ibscif_full_frame*)skb->data;
+			u16 opcode = __be16_to_cpu(pdu->ibscif.hdr.opcode);
+			if (ibscif_pdu_is_last(opcode)) {
+				struct ibscif_wr *wr = GET_SKB_WR(skb);
+				ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+				wr->state = WR_COMPLETED;
+				ibscif_process_sq_completions(GET_SKB_QP(skb));
+			}
+			/* Release the reference held on UD QPs */
+			ibscif_put_qp(qp);
+		}
+		kfree_skb(skb);
+	}
+
+	if (!skb_queue_empty(&xmit_queue))
+		goto again;
+
+	atomic_set(&xmit_busy, 0);
+}
+
+static void ibscif_dev_queue_xmit(struct sk_buff *skb)
+{
+	struct ibscif_dev *dev=NULL;
+	int len = 0;
+
+	if (skb) {
+		dev = GET_SKB_DEV(skb);
+		len = skb->len;
+		skb_queue_tail(&xmit_queue, skb);
+	}
+
+	/* only one instance can be enqueued, otherwise there is race condition between scif_send() calls. */
+	/* notice that the current running worker may miss the newly added item, but it will be picked up in the poll_thread */ 
+	if (!atomic_xchg(&xmit_busy, 1))
+		schedule_work(&ibscif_xmit_work);
+
+	if (likely(dev)) {
+		DEV_STAT(dev, packets_sent++);
+		DEV_STAT(dev, bytes_sent += len);
+	}
+}
+
+static int ibscif_create_hdr(struct ibscif_qp *qp, struct ibscif_wr *wr, struct sk_buff *skb,
+			    u32 seq_num, u32 wr_len_remaining, int force)
+{
+	struct ibscif_full_frame *pdu = (struct ibscif_full_frame*)skb->data;
+	u32 sq_seq, iq_seq;
+	u16 opcode;
+	int i;
+
+	sq_seq = qp->wire.sq.rx.last_in_seq;
+	iq_seq = qp->wire.iq.rx.last_in_seq;
+	qp->wire.sq.rx.last_seq_acked = sq_seq;
+	qp->wire.iq.rx.last_seq_acked = iq_seq;
+
+	pdu->ibscif.hdr.length	 = __cpu_to_be16(skb->data_len);
+	if (qp->ibqp.qp_type == IB_QPT_UD) {
+		pdu->ibscif.hdr.dst_qp	 = __cpu_to_be32(wr->ud.remote_qpn);
+	}
+	else {
+		pdu->ibscif.hdr.dst_qp	 = __cpu_to_be32(qp->remote_qpn);
+	}
+	pdu->ibscif.hdr.src_qp	 = __cpu_to_be32(qp->ibqp.qp_num);
+	pdu->ibscif.hdr.seq_num	 = __cpu_to_be32(seq_num);
+	pdu->ibscif.hdr.sq_ack_num = __cpu_to_be32(sq_seq);
+	pdu->ibscif.hdr.iq_ack_num = __cpu_to_be32(iq_seq);
+
+	switch (wr->opcode) {
+	case WR_UD:
+		opcode = ibscif_op_ud;
+		if (skb->data_len == wr_len_remaining) {
+			opcode = ibscif_pdu_set_last(opcode);
+			if (wr->flags & IB_SEND_SIGNALED)
+				force = 1;
+			if (wr->flags & IB_SEND_SOLICITED)
+				opcode = ibscif_pdu_set_se(opcode);
+		}
+		pdu->ibscif.ud.msg_length = __cpu_to_be32(wr->length);
+		pdu->ibscif.ud.msg_offset = __cpu_to_be32(wr->length - wr_len_remaining);
+		memset(&pdu->ibscif.ud.grh, 0, 40);
+		break;
+
+	case WR_SEND:
+	case WR_SEND_WITH_IMM:
+		opcode = ibscif_op_send;
+		if (skb->data_len == wr_len_remaining || opcode == ibscif_op_send_rma) {
+			opcode = ibscif_pdu_set_last(opcode);
+			if (wr->flags & IB_SEND_SIGNALED)
+				force = 1;
+			if (wr->opcode == WR_SEND_WITH_IMM) {
+				opcode = ibscif_pdu_set_immed(opcode);
+				pdu->ibscif.send.immed_data = __cpu_to_be32(wr->send.immediate_data);
+			} else pdu->ibscif.send.immed_data = 0;
+			if (wr->flags & IB_SEND_SOLICITED)
+				opcode = ibscif_pdu_set_se(opcode);
+		}
+		pdu->ibscif.send.msg_id	  = __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.send.msg_length = __cpu_to_be32(wr->length);
+		pdu->ibscif.send.msg_offset = __cpu_to_be32(wr->length - wr_len_remaining);
+		if (wr->use_rma) {
+			opcode = ibscif_op_send_rma;
+			pdu->ibscif.send.rma_id	     = __cpu_to_be32(wr->rma_id);
+			pdu->ibscif.send.num_rma_addrs = __cpu_to_be32(wr->num_ds);
+			for (i=0; i<wr->num_ds; i++) {
+				pdu->ibscif.send.rma_addrs[i].offset = __cpu_to_be64(wr->ds_list[i].current_mreg->offset + wr->ds_list[i].offset);
+				pdu->ibscif.send.rma_addrs[i].length = __cpu_to_be32(wr->ds_list[i].length);
+			}
+		}
+		break;
+
+	case WR_RDMA_READ:
+		opcode = ibscif_op_read;
+		pdu->ibscif.read_req.rdma_id	= __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.read_req.rdma_key	= __cpu_to_be32(wr->read.rkey);
+		pdu->ibscif.read_req.rdma_length= __cpu_to_be32(wr->read.remote_length);
+		pdu->ibscif.read_req.rdma_address = __cpu_to_be64(wr->read.remote_address);
+		if (wr->use_rma) {
+			opcode = ibscif_op_read_rma;
+			pdu->ibscif.read_req.num_rma_addrs = __cpu_to_be32(wr->num_ds);
+			for (i=0; i<wr->num_ds; i++) {
+				pdu->ibscif.read_req.rma_addrs[i].offset = __cpu_to_be64(wr->ds_list[i].current_mreg->offset + wr->ds_list[i].offset);
+				pdu->ibscif.read_req.rma_addrs[i].length = __cpu_to_be32(wr->ds_list[i].length);
+			}
+		}
+		break;
+
+	case WR_RDMA_WRITE:
+	case WR_RDMA_WRITE_WITH_IMM:
+		opcode = ibscif_op_write;
+		if ((enum ib_wr_opcode)wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM) {
+			opcode = ibscif_pdu_set_immed(opcode);
+			pdu->ibscif.write.immed_data = __cpu_to_be32(wr->write.immediate_data);
+			if (wr->flags & IB_SEND_SOLICITED)
+				opcode = ibscif_pdu_set_se(opcode);
+		} else pdu->ibscif.write.immed_data = 0;
+		if (skb->data_len == wr_len_remaining || opcode == ibscif_op_write_rma) {
+			opcode = ibscif_pdu_set_last(opcode);
+			if (wr->flags & IB_SEND_SIGNALED)
+				force = 1;
+		}
+		pdu->ibscif.write.msg_id	     = __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.write.rdma_key     = __cpu_to_be32(wr->write.rkey);
+		pdu->ibscif.write.rdma_address = __cpu_to_be64(wr->write.remote_address + 
+							      (wr->length - wr_len_remaining));
+		if (wr->use_rma) {
+			opcode = ibscif_op_write_rma;
+			if (wr->opcode == WR_RDMA_WRITE_WITH_IMM)
+				opcode = ibscif_pdu_set_immed(opcode);
+			pdu->ibscif.write.rma_id	= __cpu_to_be32(wr->rma_id);
+			pdu->ibscif.write.rma_length    = __cpu_to_be32(wr->length);
+			pdu->ibscif.write.num_rma_addrs = __cpu_to_be32(wr->num_ds);
+			for (i=0; i<wr->num_ds; i++) {
+				pdu->ibscif.write.rma_addrs[i].offset = __cpu_to_be64(wr->ds_list[i].current_mreg->offset + wr->ds_list[i].offset);
+				pdu->ibscif.write.rma_addrs[i].length = __cpu_to_be32(wr->ds_list[i].length);
+			}
+		}
+		break;
+
+	case WR_ATOMIC_CMP_AND_SWP:
+		opcode = ibscif_pdu_set_last(ibscif_op_comp_swap);
+		pdu->ibscif.comp_swap.atomic_id	    = __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.comp_swap.atomic_key    = __cpu_to_be32(wr->cmp_swp.rkey);
+		pdu->ibscif.comp_swap.comp_data	    = __cpu_to_be64(wr->cmp_swp.cmp_operand);
+		pdu->ibscif.comp_swap.swap_data	    = __cpu_to_be64(wr->cmp_swp.swp_operand);
+		pdu->ibscif.comp_swap.atomic_address = __cpu_to_be64(wr->cmp_swp.remote_address);
+		break;
+
+	case WR_ATOMIC_FETCH_AND_ADD:
+		opcode = ibscif_pdu_set_last(ibscif_op_fetch_add);
+		pdu->ibscif.fetch_add.atomic_id	   = __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.fetch_add.atomic_key   = __cpu_to_be32(wr->fetch_add.rkey);
+		pdu->ibscif.fetch_add.add_data	   = __cpu_to_be64(wr->fetch_add.add_operand);
+		pdu->ibscif.fetch_add.atomic_address = __cpu_to_be64(wr->fetch_add.remote_address);
+		break;
+
+	case WR_RDMA_READ_RSP:
+		opcode = ibscif_op_read_rsp;
+		if (skb->data_len == wr_len_remaining)
+			opcode = ibscif_pdu_set_last(opcode);
+		pdu->ibscif.read_rsp.rdma_id	= __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.read_rsp.rdma_offset  = __cpu_to_be32(wr->length - wr_len_remaining);
+		break;
+
+	case WR_ATOMIC_RSP:
+		opcode = ibscif_pdu_set_last(wr->atomic_rsp.opcode);
+		pdu->ibscif.atomic_rsp.atomic_id = __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.atomic_rsp.orig_data = __cpu_to_be64(wr->atomic_rsp.orig_data);
+		break;
+
+	case WR_RMA_RSP:
+		opcode = ibscif_op_rma_rsp;
+		pdu->ibscif.rma_rsp.rma_id	= __cpu_to_be32(wr->msg_id);
+		pdu->ibscif.rma_rsp.xfer_length	= __cpu_to_be32(wr->rma_rsp.xfer_length);
+		pdu->ibscif.rma_rsp.error	= __cpu_to_be32(wr->rma_rsp.error);
+		break;
+	default:
+		printk(KERN_ERR PFX "%s() invalid opcode %d\n", __func__, wr->opcode);
+		return 1;
+	}
+
+	if (force)
+		opcode = ibscif_pdu_set_force_ack(opcode);
+
+	pdu->ibscif.hdr.opcode = __cpu_to_be16(opcode);
+
+	return 0;
+}
+
+static struct sk_buff* ibscif_alloc_pdu(struct ibscif_dev *dev, struct ibscif_qp *qp, struct ibscif_wr *wr,
+				       int hdr_size, u32 seq_num, u32 payload_size, u32 len_remaining, int force)
+{
+	struct sk_buff *skb;
+	struct ibscif_full_frame *pdu;
+
+	if (unlikely(!qp->conn && qp->ibqp.qp_type != IB_QPT_UD)) {
+		printk(KERN_ALERT PFX "%s: ERROR: qp->conn == NULL\n", __func__);
+		return NULL;
+	}
+
+	if (!atomic_add_unless(&dev->available, -1, 0)) {
+		printk(KERN_NOTICE PFX "%s throttled by available tx buffer limit\n", dev->ibdev.name);
+		DEV_STAT(dev, unavailable++);
+		return NULL;
+	}
+
+	/* Get an skb for this protocol packet. */
+	skb = ibscif_alloc_tx_skb(dev, hdr_size, payload_size);
+	if (unlikely(!skb))
+		goto bail;
+
+	/* Hold a reference on the module until skb->destructor is called. */
+	__module_get(THIS_MODULE);
+	skb->destructor = ibscif_skb_destructor;
+
+	SET_SKB_DEV(skb, dev);
+	SET_SKB_WR(skb, wr);
+
+	if (qp->ibqp.qp_type == IB_QPT_UD) {
+		struct ibscif_conn *conn;
+		int flag = qp->ibqp.qp_num > wr->ud.remote_qpn;
+		conn = ibscif_get_conn(qp->local_node_id, wr->ud.remote_node_id, flag);
+		if (unlikely(!conn)) {
+			kfree_skb(skb);
+			goto bail;
+		}
+
+		ibscif_qp_add_ud_conn(qp, conn);
+		ibscif_put_conn(conn);
+		SET_SKB_EP(skb, conn->ep);
+		SET_SKB_QP(skb, qp);
+
+		/* Reference UD QPs until the wr is transmitted by ibscif_xmit_work_handler */
+		kref_get(&qp->ref);
+	}
+	else {
+		SET_SKB_EP(skb, qp->conn->ep);
+	}
+
+	/* Construct the header and copy it to the skb. */
+	if (unlikely(ibscif_create_hdr(qp, wr, skb, seq_num, len_remaining, force))) {
+		kfree_skb(skb);
+		goto bail;
+	}
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.hdr.hdr_size	= __cpu_to_be16(hdr_size);
+	
+	return skb;
+bail:
+	atomic_inc(&dev->available);
+	return NULL;
+}
+
+static int ibscif_send_null_pdu(struct ibscif_dev *dev, struct ibscif_qp *qp, struct ibscif_wr *wr, u32 hdr_size)
+{
+	struct sk_buff *skb;
+
+	/* Allocate an initialized skb with a PDU header. */
+	skb = ibscif_alloc_pdu(dev, qp, wr, hdr_size, wr->sar.seg.starting_seq, 0, 0, 0);
+	if (unlikely(!skb))
+		return 0;
+
+	ibscif_dev_queue_xmit(skb);
+	return 1;
+}
+
+static int get_hdr_size_from_wr(struct ibscif_wr *wr)
+{
+	switch (wr->opcode) {
+	case WR_UD:			return sizeof(struct ud_hdr);
+	case WR_SEND:
+	case WR_SEND_WITH_IMM:		return sizeof(struct send_hdr);
+	case WR_RDMA_WRITE:
+	case WR_RDMA_WRITE_WITH_IMM:	return sizeof(struct write_hdr);
+	case WR_RDMA_READ:		return sizeof(struct read_req_hdr);
+	case WR_ATOMIC_CMP_AND_SWP:	return sizeof(struct comp_swap_hdr);
+	case WR_ATOMIC_FETCH_AND_ADD:	return sizeof(struct fetch_add_hdr);
+	case WR_RDMA_READ_RSP:		return sizeof(struct read_rsp_hdr);
+	case WR_ATOMIC_RSP:		return sizeof(struct atomic_rsp_hdr);
+	case WR_RMA_RSP:		return sizeof(struct rma_rsp_hdr);
+	default:			return 0;
+	}
+}
+
+static int get_rma_addr_size_from_wr(struct ibscif_wr *wr)
+{
+	switch (wr->opcode) {
+	case WR_UD:			return 0;
+	case WR_SEND:
+	case WR_SEND_WITH_IMM:
+	case WR_RDMA_WRITE:
+	case WR_RDMA_WRITE_WITH_IMM:
+	case WR_RDMA_READ:		return wr->num_ds * sizeof(struct rma_addr);
+	case WR_ATOMIC_CMP_AND_SWP:	return 0;
+	case WR_ATOMIC_FETCH_AND_ADD:	return 0;
+	case WR_RDMA_READ_RSP:		return 0;
+	case WR_ATOMIC_RSP:		return 0;
+	case WR_RMA_RSP:		return 0;
+	default:			return 0;
+	}
+}
+
+static int setup_rma_addrs(struct ibscif_wq *wq, struct ibscif_wr *wr)
+{
+	struct ibscif_ds *ds;
+	int i;
+
+	if (!wr->num_ds)
+		return 1;
+
+	for (i=0; i<wr->num_ds; i++) {
+		ds = &wr->ds_list[i];
+		if (!ds->current_mreg) 
+			ds->current_mreg = ibscif_mr_get_mreg(ds->mr, wq->qp->conn);
+
+		if (!ds->current_mreg) 
+			return 0;
+	}
+
+	return 1;
+}
+
+/* when necessary SCIF will allocate temp buffer to align up cache line offset.
+ *  * so we only need to use roffset to calculate the dma size.
+ *   */
+static inline int ibscif_dma_size(u32 len, u64 roffset)
+{
+	u32 head, tail;
+
+	tail = (roffset + len) % 64;
+	head = (64 - roffset % 64) % 64;
+	if (len >= head + tail) 
+		return (len - head - tail);
+	else
+		return 0;
+}
+
+static void ibscif_send_ack(struct ibscif_qp *qp); /* defined later in this file */
+
+static int ibscif_try_fast_rdma(struct ibscif_wq *wq, struct ibscif_wr *wr)
+{
+	struct ibscif_qp *qp;
+	int i, err;
+	u64 loffset, roffset;
+	u32 total_length, rdma_length, xfer_len;
+	u64 raddress;
+	u32 rkey;
+	enum ib_access_flags access;
+	u32 dma_size = 0;
+	int rma_flag = 0;
+
+	IBSCIF_PERF_SAMPLE(2, 0);
+
+	switch (wr->opcode) {
+	  case WR_RDMA_WRITE:
+		raddress = wr->write.remote_address;
+		rkey = wr->write.rkey;
+		total_length = rdma_length = wr->length;
+		access = IB_ACCESS_REMOTE_WRITE;
+		break;
+
+	  case WR_RDMA_READ:
+		raddress = wr->read.remote_address;
+		rkey = wr->read.rkey;
+		total_length = rdma_length = wr->read.remote_length; /* wr->length is 0 */
+		access = IB_ACCESS_REMOTE_READ;
+		break;
+
+	  default:
+		return 0;
+	}
+
+	qp = wq->qp;
+
+	if (unlikely(!qp->conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: qp->conn == NULL\n", __func__);
+		return 0;
+	}
+
+	if (!setup_rma_addrs(wq, wr)) {
+		DEV_STAT(qp->dev, fast_rdma_fallback++);
+		return 0;
+	}
+
+	roffset = IBSCIF_MR_VADDR_TO_OFFSET( rkey, raddress );
+
+	for (i=0; i<wr->num_ds; i++) {
+		if (rdma_length == 0)
+			break;
+
+		loffset = wr->ds_list[i].current_mreg->offset + wr->ds_list[i].offset;
+		xfer_len = min(wr->ds_list[i].length, rdma_length);
+		if (xfer_len == 0)
+			continue;
+
+		IBSCIF_PERF_SAMPLE(3, 0);
+
+		dma_size = ibscif_dma_size(xfer_len, roffset);
+
+		if (i==wr->num_ds-1)
+			rma_flag = dma_size ? SCIF_RMA_SYNC : 0;
+
+		if (wr->opcode == WR_RDMA_WRITE) {
+			err = scif_writeto(wq->qp->conn->ep, loffset, xfer_len, roffset, rma_flag|SCIF_RMA_ORDERED);
+			if (err)
+				printk(KERN_INFO PFX "%s(): error writing ordered message, size=%d, err=%d.\n", __func__, xfer_len, err);
+		}
+		else {
+			err = scif_readfrom(wq->qp->conn->ep, loffset, xfer_len, roffset, rma_flag);
+			if (err)
+				printk(KERN_INFO PFX "%s(): error reading the message, size=%d, err=%d.\n", __func__, xfer_len, err);
+		}
+
+		IBSCIF_PERF_SAMPLE(4, 0);
+
+		if (err){
+			DEV_STAT(qp->dev, fast_rdma_fallback++);
+			return 0;
+		}
+
+		roffset += xfer_len;
+		rdma_length -= xfer_len;
+	}
+
+	if (rdma_length)
+		printk(KERN_INFO PFX "%s(): remaining rdma_length=%d.\n", __func__, rdma_length);
+
+	IBSCIF_PERF_SAMPLE(5, 0);
+
+	/* complete the wr */
+	ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+	wr->state = WR_COMPLETED;
+	wr->sar.rea.final_length = total_length - rdma_length;
+
+	/* we can't call ibscif_process_sq_completions here because we are holding the sq lock.
+ 	 * set the flag and let the upper level make the call */
+	wq->fast_rdma_completions = 1;
+
+	if (wr->opcode == WR_RDMA_WRITE)
+		DEV_STAT(qp->dev, fast_rdma_write++);
+	else
+		DEV_STAT(qp->dev, fast_rdma_read++);
+
+	/* the fast rdma protocol doesn't send any packet, and thus can not piggyback any ack
+ 	 * for the peer. send separate ack packet when necessary. */
+	if (qp->wire.sq.rx.last_seq_acked < qp->wire.sq.rx.last_in_seq ||
+	    qp->wire.iq.rx.last_seq_acked < qp->wire.iq.rx.last_in_seq) {
+		ibscif_send_ack(qp);
+		DEV_STAT(qp->dev, fast_rdma_force_ack++);
+	}
+
+	IBSCIF_PERF_SAMPLE(8, 0);
+
+	return 1;
+}
+
+/*
+ * Setup for a fresh data descriptor.
+ */
+#define DS_SETUP(ds, mr, page_offset, page_index, ds_len_left)	\
+do {								\
+	mr = ds->mr;						\
+	ds_len_left  = ds->length;				\
+	page_offset  = ds->offset + (mr->addr & ~PAGE_MASK);	\
+	page_index   = page_offset >> PAGE_SHIFT;		\
+	page_offset &= ~PAGE_MASK;				\
+} while(0)
+
+/*
+ * Setup for page crossing within a data descriptor.
+ */
+#define NEXT_PAGE(ds, mr, page_offset, page_index, ds_len_left)		\
+do {									\
+	if (!ds_len_left) {						\
+		ds++;							\
+		DS_SETUP(ds, mr, page_offset, page_index, ds_len_left);	\
+	} else {							\
+		page_index++;						\
+		BUG_ON(!(mr->npages > page_index));			\
+		page_offset = 0;					\
+	}								\
+} while(0)
+
+/*
+ * Setup the data descriptor, page, and offset for specified sequence number
+ */
+#define SETUP_BY_SEQ(wr, ds, mr, from_seq, wr_length, page_offset, page_index,		\
+		     ds_len_left, max_payload)						\
+do {											\
+	u32 i, frag_len_max;								\
+											\
+	DS_SETUP(ds, mr, page_offset, page_index, ds_len_left);				\
+	for (i = wr->sar.seg.starting_seq; seq_before(i, from_seq); i++) {		\
+		num_frags = 0;								\
+		payload_left = max_payload;						\
+		while (payload_left && (num_frags < MAX_SKB_FRAGS)) {			\
+			frag_len_max = min(ds_len_left, (u32)(PAGE_SIZE - page_offset));\
+			if (wr_length > payload_left) {					\
+				if (payload_left > frag_len_max) {			\
+					ds_len_left -= frag_len_max;			\
+					NEXT_PAGE(ds, mr, page_offset,			\
+						  page_index, ds_len_left);		\
+				} else {						\
+					frag_len_max = payload_left; /* frag->size */	\
+					ds_len_left -= payload_left;			\
+					page_offset += payload_left;			\
+				}							\
+			} else {							\
+				if (wr_length > frag_len_max) {				\
+					ds_len_left -= frag_len_max;			\
+					NEXT_PAGE(ds, mr, page_offset,			\
+						  page_index, ds_len_left);		\
+				} else {						\
+					printk(KERN_ERR	PFX				\
+				"from_seq (%d) botch wr %p opcode %d length %d\n",	\
+					from_seq, wr, wr->opcode, wr_length);		\
+					return 0;					\
+				}							\
+			}								\
+			wr_length    -= frag_len_max;					\
+			payload_left -= frag_len_max;					\
+			num_frags++;							\
+		}									\
+	}										\
+} while(0)
+
+int ibscif_xmit_wr(struct ibscif_wq *wq, struct ibscif_wr *wr, int tx_limit, int retransmit, u32 from_seq, u32 *posted)
+{
+	struct ibscif_dev *dev;
+	struct ibscif_qp *qp;
+	struct ibscif_ds *ds;
+	struct ibscif_mr *mr;
+	int hdr_size, page_index, num_frags, num_xmited;
+	u32 max_payload, wr_length, page_offset, ds_len_left, payload_left;
+
+	/* Try to process RDMA read/write directly with SCIF functions.
+	 * The usual reason for failure is that the remote memory has not yet been 
+	 * registered with SCIF. The normal packet based path should handle that. 
+	 */
+	if (host_proxy && wq->qp->local_node_id>0 && wq->qp->remote_node_id==0) {
+		/* don't try fast rdma becasue we want to let the host do the data transfer */
+	}
+	else if (fast_rdma) { 
+		num_xmited = 0;
+		if (ibscif_try_fast_rdma(wq, wr))
+			goto finish2;
+	}
+
+	if (!tx_limit) {
+		printk(KERN_INFO PFX "%s() called with tx_limit of zero\n", __func__);
+		return 0;
+	}
+
+	qp = wq->qp;
+	dev = qp->dev;
+	hdr_size = get_hdr_size_from_wr(wr);
+	max_payload = qp->mtu - hdr_size;
+
+	if (wr->use_rma) {
+		struct sk_buff *skb;
+
+		wr_length = wr->length;
+		wr->sar.seg.starting_seq = from_seq;
+		wr->sar.seg.ending_seq	 = from_seq;
+		wr->state = WR_STARTED;
+
+		num_xmited = 0;
+		if (setup_rma_addrs(wq, wr)) {
+			/* Make room in the header for RMA addresses */
+			hdr_size += get_rma_addr_size_from_wr(wr);
+
+			/* Allocate an initialized skb with PDU header. */
+			skb = ibscif_alloc_pdu(dev, qp, wr, hdr_size, from_seq, 0, wr_length, 0);
+			if (likely(skb)) {
+				ibscif_dev_queue_xmit(skb);
+				num_xmited++;
+				from_seq++;
+			}
+		}
+		else 
+			printk(KERN_ALERT PFX "%s: fail to set up RMA addresses for the work request.\n", __func__);
+
+		goto finish;
+	}
+
+	if (!wr->sar.seg.current_ds) {
+		/*
+		 * This is a fresh send so intialize the wr by setting the static
+		 * parts of the header and sequence number range for this wr.
+		 */
+		wr_length = wr->length;
+		wr->sar.seg.starting_seq = from_seq;
+		wr->sar.seg.ending_seq	 = from_seq;
+		if (wr->opcode == WR_UD)
+			max_payload = wr_length;
+		else if (wr_length > max_payload) {
+			wr->sar.seg.ending_seq += (wr_length / max_payload);
+			if (!(wr_length % max_payload))
+				wr->sar.seg.ending_seq--;
+		}
+
+		wr->state = WR_STARTED;
+
+		/*
+		 * If this request has a payload, setup for fragmentation.
+		 * Otherwise, send it on its way.
+		 */
+		if (wr_length) {
+			ds = wr->ds_list;
+			DS_SETUP(ds, mr, page_offset, page_index, ds_len_left);
+		} else {
+			num_xmited = ibscif_send_null_pdu(dev, qp, wr, hdr_size);
+			/* from_seq must always advanced even in null PDU cases. */
+			from_seq++;
+			goto finish;
+		}
+	} else {
+		/* We're picking up from a paritally sent request. */
+		ds = wr->sar.seg.current_ds;
+		mr = ds->mr;
+		wr_length   = wr->sar.seg.wr_length_remaining;
+		ds_len_left = wr->sar.seg.ds_length_remaining;
+		page_index  = wr->sar.seg.current_page_index;
+		page_offset = wr->sar.seg.current_page_offset;
+		from_seq    = wr->sar.seg.next_seq;
+	}
+
+	/* Ok, let's break this bad-boy up. */
+	num_xmited = 0;
+	while (wr_length && (num_xmited < tx_limit) && (qp->state == QP_CONNECTED)) {
+		struct sk_buff *skb;
+		skb_frag_t *frag;
+
+		/* Allocate an initialized skb with PDU header. */
+		skb = ibscif_alloc_pdu(dev, qp, wr, hdr_size, from_seq, min(wr_length, max_payload),
+				      wr_length, retransmit && (num_xmited == (tx_limit - 1)));
+		if (unlikely(!skb))
+			break;
+
+		/* Update sequence number for next pass. */
+		from_seq++;
+
+		/* Fill the skb fragment list. */
+		frag = skb_shinfo(skb)->frags;
+		num_frags = 0;
+		payload_left = max_payload;
+
+		while (payload_left && (num_frags < MAX_SKB_FRAGS)) {
+			u32 frag_len_max;
+
+			SET_PAGE(frag, mr->page[page_index]);
+			frag->page_offset = page_offset;
+
+			/* Take a reference on the page - kfree_skb will release. */
+			GET_PAGE(frag);
+
+			frag_len_max = min(ds_len_left, (u32)(PAGE_SIZE - page_offset));
+			if (wr_length > payload_left) {
+				if (payload_left > frag_len_max) {
+					/* Deal with page boundary crossing. */
+					frag->size   = frag_len_max;
+					ds_len_left -= frag_len_max;
+					NEXT_PAGE(ds, mr, page_offset, page_index, ds_len_left);
+				} else {
+					frag->size   = payload_left;
+					ds_len_left -= payload_left;
+					page_offset += payload_left;
+				}
+			} else {
+				if (wr_length > frag_len_max) {
+					/* Deal with page boundary crossing. */
+					frag->size   = frag_len_max;
+					ds_len_left -= frag_len_max;
+					NEXT_PAGE(ds, mr, page_offset, page_index, ds_len_left);
+				} else {
+					frag->size    = wr_length;
+					payload_left -= wr_length;
+					wr_length = 0;
+					num_frags++; /* Change from index to number. */
+					break;
+				}
+			}
+
+			wr_length    -= frag->size;
+			payload_left -= frag->size;
+			num_frags++;
+			frag++;
+		}
+		skb_shinfo(skb)->nr_frags = num_frags;
+
+		/* Check if we need to do a fixup because we ran out of frags. */
+		if ((num_frags == MAX_SKB_FRAGS) && wr_length) {
+			struct ibscif_full_frame *pdu = (struct ibscif_full_frame*)skb->data;
+			skb->len      = hdr_size + (max_payload - payload_left);
+			skb->data_len = (max_payload - payload_left);
+			pdu->ibscif.hdr.length = __cpu_to_be16(skb->data_len);
+			pdu->ibscif.hdr.opcode = __cpu_to_be16(__be16_to_cpu(pdu->ibscif.hdr.opcode) & ~ibscif_last_flag);
+		}
+
+		/*
+		 * Send it.
+		 */
+		ibscif_dev_queue_xmit(skb);
+		num_xmited++;
+	}
+
+	/*
+	 * Update state. If this is a retransmit, don't update anything.  If not and
+	 * there's more to do on the wr, save state.  Otherwise, setup for next wr.
+	 */
+	if (wr_length && !wr->use_rma) {
+		wr->sar.seg.current_ds = ds;
+		wr->sar.seg.wr_length_remaining = wr_length;
+		wr->sar.seg.ds_length_remaining = ds_len_left;
+		wr->sar.seg.current_page_index	= page_index;
+		wr->sar.seg.current_page_offset = page_offset;
+	} else {
+finish:		if (wr->opcode != WR_UD)
+			wr->state = WR_WAITING_FOR_ACK;
+finish2:	wq->next_wr = (wq->next_wr + 1) % wq->size;
+	}
+	wr->sar.seg.next_seq = from_seq;
+	if (posted)
+		*posted = from_seq;
+
+	return num_xmited;
+}
+
+static struct sk_buff *ibscif_create_disconnect_hdr(struct ibscif_dev *dev, u32 src_qpn,
+						   u32 dst_qpn, enum ibscif_reason reason)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_alloc_tx_skb(dev, sizeof pdu->ibscif.disconnect, 0);
+	if (unlikely(!skb)) {
+		printk(KERN_ERR PFX "%s() can't allocate skb\n", __func__);
+		return NULL;
+	}
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+
+	/* The eth_hdr and ack fields are set by the caller. */
+	pdu->ibscif.disconnect.hdr.opcode  = __cpu_to_be16(ibscif_op_disconnect);
+	pdu->ibscif.disconnect.hdr.length  = 0; /* Length has no meaning. */
+	pdu->ibscif.disconnect.hdr.dst_qp  = __cpu_to_be32(dst_qpn);
+	pdu->ibscif.disconnect.hdr.src_qp  = __cpu_to_be32(src_qpn);
+	pdu->ibscif.disconnect.hdr.seq_num = 0; /* seq_num has no meaning. */
+	pdu->ibscif.disconnect.hdr.hdr_size = __cpu_to_be16(sizeof(pdu->ibscif.disconnect));
+	pdu->ibscif.disconnect.reason	  = __cpu_to_be32(reason);
+
+	SET_SKB_DEV(skb, dev);
+	SET_SKB_WR(skb, NULL);
+
+	return skb;
+}
+
+void ibscif_send_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason)
+{
+	struct ibscif_dev *dev = qp->dev;
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	if (qp->ibqp.qp_type == IB_QPT_UD)
+		return;
+
+	if (qp->loopback) {
+		ibscif_loopback_disconnect(qp, reason);
+		return;
+	}
+
+	if (unlikely(!qp->conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: qp->conn == NULL\n", __func__);
+		return;
+	}
+
+	skb = ibscif_create_disconnect_hdr(dev, qp->ibqp.qp_num, qp->remote_qpn, reason);
+	if (unlikely(!skb))
+		return;
+
+	SET_SKB_EP(skb, qp->conn->ep);
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+
+	pdu->ibscif.disconnect.hdr.sq_ack_num = __cpu_to_be32(qp->wire.sq.rx.last_in_seq);
+	pdu->ibscif.disconnect.hdr.iq_ack_num = __cpu_to_be32(qp->wire.iq.rx.last_in_seq);
+
+	ibscif_dev_queue_xmit(skb);
+}
+
+void ibscif_reflect_disconnect(struct ibscif_qp *qp, struct base_hdr *hdr, struct sk_buff *in_skb, enum ibscif_reason reason)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	if (!qp || IS_ERR(qp)) {
+		if (qp != ERR_PTR(-ENOENT) && verbose)
+			printk(KERN_ALERT PFX "%s: qp=%p hdr=%p in_skb=%p reason=%d\n", __func__, qp, hdr, in_skb, reason);
+		return;
+	}
+
+	/* Don't send a disconnect for a disconnect. */
+	if (ibscif_pdu_base_type(hdr->opcode) == ibscif_op_disconnect)
+		return;
+
+	if (!qp->conn || !qp->conn->ep)
+		return;
+
+	skb = ibscif_create_disconnect_hdr((void *)in_skb->dev, hdr->dst_qp, hdr->src_qp, reason);
+	if (unlikely(!skb))
+		return;
+
+	SET_SKB_EP(skb, qp->conn->ep);
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+
+	pdu->ibscif.disconnect.hdr.sq_ack_num = 0; /* sq_ack_num has no meaning. */
+	pdu->ibscif.disconnect.hdr.iq_ack_num = 0; /* iq_ack_num has no meaning. */
+
+	ibscif_dev_queue_xmit(skb);
+}
+
+static struct sk_buff *ibscif_create_ack_hdr(struct ibscif_qp *qp, int size)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+	u32 sq_seq, iq_seq;
+
+	if (unlikely(!qp->conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: qp->conn == NULL\n", __func__);
+		return NULL;
+	}
+
+	skb = ibscif_alloc_tx_skb(qp->dev, size, 0);
+	if (unlikely(!skb)) {
+		printk(KERN_ERR PFX "%s() can't allocate skb\n", __func__);
+		return NULL;
+	}
+
+	SET_SKB_DEV(skb, qp->dev);
+	SET_SKB_WR(skb, NULL);
+	SET_SKB_EP(skb, qp->conn->ep);
+
+	sq_seq = qp->wire.sq.rx.last_in_seq;
+	iq_seq = qp->wire.iq.rx.last_in_seq;
+	qp->wire.sq.rx.last_seq_acked = sq_seq;
+	qp->wire.iq.rx.last_seq_acked = iq_seq;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+
+	/* The opcode field set by the caller. */
+	pdu->ibscif.hdr.length	  = 0; /* Length has no meaning. */
+	pdu->ibscif.hdr.dst_qp	  = __cpu_to_be32(qp->remote_qpn);
+	pdu->ibscif.hdr.src_qp	  = __cpu_to_be32(qp->ibqp.qp_num);
+	pdu->ibscif.hdr.seq_num	  = 0; /* seq_num has no meaning. */
+	pdu->ibscif.hdr.sq_ack_num = __cpu_to_be32(sq_seq);
+	pdu->ibscif.hdr.iq_ack_num = __cpu_to_be32(iq_seq);
+	pdu->ibscif.hdr.hdr_size	 = __cpu_to_be16(size); 
+
+	return skb;
+}
+
+static void ibscif_send_ack(struct ibscif_qp *qp)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_create_ack_hdr(qp, sizeof pdu->ibscif.ack);
+	if (unlikely(!skb))
+		return;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.ack.hdr.opcode = __cpu_to_be16(ibscif_op_ack);
+
+	ibscif_dev_queue_xmit(skb);
+}
+
+static struct sk_buff *ibscif_create_close_hdr(struct ibscif_conn *conn, int size)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	if (unlikely(!conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: conn == NULL\n", __func__);
+		return NULL;
+	}
+
+	skb = ibscif_alloc_tx_skb(conn->dev, size, 0);
+	if (unlikely(!skb)) {
+		printk(KERN_ERR PFX "%s() can't allocate skb\n", __func__);
+		return NULL;
+	}
+
+	SET_SKB_DEV(skb, conn->dev);
+	SET_SKB_WR(skb, NULL);
+	SET_SKB_EP(skb, conn->ep);
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+
+	/* The opcode field set by the caller. */
+	pdu->ibscif.hdr.length	  = 0; /* Length has no meaning. */
+	pdu->ibscif.hdr.dst_qp	  = 0; /* unused */
+	pdu->ibscif.hdr.src_qp	  = 0; /* unused */
+	pdu->ibscif.hdr.seq_num	  = 0; /* seq_num has no meaning. */
+	pdu->ibscif.hdr.sq_ack_num = 0; /* unused */
+	pdu->ibscif.hdr.iq_ack_num = 0; /* unused */
+	pdu->ibscif.hdr.hdr_size	 = __cpu_to_be16(size); 
+
+	return skb;
+}
+
+void ibscif_send_close(struct ibscif_conn *conn)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_create_close_hdr(conn, sizeof pdu->ibscif.close);
+	if (unlikely(!skb))
+		return;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.close.hdr.opcode = __cpu_to_be16(ibscif_op_close);
+
+	ibscif_dev_queue_xmit(skb);
+}
+
+void ibscif_send_reopen(struct ibscif_conn *conn)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_create_close_hdr(conn, sizeof pdu->ibscif.close);
+	if (unlikely(!skb))
+		return;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.close.hdr.opcode = __cpu_to_be16(ibscif_op_reopen);
+
+	ibscif_dev_queue_xmit(skb);
+}
+
+static struct sk_buff *ibscif_create_cm_hdr(struct ibscif_conn *conn, int size)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	if (unlikely(!conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: conn == NULL\n", __func__);
+		return NULL;
+	}
+
+	skb = ibscif_alloc_tx_skb(conn->dev, size, 0);
+	if (unlikely(!skb)) {
+		printk(KERN_ERR PFX "%s() can't allocate skb\n", __func__);
+		return NULL;
+	}
+
+	SET_SKB_DEV(skb, conn->dev);
+	SET_SKB_WR(skb, NULL);
+	SET_SKB_EP(skb, conn->ep);
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+
+	pdu->ibscif.hdr.opcode    = __cpu_to_be16(ibscif_op_cm);
+	pdu->ibscif.hdr.length	  = 0; /* Length has no meaning. */
+	pdu->ibscif.hdr.dst_qp	  = 0; /* unused */
+	pdu->ibscif.hdr.src_qp	  = 0; /* unused */
+	pdu->ibscif.hdr.seq_num	  = 0; /* seq_num has no meaning. */
+	pdu->ibscif.hdr.sq_ack_num = 0; /* unused */
+	pdu->ibscif.hdr.iq_ack_num = 0; /* unused */
+	pdu->ibscif.hdr.hdr_size  = __cpu_to_be16(size); 
+
+	return skb;
+}
+
+int ibscif_send_cm_req(struct ibscif_cm *cm_ctx)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_create_cm_hdr(cm_ctx->conn, sizeof pdu->ibscif.cm + cm_ctx->plen); 
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.cm.req_ctx	= __cpu_to_be64((u64)(uintptr_t)cm_ctx);
+	pdu->ibscif.cm.cmd	= __cpu_to_be32(IBSCIF_CM_REQ);
+	pdu->ibscif.cm.port	= __cpu_to_be32((u32)cm_ctx->remote_addr.sin_port);
+	pdu->ibscif.cm.qpn	= __cpu_to_be32(cm_ctx->qpn);
+	pdu->ibscif.cm.plen	= __cpu_to_be32(cm_ctx->plen);
+	memcpy(pdu->ibscif.cm.pdata, cm_ctx->pdata, cm_ctx->plen);
+
+	ibscif_dev_queue_xmit(skb);
+
+	return 0;
+}
+
+int ibscif_send_cm_rep(struct ibscif_cm *cm_ctx)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_create_cm_hdr(cm_ctx->conn, sizeof pdu->ibscif.cm + cm_ctx->plen); 
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.cm.req_ctx	= __cpu_to_be64(cm_ctx->peer_context);
+	pdu->ibscif.cm.rep_ctx	= __cpu_to_be64((__u64)cm_ctx);
+	pdu->ibscif.cm.cmd	= __cpu_to_be32(IBSCIF_CM_REP);
+	pdu->ibscif.cm.qpn	= __cpu_to_be32(cm_ctx->qpn);
+	pdu->ibscif.cm.status	= __cpu_to_be32(0);
+	pdu->ibscif.cm.plen	= __cpu_to_be32(cm_ctx->plen);
+	memcpy(pdu->ibscif.cm.pdata, cm_ctx->pdata, cm_ctx->plen);
+
+	ibscif_dev_queue_xmit(skb);
+
+	return 0;
+}
+
+int ibscif_send_cm_rej(struct ibscif_cm *cm_ctx, const void *pdata, u8 plen)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_create_cm_hdr(cm_ctx->conn, sizeof pdu->ibscif.cm + plen); 
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.cm.req_ctx	= __cpu_to_be64(cm_ctx->peer_context);
+	pdu->ibscif.cm.cmd	= __cpu_to_be32(IBSCIF_CM_REJ);
+	pdu->ibscif.cm.status	= __cpu_to_be32(-ECONNREFUSED);
+	pdu->ibscif.cm.plen	= __cpu_to_be32((u32)plen);
+	memcpy(pdu->ibscif.cm.pdata, pdata, plen);
+
+	ibscif_dev_queue_xmit(skb);
+
+	return 0;
+}
+
+int ibscif_send_cm_rtu(struct ibscif_cm *cm_ctx)
+{
+	struct ibscif_full_frame *pdu;
+	struct sk_buff *skb;
+
+	skb = ibscif_create_cm_hdr(cm_ctx->conn, sizeof pdu->ibscif.cm); 
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	pdu = (struct ibscif_full_frame *)skb->data;
+	pdu->ibscif.cm.rep_ctx	= __cpu_to_be64(cm_ctx->peer_context);
+	pdu->ibscif.cm.cmd	= __cpu_to_be32(IBSCIF_CM_RTU);
+
+	ibscif_dev_queue_xmit(skb);
+
+	return 0;
+}
+
+/* ---------------------- tx routines above this line ---------------------- */
+/* ---------------------- rx routines below this line ---------------------- */
+
+static void ibscif_protocol_error(struct ibscif_qp *qp, enum ibscif_reason reason)
+{
+	printk(KERN_NOTICE PFX "Disconnect due to protocol error %d\n", reason);
+	ibscif_qp_internal_disconnect(qp, reason);
+}
+
+int ibscif_process_sq_completions(struct ibscif_qp *qp)
+{
+	struct ibscif_cq *cq = to_cq(qp->ibqp.send_cq);
+	struct ibscif_wq *sq = &qp->sq;
+	struct ibscif_wr *wr;
+	struct ibscif_wc *wc;
+	int index, err = 0, i;
+
+	spin_lock_bh(&sq->lock);
+
+	/* Prevent divide by zero traps on wrap math. */
+	if (!sq->size)
+		goto out;
+
+	/* Iterate the send queue looking for defered completions. */
+	for (i=sq->completions; i<sq->depth; i++) {
+		index = (sq->head + i) % sq->size;
+
+		wr = ibscif_get_wr(sq, index);
+		if (wr->state != WR_COMPLETED)
+			break;
+
+		sq->completions++;
+		sq->reap++;
+
+		/* An IQ request has been completed; update the throttling variables. */
+		if ((wr->opcode == WR_RDMA_READ)	  ||
+		    (wr->opcode == WR_ATOMIC_CMP_AND_SWP) ||
+		    (wr->opcode == WR_ATOMIC_FETCH_AND_ADD)) {
+			BUG_ON(!atomic_read(&qp->or_depth));
+			atomic_dec(&qp->or_depth);
+			atomic_dec(&qp->or_posted);
+		}
+
+		/* See if we need to generate a completion. */
+		if (!(wr->flags & IB_SEND_SIGNALED))
+			continue;
+
+		err = ibscif_reserve_cqe(cq, &wc);
+		if (unlikely(err))
+			break;
+
+		wc->ibwc.qp	  = &qp->ibqp;
+		wc->ibwc.src_qp	  = qp->remote_qpn;
+		wc->ibwc.wr_id	  = wr->id;
+		wc->ibwc.opcode	  = to_ib_wc_opcode(wr->opcode);
+		wc->ibwc.wc_flags = (((enum ib_wr_opcode)wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM) ||
+				     ((enum ib_wr_opcode)wr->opcode == IB_WR_SEND_WITH_IMM)) ?
+					IB_WC_WITH_IMM : 0;
+		wc->ibwc.status	  = IB_WC_SUCCESS;
+		wc->ibwc.ex.imm_data = 0;
+		wc->ibwc.port_num = 1;
+		wc->ibwc.byte_len = (((enum ib_wr_opcode)wr->opcode == IB_WR_RDMA_READ)	      ||
+				     ((enum ib_wr_opcode)wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP) ||
+				     ((enum ib_wr_opcode)wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD)) ?
+					wr->sar.rea.final_length : 0;
+		wc->wq	 = sq;
+		wc->reap = sq->reap;
+		sq->reap = 0;
+
+		ibscif_append_cqe(cq, wc, 0);
+	}
+out:
+	spin_unlock_bh(&sq->lock);
+
+	ibscif_notify_cq(cq);
+	return err;
+}
+
+static int ibscif_schedule_rx_completions(struct ibscif_qp *qp, int iq_flag, struct ibscif_rx_state *rx)
+{
+	struct ibscif_cq *cq = to_cq(qp->ibqp.recv_cq);
+	struct ibscif_wq *wq;
+	struct ibscif_wr *wr;
+	struct ibscif_wc *wc;
+	u32 last_in_seq;
+	int index, err, i;
+
+	wq = iq_flag ? &qp->sq /* yep, the SQ */ : &qp->rq;
+	last_in_seq = rx->last_in_seq;
+
+	/* Prevent divide by zero traps on wrap math. */
+	if (!wq->size)
+		return 0;
+
+	spin_lock_bh(&wq->lock);
+	for (i=wq->completions; i<wq->depth; i++) {
+		index = (wq->head + i) % wq->size;
+
+		wr = ibscif_get_wr(wq, index);
+
+		/* Skip over non-IQ entries. */
+		if (iq_flag && 
+		    ((wr->opcode == WR_UD)	      ||
+		     (wr->opcode == WR_SEND)	      ||
+		     (wr->opcode == WR_SEND_WITH_IMM) ||
+		     (wr->opcode == WR_RDMA_WRITE)    ||
+		     (wr->opcode == WR_RDMA_WRITE_WITH_IMM)))
+			continue;
+
+		/*
+		 * If this WR hasn't seen the final segment in sequence then
+		 * there is nothing more to process in this queue.  We use the
+		 * last seen state as a qualifier because last_packet_seq will
+		 * be uninitialized until last packet is seen.
+		 */
+		if ((wr->state != WR_LAST_SEEN) ||
+		    seq_before(last_in_seq, wr->sar.rea.last_packet_seq))
+			break;
+
+		/* Clear references on memory regions. */
+		ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+
+		if (iq_flag) {
+			/*
+			 * Completed IQ replies are defered until earlier
+			 * non-IQ WR have completed.  This is determined 
+			 * with a second iteration of the WQ below.
+			 */
+			wr->state = WR_COMPLETED;
+			continue; /* Look for more IQ completions. */
+		}
+
+		/* All receive queue completions are done here. */
+		err = ibscif_reserve_cqe(cq, &wc);
+		if (unlikely(err)) {
+			spin_unlock_bh(&wq->lock);
+			return err;
+		}
+
+		wc->ibwc.qp	  = &qp->ibqp;
+		wc->ibwc.src_qp	  = qp->remote_qpn;
+		wc->ibwc.wr_id	  = wr->id;
+		wc->ibwc.status	  = IB_WC_SUCCESS;
+		wc->ibwc.byte_len = wr->sar.rea.final_length;
+		wc->ibwc.port_num = 1;
+
+		if (ibscif_pdu_is_immed(wr->sar.rea.opcode)) {
+			DEV_STAT(qp->dev, recv_imm++);
+			wc->ibwc.opcode	  = IB_WC_RECV_RDMA_WITH_IMM;
+			wc->ibwc.ex.imm_data = wr->sar.rea.immediate_data;
+		} else {
+			DEV_STAT(qp->dev, recv++);
+			wc->ibwc.opcode	  = IB_WC_RECV;
+			wc->ibwc.ex.imm_data = 0;
+		}
+
+		wc->wq	 = wq;
+		wc->reap = 1;
+		wq->completions++;
+
+		ibscif_append_cqe(cq, wc, !!ibscif_pdu_is_se(wr->sar.rea.opcode));
+	}
+	spin_unlock_bh(&wq->lock);
+
+	/* If this was the recieve queue, there is no more processing to be done. */
+	if (!iq_flag) {
+		ibscif_notify_cq(cq);
+		return 0;
+	}
+
+	err = ibscif_process_sq_completions(qp);
+	if (unlikely(err))
+		return err;
+
+	/*
+	 * If we just created room for a backlogged IQ stream request
+	 * and there is a tx window, reschedule to get it sent.
+	 */
+	if ((atomic_read(&qp->or_posted) > atomic_read(&qp->or_depth)) &&
+	    (atomic_read(&qp->or_depth) < qp->max_or)		       &&
+	    ibscif_tx_window(&qp->wire.sq.tx))
+		qp->schedule |= SCHEDULE_RESUME | SCHEDULE_SQ;
+
+	return 0;
+}
+
+static enum ibscif_schedule ibscif_process_wq_ack(struct ibscif_wq *wq, u32 seq_num)
+{
+	struct ibscif_tx_state *tx = &wq->wirestate->tx;
+	enum ibscif_schedule status = 0;
+	int throttled, index, err = 0, i;
+
+	if (!wq->size || !wq->depth)
+		return 0;
+
+	/* If this is old news, get out. */
+	if (!seq_after(seq_num, tx->last_ack_seq_recvd))
+		return 0;
+
+	/* Capture if window was closed before updating. */
+	throttled = !ibscif_tx_window(tx);
+	tx->last_ack_seq_recvd = seq_num;
+
+	/*
+	 * If were were throttled and now have an open window or
+	 * simply up to date, resume streaming transfers.  This
+	 * can be overwritten with other schedule states below.
+	 */
+	if (throttled && ibscif_tx_window(tx))
+		status = SCHEDULE_RESUME;
+
+	spin_lock_bh(&wq->lock);
+	for (i=wq->completions; i<wq->depth; i++) {
+		struct ibscif_wr *wr;
+
+		index = (wq->head + i) % wq->size;
+
+		wr = ibscif_get_wr(wq, index);
+
+		/* Get out if the WR hasn't been scheduled. */
+		if (wr->state == WR_WAITING)
+			break;
+
+		if (seq_after(wr->sar.seg.ending_seq, seq_num)) {
+
+			if ((wr->state == WR_STARTED) && !ibscif_tx_unacked_window(tx))
+				status = SCHEDULE_RESUME;
+
+			break;
+		}
+
+		/* We seem to have a completed WQ element. */
+
+		if (is_iq(wq)) {
+			/*
+			 * We have a completed IQ reply.
+			 * Clear references to the memory region.
+			 */
+			ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+
+			/*
+			 * It's more effecient to retire an IQ wqe manually
+			 * here instead of calling ibscif_retire_wqes().
+			 */
+			wq->head   = (wq->head + 1) % wq->size;
+			wq->depth -= 1;
+
+		} else if ((wr->opcode == WR_RDMA_READ)		  ||
+			   (wr->opcode == WR_ATOMIC_CMP_AND_SWP)  ||
+			   (wr->opcode == WR_ATOMIC_FETCH_AND_ADD)||
+			   (wr->opcode == WR_UD && wr->use_rma)   ||
+			   (wr->opcode == WR_SEND && wr->use_rma) ||
+			   (wr->opcode == WR_SEND_WITH_IMM && wr->use_rma) ||
+			   (wr->opcode == WR_RDMA_WRITE && wr->use_rma)    ||
+			   (wr->opcode == WR_RDMA_WRITE_WITH_IMM && wr->use_rma)) {
+			/*
+			 * We have a request acknowledgment.
+			 * Note the state change so it isn't retried.
+			 *
+			 * BTW, these request types are completed in the
+			 * ibscif_schedule_rx_completions() routine when
+			 * the data has arrived.
+			 */
+			if (wr->state == WR_WAITING_FOR_ACK)
+				wr->state = WR_WAITING_FOR_RSP;
+
+		} else if (wr->state != WR_COMPLETED) {
+			/* Request is complete so no need to keep references. */
+			ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+			wr->state = WR_COMPLETED;
+		}
+	}
+	spin_unlock_bh(&wq->lock);
+
+	if (is_sq(wq)) {
+		err = ibscif_process_sq_completions(wq->qp);
+		if (unlikely(err)) {
+			printk(KERN_ALERT PFX "%s: sq completion error: err=%d \n", __func__, err);
+			ibscif_protocol_error(wq->qp, IBSCIF_REASON_QP_FATAL);
+			status = 0;
+		}
+	}
+
+	return status;
+}
+
+static void ibscif_process_ack(struct ibscif_qp *qp, struct base_hdr *hdr)
+{
+	qp->schedule |= ibscif_process_wq_ack(&qp->sq, hdr->sq_ack_num) | SCHEDULE_SQ;
+	qp->schedule |= ibscif_process_wq_ack(&qp->iq, hdr->iq_ack_num) | SCHEDULE_IQ;
+}
+
+/* Note that the WQ lock is held on success. */
+static struct ibscif_wr *ibscif_reserve_wqe(struct ibscif_wq *wq)
+{
+	int err;
+
+	spin_lock_bh(&wq->lock);
+
+	if (unlikely(wq->qp->state != QP_CONNECTED)) {
+		err = -ENOTCONN;
+		goto out;
+	}
+	if (unlikely(!wq->size)) {
+		err = -ENOSPC;
+		goto out;
+	}
+	if (unlikely(wq->depth == wq->size)) {
+		err = -ENOBUFS;
+		goto out;
+	}
+
+	return ibscif_get_wr(wq, wq->tail);
+out:
+	spin_unlock_bh(&wq->lock);
+	return ERR_PTR(err);
+}
+
+/* Note that this assumes the WQ lock is currently held. */
+static void ibscif_append_wqe(struct ibscif_wq *wq)
+{
+	DEV_STAT(wq->qp->dev, wr_opcode[ibscif_get_wr(wq, wq->tail)->opcode]++);
+	ibscif_append_wq(wq);
+	spin_unlock_bh(&wq->lock);
+}
+
+static struct ibscif_wr* ibscif_wr_by_msg_id(struct ibscif_wq *wq, u32 msg_id)
+{
+	struct ibscif_wr *wr;
+	int size = wq->size;
+
+	if (!size)
+		return NULL;
+
+	wr = ibscif_get_wr(wq, msg_id % size);
+	if (wr->use_rma)
+		return (wr->rma_id == msg_id) ? wr : NULL;
+	else
+		return (wr->msg_id == msg_id) ? wr : NULL;
+}
+
+static int ibscif_ds_dma(struct ibscif_qp *qp, struct page **page, u32 page_offset, struct sk_buff *skb, u32 dma_len, int head_copied)
+{
+	void *dst, *src = skb->data;
+	u32 copy_len;
+
+	while (dma_len) {
+		copy_len = min(dma_len, (u32)PAGE_SIZE - page_offset);
+
+		dst = ibscif_map_dst(*page) + page_offset;
+		head_copied = ibscif_atomic_copy(dst, src, copy_len, head_copied);
+		ibscif_unmap_dst(*page, dst);
+
+		src	+= copy_len;
+		dma_len -= copy_len;
+
+		page++;
+		page_offset = 0;
+	}
+
+	return head_copied;
+}
+
+static int ibscif_place_data(struct ibscif_qp *qp, struct ibscif_wr *wr, struct sk_buff *skb,
+			    u32 length, u32 offset, u32 seq_num)
+{
+	struct ibscif_ds *ds;
+	struct ibscif_mr *mr;
+	int seg_num, page_index;
+	u32 dma_len, ds_offset, page_offset;
+	int head_copied = 0;
+
+	if (!length) {
+		ds = NULL;
+		dma_len = 0;
+		ds_offset = 0;
+		goto no_data;
+	}
+
+	/* See if we can use our ds cache. */
+	if (likely((wr->sar.rea.current_ds) && (wr->sar.rea.last_seen_seq == seq_num - 1))) {
+		/* Take the cached entires. */
+		ds = wr->sar.rea.current_ds;
+		mr = ds->mr;
+		ds_offset = wr->sar.rea.current_ds_offset;
+		seg_num = (ds - wr->ds_list) / sizeof *wr->ds_list;
+	} else {
+		ds_offset = offset;
+		ds = wr->ds_list;
+		seg_num = 0;
+		while ((ds_offset >= ds->length) && (seg_num < wr->num_ds)) {
+			ds_offset -= ds->length;
+			ds++;
+			seg_num++;
+		}
+next_ds:
+		if (unlikely(seg_num >= wr->num_ds))
+			return -EMSGSIZE;
+		/*
+		 * A memory region which may have posted receives against it can
+		 * still be freed, therefore, we need to burn the cycles here to
+		 * make sure it's still valid.  We'll take a reference on it now
+		 * that data is coming in.
+		 */
+		if (!ds->in_use) {
+			mr = ibscif_get_mr(ds->lkey);
+			if (unlikely(IS_ERR(mr)))
+				return PTR_ERR(mr);
+			ds->in_use = 1;
+			if (unlikely(mr != ds->mr))
+				return -ENXIO;
+			if (unlikely(!(mr->access & IB_ACCESS_LOCAL_WRITE)))
+				return -EACCES;
+		} else
+			mr = ds->mr;
+	}
+
+	/* Place data for this descriptor.  Routine will handle page boundary crossings. */
+	page_offset  = ds->offset + ds_offset + (mr->addr & ~PAGE_MASK);
+	page_index   = page_offset >> PAGE_SHIFT;
+	page_offset &= ~PAGE_MASK;
+
+	dma_len = min(ds->length - ds_offset, length);
+	head_copied = ibscif_ds_dma(qp, &mr->page[page_index], page_offset, skb, dma_len, head_copied);
+	length -= dma_len;
+	if (length) {
+		ds++;
+		seg_num++;
+		ds_offset = 0;
+		skb_pull(skb, dma_len);
+		goto next_ds;
+	}
+no_data:
+	wr->sar.rea.last_seen_seq = seq_num;
+
+	if (ds && ((ds_offset + dma_len) < ds->length)) {
+		wr->sar.rea.current_ds = ds;
+		wr->sar.rea.current_ds_offset = ds_offset + dma_len;
+	} else
+		wr->sar.rea.current_ds = NULL;	/* Force a validation of the next ds. */
+
+	return 0;
+}
+
+static int ibscif_process_ud(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	int err;
+	int grh_size = 40;
+	int msg_id;
+
+	if (unlikely(qp->ibqp.qp_type != IB_QPT_UD)) {
+		printk(KERN_ALERT PFX "%s: UD packet received on non-UD QP\n", __func__);
+		return -EINVAL;
+	}
+
+	pdu->ud.msg_length = __be32_to_cpu(pdu->ud.msg_length);
+	pdu->ud.msg_offset = __be32_to_cpu(pdu->ud.msg_offset);
+
+	/* Only one pdu is allowed for one UD packet, otherwise drop the pdu */
+	if (unlikely(pdu->ud.msg_length != pdu->hdr.length || pdu->ud.msg_offset)) {
+		printk(KERN_INFO PFX "%s: dropping fragmented UD packet. total_length=%d msg_length=%d msg_offset=%d\n",
+				__func__, pdu->hdr.length, pdu->ud.msg_length, pdu->ud.msg_offset);
+		return -EINVAL;
+	}
+
+	spin_lock_bh(&qp->rq.lock);
+	if (unlikely(qp->rq.ud_msg_id >= qp->rq.next_msg_id)) {
+		spin_unlock_bh(&qp->rq.lock);
+		printk(KERN_ALERT PFX "%s: ERROR: message arrives before recv is posted. msg_id=%d, rq.next_msg_id=%d\n",
+				__func__, pdu->send.msg_id, qp->rq.next_msg_id);
+		return -EBADRQC;
+	}
+	msg_id = qp->rq.ud_msg_id++;
+	spin_unlock_bh(&qp->rq.lock);
+
+	wr = ibscif_wr_by_msg_id(&qp->rq, msg_id);
+	if (unlikely(!wr))
+		return -EBADR;
+
+	if (unlikely((pdu->ud.msg_length + grh_size) > wr->length))
+		return -EMSGSIZE;
+
+	/* GRH is included as part of the received message */
+	skb_pull(skb, sizeof(pdu->ud)-grh_size); 
+
+	err = ibscif_place_data(qp, wr, skb, pdu->hdr.length+grh_size, pdu->ud.msg_offset, pdu->hdr.seq_num);
+	if (unlikely(err))
+		return err;
+
+	wr->state = WR_LAST_SEEN;
+	wr->sar.rea.opcode	    = pdu->hdr.opcode;
+	wr->sar.rea.last_packet_seq = 0;
+	wr->sar.rea.immediate_data  = 0;
+	wr->sar.rea.final_length    = pdu->ud.msg_length+grh_size;
+
+	return 0;
+}
+
+static int ibscif_process_send(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	int err;
+
+	pdu->send.msg_id = __be32_to_cpu(pdu->send.msg_id);
+	spin_lock_bh(&qp->rq.lock);
+	if (unlikely(pdu->send.msg_id >= qp->rq.next_msg_id)) {
+		spin_unlock_bh(&qp->rq.lock);
+		printk(KERN_ALERT PFX "%s: ERROR: message arrives before recv is posted. msg_id=%d, rq.next_msg_id=%d\n",
+				__func__, pdu->send.msg_id, qp->rq.next_msg_id);
+		return -EBADRQC;
+	}
+	spin_unlock_bh(&qp->rq.lock);
+
+	wr = ibscif_wr_by_msg_id(&qp->rq, pdu->send.msg_id);
+	if (unlikely(!wr))
+		return -EBADR;
+
+	pdu->send.msg_length = __be32_to_cpu(pdu->send.msg_length);
+	if (unlikely(pdu->send.msg_length > wr->length))
+		return -EMSGSIZE;
+
+	pdu->send.msg_offset = __be32_to_cpu(pdu->send.msg_offset);
+	if (unlikely(pdu->send.msg_offset > pdu->send.msg_length))
+		return -EINVAL;
+
+	if (unlikely((pdu->hdr.length + pdu->send.msg_offset) > wr->length))
+		return -ESPIPE;
+
+	skb_pull(skb, sizeof(pdu->send));
+
+	err = ibscif_place_data(qp, wr, skb, pdu->hdr.length, pdu->send.msg_offset, pdu->hdr.seq_num);
+	if (unlikely(err))
+		return err;
+
+	if (ibscif_pdu_is_last(pdu->hdr.opcode)) {
+		/*
+		 * We've got the last of the message data.
+		 * We always assume immediate data; if not needed, no harm, on foul.
+		 */
+		wr->state = WR_LAST_SEEN;
+		wr->sar.rea.opcode	    = pdu->hdr.opcode;
+		wr->sar.rea.last_packet_seq = pdu->hdr.seq_num;
+		wr->sar.rea.immediate_data  = __be32_to_cpu(pdu->send.immed_data);
+		wr->sar.rea.final_length    = pdu->send.msg_length;
+	}
+
+	return 0;
+}
+
+static int ibscif_process_write(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	struct ibscif_mr *mr;
+	u64 rdma_addr;
+	u32 rdma_len, page_offset;
+	int page_index;
+
+	if (unlikely(!(qp->access & IB_ACCESS_REMOTE_WRITE)))
+		return -EACCES;
+
+	/* Writes with immediate data consume an rq wqe. */
+	if (ibscif_pdu_is_immed(pdu->hdr.opcode)) {
+		pdu->write.msg_id = __be32_to_cpu(pdu->write.msg_id);
+		spin_lock_bh(&qp->rq.lock);
+		if (unlikely(pdu->write.msg_id >= qp->rq.next_msg_id)) {
+			spin_unlock_bh(&qp->rq.lock);
+			printk(KERN_ALERT PFX "%s: ERROR: message arrives before recv is posted. msg_id=%d, rq.next_msg_id=%d\n",
+					__func__, pdu->write.msg_id, qp->rq.next_msg_id);
+			return -EBADRQC;
+		}
+		spin_unlock_bh(&qp->rq.lock);
+
+		wr = ibscif_wr_by_msg_id(&qp->rq, pdu->write.msg_id);
+		if (unlikely(!wr))
+			return -EBADR;
+	} else
+		wr = NULL;
+
+	skb_pull(skb, sizeof(pdu->write));
+
+	rdma_addr = __be64_to_cpu(pdu->write.rdma_address);
+	rdma_len  = pdu->hdr.length;
+	if (unlikely((rdma_addr + (rdma_len - 1)) < rdma_addr))
+		return -EOVERFLOW;
+
+	mr = ibscif_validate_mr(__be32_to_cpu(pdu->write.rdma_key), rdma_addr,
+			       rdma_len, qp->ibqp.pd, IB_ACCESS_REMOTE_WRITE);
+	if (unlikely(IS_ERR(mr)))
+		return PTR_ERR(mr);
+
+	page_offset = rdma_addr & ~PAGE_MASK;
+	page_index  = ((rdma_addr - mr->addr) + (mr->addr & ~PAGE_MASK)) >> PAGE_SHIFT;
+
+	ibscif_ds_dma(qp, &mr->page[page_index], page_offset, skb, rdma_len, 0);
+
+	ibscif_put_mr(mr);
+
+	if (wr) {
+		wr->sar.rea.final_length += rdma_len;
+		if (ibscif_pdu_is_last(pdu->hdr.opcode)) {
+			/* We've got the last of the write data. */
+			wr->state = WR_LAST_SEEN;
+			wr->sar.rea.opcode	    = pdu->hdr.opcode;
+			wr->sar.rea.last_packet_seq = pdu->hdr.seq_num;
+			wr->sar.rea.immediate_data  = __be32_to_cpu(pdu->write.immed_data);
+		}
+	}
+
+	return 0;
+}
+
+static int ibscif_process_read(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	struct ibscif_mr *mr;
+	u64 rdma_addr;
+	u32 rdma_len;
+
+	if (unlikely(!(qp->access & IB_ACCESS_REMOTE_READ)))
+		return -EACCES;
+
+	rdma_addr = __be64_to_cpu(pdu->read_req.rdma_address);
+	rdma_len  = __be32_to_cpu(pdu->read_req.rdma_length);
+	if (unlikely((rdma_addr + (rdma_len - 1)) < rdma_addr))
+		return -EOVERFLOW;
+
+	mr = ibscif_validate_mr(__be32_to_cpu(pdu->read_req.rdma_key), rdma_addr,
+			       rdma_len, qp->ibqp.pd, IB_ACCESS_REMOTE_READ);
+	if (unlikely(IS_ERR(mr)))
+		return PTR_ERR(mr);
+
+	wr = ibscif_reserve_wqe(&qp->iq);
+	if (unlikely(IS_ERR(wr))) {
+		ibscif_put_mr(mr);
+		return PTR_ERR(wr);
+	}
+
+	memset(&wr->sar, 0, sizeof wr->sar);
+
+	wr->opcode = WR_RDMA_READ_RSP;
+	wr->state  = WR_WAITING;
+	wr->length = rdma_len;
+	wr->msg_id = __be32_to_cpu(pdu->read_req.rdma_id);
+	wr->num_ds = 1;
+	wr->ds_list[0].mr     = mr;
+	wr->ds_list[0].offset = rdma_addr - mr->addr;
+	wr->ds_list[0].length = rdma_len;
+	wr->ds_list[0].in_use = 1;
+
+	ibscif_append_wqe(&qp->iq);
+	qp->schedule |= SCHEDULE_RESUME | SCHEDULE_IQ;
+
+	return 0;
+}
+
+static int ibscif_process_read_rsp(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	int err;
+
+	/* Find the requesting sq wr. */
+	wr = ibscif_wr_by_msg_id(&qp->sq, __be32_to_cpu(pdu->read_rsp.rdma_id));
+	if (unlikely(!wr))
+		return -EBADR;
+	if (unlikely(wr->opcode != WR_RDMA_READ))
+		return -ENOMSG;
+
+	skb_pull(skb, sizeof(pdu->read_rsp));
+
+	pdu->read_rsp.rdma_offset = __be32_to_cpu(pdu->read_rsp.rdma_offset);
+
+	err = ibscif_place_data(qp, wr, skb, pdu->hdr.length, pdu->read_rsp.rdma_offset, pdu->hdr.seq_num);
+	if (unlikely(err))
+		return err;
+
+	if (ibscif_pdu_is_last(pdu->hdr.opcode)) {
+		/* We've got the last of the read data. */
+		wr->state = WR_LAST_SEEN;
+		wr->sar.rea.opcode	    = pdu->hdr.opcode;
+		wr->sar.rea.last_packet_seq = pdu->hdr.seq_num;
+		wr->sar.rea.final_length    = pdu->read_rsp.rdma_offset + pdu->hdr.length;
+	}
+
+	return 0;
+}
+
+static int ibscif_process_atomic_req(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	struct ibscif_mr *mr;
+	struct page *page;
+	u64 *addr;
+	u32 offset, rkey, msg_id;
+	u16 opcode;
+
+	if (unlikely(!(qp->access & IB_ACCESS_REMOTE_ATOMIC)))
+		return -EACCES;
+
+	opcode = ibscif_pdu_base_type(pdu->hdr.opcode);
+	if (opcode == ibscif_op_comp_swap) {
+		addr   = (u64 *)__be64_to_cpu(pdu->comp_swap.atomic_address);
+		rkey   = __be32_to_cpu(pdu->comp_swap.atomic_key);
+		msg_id = __be32_to_cpu(pdu->comp_swap.atomic_id);
+	} else {
+		addr   = (u64 *)__be64_to_cpu(pdu->fetch_add.atomic_address);
+		rkey   = __be32_to_cpu(pdu->fetch_add.atomic_key);
+		msg_id = __be32_to_cpu(pdu->fetch_add.atomic_id);
+	}
+
+	if (unlikely((u64)addr & (sizeof *addr - 1)))
+		return -EADDRNOTAVAIL;
+	if (unlikely((addr + (sizeof *addr - 1)) < addr))
+		return -EOVERFLOW;
+
+	mr = ibscif_validate_mr(rkey, (u64)addr, sizeof *addr, qp->ibqp.pd, IB_ACCESS_REMOTE_ATOMIC);
+	if (unlikely(IS_ERR(mr)))
+		return PTR_ERR(mr);
+
+	wr = ibscif_reserve_wqe(&qp->iq);
+	if (unlikely(IS_ERR(wr))) {
+		ibscif_put_mr(mr);
+		return PTR_ERR(wr);
+	}
+
+	/* Determine which page to map. */
+	offset	= ((u64)addr - mr->addr) + (mr->addr & ~PAGE_MASK);
+	page	= mr->page[offset >> PAGE_SHIFT];
+	offset &= ~PAGE_MASK;
+
+	/* Lock to perform the atomic operation atomically. */
+	spin_lock_bh(&qp->dev->atomic_op);
+
+	addr = ibscif_map_src(page) + offset;
+	wr->atomic_rsp.orig_data = *addr;
+	if (opcode == ibscif_op_fetch_add)
+		*addr += __be64_to_cpu(pdu->fetch_add.add_data);
+	else if (wr->atomic_rsp.orig_data == __be64_to_cpu(pdu->comp_swap.comp_data))
+		*addr  = __be64_to_cpu(pdu->comp_swap.swap_data);
+	ibscif_unmap_src(page, addr);
+
+	ibscif_put_mr(mr);
+
+	/* Atomic operation is complete. */
+	spin_unlock_bh(&qp->dev->atomic_op);
+
+	memset(&wr->sar, 0, sizeof wr->sar);
+
+	wr->opcode = WR_ATOMIC_RSP;
+	wr->state  = WR_WAITING;
+	wr->length = 0;
+	wr->msg_id = msg_id;
+	wr->num_ds = 0;
+	wr->atomic_rsp.opcode = (opcode==ibscif_op_comp_swap)? ibscif_op_comp_swap_rsp : ibscif_op_fetch_add_rsp; 
+	/* The wr->atomic_rsp.orig_data field was set above. */
+
+	ibscif_append_wqe(&qp->iq);
+	qp->schedule |= SCHEDULE_RESUME | SCHEDULE_IQ;
+
+	return 0;
+}
+
+static int ibscif_process_atomic_rsp(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	u16 opcode;
+	int err;
+
+	if (unlikely(!ibscif_pdu_is_last(pdu->atomic_rsp.hdr.opcode)))
+		return -EINVAL;
+
+	/* Find the requesting sq wr. */
+	wr = ibscif_wr_by_msg_id(&qp->sq, __be32_to_cpu(pdu->atomic_rsp.atomic_id));
+	if (unlikely(!wr))
+		return -EBADR;
+
+	opcode = ibscif_pdu_base_type(pdu->hdr.opcode);
+	if (unlikely(wr->opcode != ((opcode == ibscif_op_comp_swap_rsp) ?
+				    WR_ATOMIC_CMP_AND_SWP : WR_ATOMIC_FETCH_AND_ADD)))
+		return -ENOMSG;
+
+	skb_pull(skb, (unsigned long)&pdu->atomic_rsp.orig_data - (unsigned long)pdu);
+
+	pdu->atomic_rsp.orig_data = __be64_to_cpu(pdu->atomic_rsp.orig_data);
+	err = ibscif_place_data(qp, wr, skb, sizeof pdu->atomic_rsp.orig_data, 0, pdu->hdr.seq_num);
+	if (unlikely(err))
+		return err;
+
+	wr->state = WR_LAST_SEEN;
+	wr->sar.rea.opcode	    = pdu->hdr.opcode;
+	wr->sar.rea.last_packet_seq = pdu->hdr.seq_num;
+	wr->sar.rea.final_length    = sizeof pdu->atomic_rsp.orig_data;
+
+	return 0;
+}
+
+static int ibscif_process_disconnect(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	ibscif_qp_remote_disconnect(qp, __be32_to_cpu(pdu->disconnect.reason));
+	return 0;
+}
+
+static int ibscif_process_send_rma(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_ds *ds;
+	struct ibscif_wr *wr;
+	struct ibscif_mr *mr;
+	struct ibscif_mreg_info *mreg;
+	u32 num_rma_addrs;
+	u64 rma_offset;
+	u32 rma_length;
+	u32 total;
+	int seg_num;
+	int cur_rma_addr;
+	u32 xfer_len, ds_offset;
+	int err;
+	u64 loffset;
+	u32 dma_size = 0;
+	int rma_flag = 0;
+
+	if (unlikely(!qp->conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: qp->conn == NULL\n", __func__);
+		return -EACCES;
+	}
+
+	pdu->send.msg_id = __be32_to_cpu(pdu->send.msg_id);
+	spin_lock_bh(&qp->rq.lock);
+	if (unlikely(pdu->send.msg_id >= qp->rq.next_msg_id)) {
+		spin_unlock_bh(&qp->rq.lock);
+		printk(KERN_ALERT PFX "%s: ERROR: message arrives before recv is posted. msg_id=%d, rq.next_msg_id=%d\n",
+				__func__, pdu->send.msg_id, qp->rq.next_msg_id);
+		return -EBADRQC;
+	}
+	spin_unlock_bh(&qp->rq.lock);
+
+	wr = ibscif_wr_by_msg_id(&qp->rq, pdu->send.msg_id);
+	if (unlikely(!wr))
+		return -EBADR;
+
+	pdu->send.msg_length = __be32_to_cpu(pdu->send.msg_length);
+	if (unlikely(pdu->send.msg_length > wr->length))
+		return -EMSGSIZE;
+
+	pdu->send.msg_offset = __be32_to_cpu(pdu->send.msg_offset);
+	if (unlikely(pdu->send.msg_offset > pdu->send.msg_length))
+		return -EINVAL;
+
+	if (unlikely((pdu->hdr.length + pdu->send.msg_offset) > wr->length))
+		return -ESPIPE;
+
+	total = 0;
+
+	num_rma_addrs = __be32_to_cpu(pdu->send.num_rma_addrs);
+	cur_rma_addr = 0;
+	rma_offset = __be64_to_cpu(pdu->send.rma_addrs[cur_rma_addr].offset);
+	rma_length = __be32_to_cpu(pdu->send.rma_addrs[cur_rma_addr].length);
+
+	ds_offset = pdu->send.msg_offset;
+	ds = wr->ds_list;
+	seg_num = 0;
+	while ((ds_offset >= ds->length) && (seg_num < wr->num_ds)) {
+		ds_offset -= ds->length;
+		ds++;
+		seg_num++;
+	}
+
+	err = 0;
+	while (total < pdu->send.msg_length && !err) {
+		if (unlikely(seg_num >= wr->num_ds))
+			return -EMSGSIZE;
+
+		if (!ds->in_use) {
+			mr = ibscif_get_mr(ds->lkey);
+			if (unlikely(IS_ERR(mr)))
+				return PTR_ERR(mr);
+			ds->in_use = 1;
+			if (unlikely(mr != ds->mr))
+				return -ENXIO;
+			if (unlikely(!(mr->access & IB_ACCESS_LOCAL_WRITE)))
+				return -EACCES;
+		} else
+			mr = ds->mr;
+
+		mreg = ibscif_mr_get_mreg(mr, qp->conn);
+		if (!mreg)
+			return -EACCES;
+
+		while (ds->length > ds_offset) {
+			xfer_len = min( ds->length - ds_offset, rma_length );
+			if (xfer_len) {
+				loffset = mreg->offset + ds->offset + ds_offset;
+				dma_size += ibscif_dma_size(xfer_len, rma_offset);
+
+				if ((total + xfer_len >= pdu->send.msg_length) && dma_size)
+					rma_flag = SCIF_RMA_SYNC;
+
+				err = scif_readfrom(qp->conn->ep, loffset, xfer_len, rma_offset, rma_flag);
+				if (err) {
+					printk(KERN_ALERT PFX "%s: scif_readfrom (%d bytes) returns %d\n", __func__, xfer_len, err);
+					break;
+				}
+
+				ds_offset += xfer_len;
+				rma_offset += xfer_len;
+				rma_length -= xfer_len;
+				total += xfer_len;
+
+				if (total >= pdu->send.msg_length)
+					break;
+			}
+			if (rma_length == 0) {
+				cur_rma_addr++;
+				if (unlikely(cur_rma_addr >= num_rma_addrs))
+					return -EMSGSIZE;
+
+				rma_offset = __be64_to_cpu(pdu->send.rma_addrs[cur_rma_addr].offset);
+				rma_length = __be32_to_cpu(pdu->send.rma_addrs[cur_rma_addr].length);
+			}
+		}
+		
+		seg_num++;
+		ds++;
+	}
+
+	wr->state = WR_LAST_SEEN;
+	wr->sar.rea.opcode	    = pdu->hdr.opcode;
+	wr->sar.rea.last_packet_seq = pdu->hdr.seq_num;
+	wr->sar.rea.immediate_data  = __be32_to_cpu(pdu->send.immed_data);
+	wr->sar.rea.final_length    = pdu->send.msg_length;
+
+	/* Respond to the initiator with the result */
+	wr = ibscif_reserve_wqe(&qp->iq);
+	if (unlikely(IS_ERR(wr))) {
+		return PTR_ERR(wr);
+	}
+
+	memset(&wr->sar, 0, sizeof wr->sar);
+
+	wr->opcode = WR_RMA_RSP;
+	wr->state  = WR_WAITING;
+	wr->length = 0;
+	wr->msg_id = __be32_to_cpu(pdu->send.rma_id);
+	wr->num_ds = 0;
+	wr->rma_rsp.xfer_length = total;
+	wr->rma_rsp.error = err;
+
+	ibscif_append_wqe(&qp->iq);
+	qp->schedule |= SCHEDULE_RESUME | SCHEDULE_IQ;
+
+	return 0;
+}
+
+static int ibscif_process_write_rma(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	struct ibscif_mr *mr;
+	u64 rdma_addr;
+	u32 rdma_len;
+	struct ibscif_mreg_info *mreg;
+	u32 num_rma_addrs;
+	u64 rma_offset;
+	u32 rma_length;
+	u32 total;
+	int i;
+	int err;
+	u64 loffset;
+	u32 dma_size = 0;
+	int rma_flag = 0;
+
+	if (unlikely(!qp->conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: qp->conn == NULL\n", __func__);
+		return -EACCES;
+	}
+
+	if (unlikely(!(qp->access & IB_ACCESS_REMOTE_WRITE)))
+		return -EACCES;
+
+	/* Writes with immediate data consume an rq wqe. */
+	if (ibscif_pdu_is_immed(pdu->hdr.opcode)) {
+		pdu->write.msg_id = __be32_to_cpu(pdu->write.msg_id);
+		spin_lock_bh(&qp->rq.lock);
+		if (unlikely(pdu->write.msg_id >= qp->rq.next_msg_id)) {
+			spin_unlock_bh(&qp->rq.lock);
+			return -EBADRQC;
+		}
+		spin_unlock_bh(&qp->rq.lock);
+
+		wr = ibscif_wr_by_msg_id(&qp->rq, pdu->write.msg_id);
+		if (unlikely(!wr))
+			return -EBADR;
+	}
+	else
+		wr = NULL;
+
+	rdma_addr = __be64_to_cpu(pdu->write.rdma_address);
+	rdma_len  = __be32_to_cpu(pdu->write.rma_length);
+	if (unlikely((rdma_addr + (rdma_len - 1)) < rdma_addr))
+		return -EOVERFLOW;
+
+	mr = ibscif_validate_mr(__be32_to_cpu(pdu->write.rdma_key), rdma_addr,
+			       rdma_len, qp->ibqp.pd, IB_ACCESS_REMOTE_WRITE);
+	if (unlikely(IS_ERR(mr)))
+		return PTR_ERR(mr);
+
+	mreg = ibscif_mr_get_mreg(mr, qp->conn);
+	if (!mreg)
+		return -EACCES;
+
+	total = 0;
+	err = 0;
+	num_rma_addrs = __be32_to_cpu(pdu->write.num_rma_addrs);
+	for (i=0; i<num_rma_addrs; i++) {
+		rma_offset = __be64_to_cpu(pdu->write.rma_addrs[i].offset);
+		rma_length = __be32_to_cpu(pdu->write.rma_addrs[i].length);
+
+		if (rdma_len < rma_length)
+			rma_length = rdma_len;
+
+		if (rma_length == 0) 
+			continue;
+
+		loffset = mreg->offset + (rdma_addr - mr->addr) + total;
+		dma_size += ibscif_dma_size(rma_length, rma_offset);
+
+		if ((i==num_rma_addrs-1) && dma_size)
+			rma_flag = SCIF_RMA_SYNC;
+
+		err = scif_readfrom(qp->conn->ep, loffset, rma_length, rma_offset, rma_flag);
+		if (err) {
+			printk(KERN_ALERT PFX "%s: scif_readfrom (%d bytes) returns %d\n", __func__, rma_length, err);
+			break;
+		}
+
+		rdma_len -= rma_length;
+		total += rma_length;
+	}
+
+	ibscif_put_mr(mr);
+
+	if (wr) {
+		wr->sar.rea.final_length    = total;
+		wr->state = WR_LAST_SEEN; 
+		wr->sar.rea.opcode	    = pdu->hdr.opcode;
+		wr->sar.rea.last_packet_seq = pdu->hdr.seq_num;
+		wr->sar.rea.immediate_data  = __be32_to_cpu(pdu->write.immed_data);
+	} 
+
+	/* Respond to the initiator with the result */
+	wr = ibscif_reserve_wqe(&qp->iq);
+	if (unlikely(IS_ERR(wr))) {
+		return PTR_ERR(wr);
+	}
+
+	memset(&wr->sar, 0, sizeof wr->sar);
+
+	wr->opcode = WR_RMA_RSP;
+	wr->state  = WR_WAITING;
+	wr->length = 0;
+	wr->msg_id = __be32_to_cpu(pdu->write.rma_id);
+	wr->num_ds = 0;
+	wr->rma_rsp.xfer_length = total;
+	wr->rma_rsp.error = err;
+
+	ibscif_append_wqe(&qp->iq);
+	qp->schedule |= SCHEDULE_RESUME | SCHEDULE_IQ;
+
+	return 0;
+}
+
+static int ibscif_process_read_rma(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+	struct ibscif_mr *mr;
+	u64 rdma_addr;
+	u32 rdma_len;
+	struct ibscif_mreg_info *mreg;
+	u32 num_rma_addrs;
+	u64 rma_offset;
+	u32 rma_length;
+	u32 total;
+	int i;
+	int err;
+	u64 loffset;
+	u32 dma_size = 0;
+	int rma_flag = 0;
+
+	if (unlikely(!qp->conn)) {
+		printk(KERN_ALERT PFX "%s: ERROR: qp->conn == NULL\n", __func__);
+		return -EACCES;
+	}
+
+	if (unlikely(!(qp->access & IB_ACCESS_REMOTE_READ)))
+		return -EACCES;
+
+	rdma_addr = __be64_to_cpu(pdu->read_req.rdma_address);
+	rdma_len  = __be32_to_cpu(pdu->read_req.rdma_length);
+	if (unlikely((rdma_addr + (rdma_len - 1)) < rdma_addr))
+		return -EOVERFLOW;
+
+	mr = ibscif_validate_mr(__be32_to_cpu(pdu->read_req.rdma_key), rdma_addr,
+			       rdma_len, qp->ibqp.pd, IB_ACCESS_REMOTE_READ);
+	if (unlikely(IS_ERR(mr)))
+		return PTR_ERR(mr);
+
+	mreg = ibscif_mr_get_mreg(mr, qp->conn);
+	if (!mreg)
+		return -EACCES;
+
+	total = 0;
+	err = 0;
+	num_rma_addrs = __be32_to_cpu(pdu->read_req.num_rma_addrs);
+	for (i=0; i<num_rma_addrs; i++) {
+		rma_offset = __be64_to_cpu(pdu->read_req.rma_addrs[i].offset);
+		rma_length = __be32_to_cpu(pdu->read_req.rma_addrs[i].length);
+
+		if (rdma_len < rma_length)
+			rma_length = rdma_len;
+
+		if (rma_length == 0) 
+			continue;
+
+		loffset = mreg->offset + (rdma_addr - mr->addr) + total;
+		dma_size += ibscif_dma_size(rma_length, rma_offset);
+
+		if ((i==num_rma_addrs-1) && dma_size)
+			rma_flag = SCIF_RMA_SYNC;
+
+		err = scif_writeto(qp->conn->ep, loffset, rma_length, rma_offset, rma_flag);
+		if (err) {
+			printk(KERN_ALERT PFX "%s: scif_writeto (%d bytes) returns %d\n", __func__, rma_length, err);
+			break;
+		}
+
+		rdma_len -= rma_length;
+		total += rma_length;
+	}
+
+	ibscif_put_mr(mr);
+
+	/* Respond to the initiator with the result */
+	wr = ibscif_reserve_wqe(&qp->iq);
+	if (unlikely(IS_ERR(wr))) {
+		return PTR_ERR(wr);
+	}
+
+	memset(&wr->sar, 0, sizeof wr->sar);
+
+	wr->opcode = WR_RMA_RSP;
+	wr->state  = WR_WAITING;
+	wr->length = 0;
+	wr->msg_id = __be32_to_cpu(pdu->read_req.rdma_id);
+	wr->num_ds = 0;
+	wr->rma_rsp.xfer_length = total;
+	wr->rma_rsp.error = err;
+
+	ibscif_append_wqe(&qp->iq);
+	qp->schedule |= SCHEDULE_RESUME | SCHEDULE_IQ;
+
+	return 0;
+}
+
+static int ibscif_process_rma_rsp(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	struct ibscif_wr *wr;
+
+	wr = ibscif_wr_by_msg_id(&qp->sq, __be32_to_cpu(pdu->rma_rsp.rma_id));
+	if (unlikely(!wr))
+		return -EBADR;
+	if (unlikely(!wr->use_rma))
+		return -ENOMSG;
+
+	if (wr->opcode == WR_RDMA_READ) {
+		/* ibscif_clear_ds_refs() is called in ibscif_schedule_rx_completions() */
+		wr->state = WR_LAST_SEEN;
+	}
+	else {
+		ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+		wr->state = WR_COMPLETED;
+	}
+
+	wr->sar.rea.opcode	    = pdu->hdr.opcode;
+	wr->sar.rea.last_packet_seq = pdu->hdr.seq_num;
+	wr->sar.rea.final_length    = pdu->rma_rsp.xfer_length;
+
+	return 0;
+}
+
+static int ibscif_process_pdu(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct sk_buff *skb)
+{
+	int err;
+
+	switch (ibscif_pdu_base_type(pdu->hdr.opcode)) {
+	case ibscif_op_ud:
+		err = ibscif_process_ud(qp, pdu, skb);
+		break;
+	case ibscif_op_send:
+		err = ibscif_process_send(qp, pdu, skb);
+		break;
+	case ibscif_op_write:
+		err = ibscif_process_write(qp, pdu, skb);
+		break;
+	case ibscif_op_read:
+		err = ibscif_process_read(qp, pdu, skb);
+		break;
+	case ibscif_op_read_rsp:
+		err = ibscif_process_read_rsp(qp, pdu, skb);
+		break;
+	case ibscif_op_comp_swap_rsp:
+	case ibscif_op_fetch_add_rsp:
+		err = ibscif_process_atomic_rsp(qp, pdu, skb);
+		break;
+	case ibscif_op_comp_swap:
+	case ibscif_op_fetch_add:
+		err = ibscif_process_atomic_req(qp, pdu, skb);
+		break;
+	case ibscif_op_ack:
+		/* Handled in piggyback ack processing. */
+		err = 0;
+		break;
+	case ibscif_op_disconnect:
+		/* Post send completions before the disconnect flushes the queues. */
+		ibscif_process_ack(qp, &pdu->hdr);
+		/* Now disconnect the QP. */
+		err = ibscif_process_disconnect(qp, pdu, skb);
+		break;
+	case ibscif_op_send_rma:
+		err = ibscif_process_send_rma(qp, pdu, skb);
+		break;
+	case ibscif_op_write_rma:
+		err = ibscif_process_write_rma(qp, pdu, skb);
+		break;
+	case ibscif_op_read_rma:
+		err = ibscif_process_read_rma(qp, pdu, skb);
+		break;
+	case ibscif_op_rma_rsp:
+		err = ibscif_process_rma_rsp(qp, pdu, skb);
+		break;
+	default:
+		printk(KERN_INFO PFX "Received invalid opcode (%x)\n",
+		       ibscif_pdu_base_type(pdu->hdr.opcode));
+		err = IBSCIF_REASON_INVALID_OPCODE;
+		break;
+	}
+
+	if (unlikely(err)) {
+		printk(KERN_ALERT PFX "%s: ERROR: err=%d, opcode=%d\n", __func__, err, ibscif_pdu_base_type(pdu->hdr.opcode));
+		ibscif_protocol_error(qp, IBSCIF_REASON_QP_FATAL);
+	}
+
+	return err;
+}
+
+static int update_rx_seq_numbers(struct ibscif_qp *qp, union ibscif_pdu *pdu, struct ibscif_rx_state *rx)
+{
+	u32 seq_num = pdu->hdr.seq_num;
+
+	if (pdu->hdr.opcode == ibscif_op_ack)
+		return 0;
+
+	if (seq_num != rx->last_in_seq + 1)
+		return 0;
+
+	rx->last_in_seq = seq_num;
+
+	return 1;
+}
+
+static void ibscif_process_qp_skb(struct ibscif_qp *qp, struct sk_buff *skb)
+{
+	union ibscif_pdu *pdu = (union ibscif_pdu *)skb->data;
+	struct ibscif_rx_state *rx;
+	int err = 0;
+
+	/* Start with no scheduling. */
+	qp->schedule = 0;
+
+	rx = ibscif_pdu_is_iq(pdu->hdr.opcode) ? &qp->wire.iq.rx : &qp->wire.sq.rx;
+
+	if (ibscif_process_pdu(qp, pdu, skb) == IBSCIF_REASON_INVALID_OPCODE)
+		return;
+
+	/* skip ack and seq_num for UD QP */
+	if (qp->ibqp.qp_type == IB_QPT_UD) {
+		err = ibscif_schedule_rx_completions(qp, 0, rx);
+		if (unlikely(err)) {
+			printk(KERN_ALERT PFX "%s: rx completion error: err=%d, opcode=%d\n", __func__, err, ibscif_pdu_base_type(pdu->hdr.opcode));
+			ibscif_protocol_error(qp, IBSCIF_REASON_QP_FATAL);
+		}
+		goto done;
+	}
+
+	/* Process piggybacked acks. */
+	ibscif_process_ack(qp, &pdu->hdr);
+
+	if (update_rx_seq_numbers(qp, pdu, rx)) {
+		/* PDU is in sequence so schedule/remove completed work requests. */
+		err = ibscif_schedule_rx_completions(qp, ibscif_pdu_is_iq(pdu->hdr.opcode), rx);
+		if (unlikely(err)) {
+			printk(KERN_ALERT PFX "%s: rx completion error: err=%d, opcode=%d\n", __func__, err, ibscif_pdu_base_type(pdu->hdr.opcode));
+			ibscif_protocol_error(qp, IBSCIF_REASON_QP_FATAL);
+			goto done;
+		}
+	}
+
+	/* Generate an ack if forced or if the current window dictates it. */
+	if (ibscif_pdu_is_force_ack(pdu->hdr.opcode)) {
+		ibscif_send_ack(qp);
+	} else if (pdu->hdr.opcode != ibscif_op_ack) {
+		u32 window = ibscif_rx_window(rx);
+		if (window && (window % (window_size / MIN_WINDOW_SIZE)) == 0)
+			ibscif_send_ack(qp);
+	}
+done:
+	/* Run the scheduler if it was requested. */
+	if (qp->schedule & SCHEDULE_RESUME) {
+		if (qp->schedule & SCHEDULE_SQ)
+			ibscif_schedule(&qp->sq);
+		if (qp->schedule & SCHEDULE_IQ)
+			ibscif_schedule(&qp->iq);
+	}
+
+	return;
+}
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,21)
+#define skb_mac_header(skb) (skb->mac.raw) 
+#endif
+
+static int ibscif_recv_pkt(struct sk_buff *skb, struct ibscif_dev *dev, scif_epd_t ep, struct ibscif_conn *conn)
+{
+	union ibscif_pdu *pdu = (union ibscif_pdu *)skb->data;
+	struct ibscif_qp *qp = ERR_PTR(-ENOENT);
+
+	/* Convert the base header. */
+	pdu->hdr.opcode	    = __be16_to_cpu(pdu->hdr.opcode);
+	pdu->hdr.length	    = __be16_to_cpu(pdu->hdr.length);
+	pdu->hdr.dst_qp	    = __be32_to_cpu(pdu->hdr.dst_qp);
+	pdu->hdr.src_qp	    = __be32_to_cpu(pdu->hdr.src_qp);
+	pdu->hdr.seq_num    = __be32_to_cpu(pdu->hdr.seq_num);
+	pdu->hdr.sq_ack_num = __be32_to_cpu(pdu->hdr.sq_ack_num);
+	pdu->hdr.iq_ack_num = __be32_to_cpu(pdu->hdr.iq_ack_num);
+
+	if (pdu->hdr.opcode == ibscif_op_close) {
+		//printk(KERN_INFO PFX "%s: op_close, conn=%p, local_close=%d\n", __func__, conn, conn->local_close);
+		conn->remote_close = 1;
+		goto done_no_qp;
+	}
+	else if (pdu->hdr.opcode == ibscif_op_reopen) {
+		//printk(KERN_INFO PFX "%s: op_reopen, conn=%p, local_close=%d\n", __func__, conn, conn->local_close);
+		conn->remote_close = 0;
+		goto done_no_qp;
+	}
+	else if (pdu->hdr.opcode == ibscif_op_cm) {
+		ibscif_process_cm_skb(skb, conn);
+		goto done_no_qp;
+	}
+
+	qp = ibscif_get_qp(pdu->hdr.dst_qp);
+	if (unlikely(IS_ERR(qp)				  ||
+		     (qp->state != QP_CONNECTED && qp->ibqp.qp_type != IB_QPT_UD) ||
+		     (qp->ibqp.qp_num != pdu->hdr.dst_qp) ||
+		     (qp->remote_qpn != pdu->hdr.src_qp && qp->ibqp.qp_type != IB_QPT_UD) ||
+		     0)) {
+		/* Disconnect the rogue. */
+		ibscif_reflect_disconnect(qp, &pdu->hdr, skb, IBSCIF_REASON_INVALID_QP);
+		goto done;
+	}
+
+	if (qp->ibqp.qp_type == IB_QPT_UD)
+		ibscif_qp_add_ud_conn(qp, conn);
+
+	DEV_STAT(qp->dev, packets_rcvd++);
+	DEV_STAT(qp->dev, bytes_rcvd += skb->len);
+
+	ibscif_process_qp_skb(qp, skb);
+done:
+	if (likely(!IS_ERR(qp)))
+		ibscif_put_qp(qp);
+
+done_no_qp:
+	kfree_skb(skb);
+	return 0;
+}
+
+void ibscif_do_recv( struct ibscif_dev *dev, scif_epd_t ep, struct ibscif_conn *conn )
+{
+	struct sk_buff *skb;
+	union ibscif_pdu *pdu;
+	int hdr_size, payload_size, recv_size, pdu_size;
+	char *recv_buffer;
+	int ret;
+
+	skb = dev_alloc_skb(IBSCIF_MTU + sizeof(struct ud_hdr)); /* allow full UD payload */
+	if (unlikely(skb==NULL)) {
+		printk(KERN_ALERT PFX "%s(): fail to allocate skb, exiting\n", __func__);
+		return;
+	}
+
+	skb->protocol  = IBSCIF_PACKET_TYPE;
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+	skb->priority  = TC_PRIO_CONTROL;	/* highest defined priority */
+	skb->dev       = (void *) dev;
+
+	pdu = (union ibscif_pdu *)skb->data;
+
+	/* get the base header first so the packet size can be determinied */
+	recv_size = sizeof(pdu->hdr);
+	recv_buffer = (char *)&pdu->hdr;
+	while (recv_size) {
+		ret = scif_recv(ep, recv_buffer, recv_size, blocking_recv ? SCIF_RECV_BLOCK : 0);
+		if (ret < 0) {
+			printk(KERN_ALERT PFX "%s(): fail to receive hdr, ret=%d, expecting %d\n", __func__, ret, (int)recv_size);
+			if (ret == -ENOTCONN || ret == -ECONNRESET) {
+				if (verbose)
+					printk(KERN_INFO PFX "%s: ep disconnected by peer (%d). conn=%p, local_close=%d\n",
+							__func__, ret, conn, conn->local_close);
+				ibscif_remove_ep( dev, ep );
+				ibscif_refresh_pollep_list();
+				conn->remote_close = 1;
+				if (conn->local_close) {
+					ibscif_free_conn(conn);
+				}
+			}
+			goto errout;
+		}
+		recv_size -= ret;
+		recv_buffer += ret;
+	}
+
+	hdr_size = __be16_to_cpu(pdu->hdr.hdr_size);
+	payload_size = __be16_to_cpu(pdu->hdr.length);
+	pdu_size = hdr_size + payload_size;
+	if (unlikely(payload_size > IBSCIF_MTU)) {
+		printk(KERN_ALERT PFX "%s(): payload exceeds MTU, size=%d\n",
+		       __func__, payload_size);
+		goto errout;
+	}
+
+	recv_size = pdu_size - sizeof(pdu->hdr);
+	recv_buffer = (char *)pdu + sizeof(pdu->hdr);
+
+	/* get the remaining of the packet */
+	//printk(KERN_INFO PFX "%s(): hdr_size=%d payload_size=%d pdu_size=%d recv_size=%d\n", __func__, hdr_size, payload_size, pdu_size, recv_size);
+	ret = 0;
+	while (recv_size) {
+		ret = scif_recv(ep, recv_buffer, recv_size, blocking_recv ? SCIF_RECV_BLOCK : 0);
+
+		if (ret < 0) {
+			printk(KERN_ALERT PFX "%s(): fail to receive data, ret=%d, expecting %d\n", __func__, ret, recv_size);
+			break;
+		}
+
+		recv_size -= ret;
+		recv_buffer += ret;
+	}
+
+	if (ret < 0) 
+		goto errout;
+
+	skb->len       = pdu_size;
+	skb->data_len  = payload_size;
+	skb->tail     += pdu_size;
+
+	ibscif_recv_pkt(skb, dev, ep, conn);
+	return;
+
+errout:
+	kfree_skb(skb);
+}
+
+#define IBSCIF_MAX_POLL_COUNT (IBSCIF_MAX_DEVICES * 2)
+static struct scif_pollepd	poll_eps[IBSCIF_MAX_POLL_COUNT];
+static struct ibscif_dev	*poll_devs[IBSCIF_MAX_POLL_COUNT];
+static int			poll_types[IBSCIF_MAX_POLL_COUNT];
+static struct ibscif_conn	*poll_conns[IBSCIF_MAX_POLL_COUNT];
+static struct task_struct	*poll_thread = NULL;
+static atomic_t			poll_eps_changed = ATOMIC_INIT(0);
+static volatile int		poll_thread_running = 0;
+
+void ibscif_refresh_pollep_list( void )
+{
+	atomic_set(&poll_eps_changed, 1);
+}
+
+int ibscif_poll_thread( void *unused )
+{
+	int poll_count = 0;
+	int ret;
+	int i;
+	int busy;
+	int idle_count = 0;
+
+	poll_thread_running = 1;
+	while (!kthread_should_stop()) {
+		if (atomic_xchg(&poll_eps_changed, 0)) {
+			poll_count = IBSCIF_MAX_POLL_COUNT;
+			ibscif_get_pollep_list( poll_eps, poll_devs, poll_types, poll_conns, &poll_count );
+		}
+
+		if (poll_count == 0) {
+			schedule();
+			continue;
+		}
+
+		ret = scif_poll(poll_eps, poll_count, 1000); /* 1s timeout */
+
+		busy = 0;
+		if (ret > 0) {
+			for (i=0; i<poll_count; i++) {
+				if (poll_eps[i].revents & POLLIN) {
+					if (poll_types[i] == IBSCIF_EP_TYPE_LISTEN) { 
+						ibscif_do_accept( poll_devs[i] );
+						busy = 1;
+					}
+					else {
+						ibscif_do_recv( poll_devs[i], poll_eps[i].epd, poll_conns[i] );
+						busy = 1;
+					}
+				}
+				else if (poll_eps[i].revents & POLLERR) {
+					if (verbose)
+						printk(KERN_INFO PFX "%s: ep error, conn=%p.\n", __func__, poll_conns[i]);
+					ibscif_remove_ep( poll_devs[i], poll_eps[i].epd );
+					ibscif_refresh_pollep_list();
+					/* in most the case, the error is caused by ep being already closed */
+					busy = 1;
+				}
+				else if (poll_eps[i].revents & POLLHUP) {
+					struct ibscif_conn *conn = poll_conns[i];
+					if (verbose)
+						printk(KERN_INFO PFX "%s: ep disconnected by peer.\n", __func__);
+					ibscif_remove_ep( poll_devs[i], poll_eps[i].epd );
+					ibscif_refresh_pollep_list();
+					if (conn) {
+						if (verbose)
+							printk(KERN_INFO PFX "%s: conn=%p, local_close=%d.\n", __func__, conn, conn->local_close);
+						conn->remote_close = 1;
+						if (conn->local_close) {
+							ibscif_free_conn(conn);	
+						}
+					}
+					busy = 1;
+				}
+			}
+		}
+
+		if (busy) {
+			idle_count = 0;
+		}
+		else {
+			idle_count++;
+			/* close unused endpoint after 60 seconds */
+			if (idle_count == 60) {
+				if (ibscif_cleanup_idle_conn())
+					ibscif_refresh_pollep_list();
+				idle_count = 0;
+			}
+			/* pick up the unprocessed items in the xmit queue */
+			if (!skb_queue_empty(&xmit_queue))
+				ibscif_dev_queue_xmit(NULL);
+			schedule();
+		}
+	}
+
+	poll_thread_running = 0;
+	return 0;
+}
+
+void ibscif_protocol_init_pre(void)
+{
+	skb_queue_head_init(&xmit_queue);
+}
+
+void ibscif_protocol_init_post(void)
+{
+	poll_thread = kthread_run( ibscif_poll_thread, NULL, "ibscif_polld" );
+}
+
+void ibscif_protocol_cleanup(void)
+{
+	kthread_stop( poll_thread );
+
+	while (poll_thread_running)
+		schedule();
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_protocol.h b/drivers/infiniband/hw/scif/ibscif_protocol.h
--- a/drivers/infiniband/hw/scif/ibscif_protocol.h	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_protocol.h	2016-04-14 13:33:08.865411106 -0700
@@ -0,0 +1,395 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef IBSCIF_PROTOCOL_H
+#define IBSCIF_PROTOCOL_H
+
+/*
+ * Protocol EtherType
+ */
+#define	IBSCIF_PACKET_TYPE	0x8086
+
+/*
+ * Base protocol header version
+ */
+#define	IBSCIF_PROTOCOL_VER_1	1
+#define	IBSCIF_PROTOCOL_VER	IBSCIF_PROTOCOL_VER_1
+
+/*
+ * Protocol opcode values - All other values are reserved.
+ */
+#define ibscif_last_flag		0x4000
+#define ibscif_immed_flag		0x2000
+#define ibscif_se_flag			0x1000
+#define ibscif_force_ack_flag		0x0800
+#define ibscif_iq_flag			0x0400
+
+#define	ibscif_op_send			0
+#define	ibscif_op_send_last		(ibscif_op_send | ibscif_last_flag)
+#define	ibscif_op_send_last_se		(ibscif_op_send | ibscif_last_flag  | ibscif_se_flag)
+#define	ibscif_op_send_immed		(ibscif_op_send | ibscif_immed_flag)
+#define	ibscif_op_send_immed_se		(ibscif_op_send | ibscif_immed_flag | ibscif_se_flag)
+
+#define	ibscif_op_write			1
+#define	ibscif_op_write_last		(ibscif_op_write | ibscif_last_flag)
+#define	ibscif_op_write_immed		(ibscif_op_write | ibscif_immed_flag)
+#define	ibscif_op_write_immed_se	(ibscif_op_write | ibscif_immed_flag | ibscif_se_flag)
+
+#define	ibscif_op_read			2
+#define	ibscif_op_read_rsp		(ibscif_op_read | ibscif_iq_flag)
+#define	ibscif_op_read_rsp_last		(ibscif_op_read_rsp | ibscif_last_flag)
+
+#define	ibscif_op_comp_swap		3
+#define ibscif_op_comp_swap_rsp		(ibscif_op_comp_swap | ibscif_iq_flag)
+
+#define	ibscif_op_fetch_add		4
+#define ibscif_op_fetch_add_rsp		(ibscif_op_fetch_add | ibscif_iq_flag)
+
+#define	ibscif_op_ack			5
+#define	ibscif_op_disconnect		6
+
+#define ibscif_op_send_rma		7
+#define ibscif_op_send_rma_se		(ibscif_op_send_rma | ibscif_se_flag)
+#define ibscif_op_send_rma_immed	(ibscif_op_send_rma | ibscif_immed_flag)
+#define ibscif_op_send_rma_immed_se	(ibscif_op_send_rma | ibscif_immed_flag | ibscif_se_flag)
+
+#define ibscif_op_write_rma		8
+#define ibscif_op_write_rma_immed	(ibscif_op_write_rma | ibscif_immed_flag)
+#define ibscif_op_write_rma_immed_se	(ibscif_op_write_rma | ibscif_immed_flag | ibscif_se_flag)
+
+#define	ibscif_op_read_rma		9
+#define ibscif_op_rma_rsp		(10 | ibscif_iq_flag)
+
+#define	ibscif_op_reg			11
+#define	ibscif_op_dereg			12
+
+#define ibscif_op_close			13
+#define ibscif_op_reopen		14
+
+#define ibscif_op_ud			15
+#define ibscif_op_cm			16
+
+#define ibscif_pdu_is_last(op)		(op & ibscif_last_flag)
+#define ibscif_pdu_is_immed(op)		(op & ibscif_immed_flag)
+#define ibscif_pdu_is_se(op)		(op & ibscif_se_flag)
+#define ibscif_pdu_is_force_ack(op)	(op & ibscif_force_ack_flag)
+#define ibscif_pdu_is_iq(op)		(op & ibscif_iq_flag)
+
+#define ibscif_pdu_set_last(op)		(op | ibscif_last_flag)
+#define ibscif_pdu_set_immed(op)	(op | ibscif_immed_flag)
+#define ibscif_pdu_set_se(op)		(op | ibscif_se_flag)
+#define ibscif_pdu_set_force_ack(op)	(op | ibscif_force_ack_flag)
+#define ibscif_pdu_set_iq(op)		(op | ibscif_iq_flag)
+
+#define ibscif_pdu_base_type(op)	\
+	(op & ~(ibscif_last_flag       | \
+		ibscif_se_flag         | \
+		ibscif_immed_flag      | \
+		ibscif_force_ack_flag))
+
+/*
+ * Remote address descriptor for SCIF RMA operations
+ */
+struct rma_addr {
+	__be64			offset;
+	__be32			length;
+	__be32			reserved;
+} __attribute__ ((packed));
+
+/*
+ * Base header present in every packet
+ */
+struct base_hdr {
+	__be16			opcode;
+	__be16			length;
+	__be32			dst_qp;
+	__be32			src_qp;
+	__be32			seq_num;
+	__be32			sq_ack_num;
+	__be32			iq_ack_num;
+	__be16			hdr_size;
+	__be16			reserved[3];
+} __attribute__ ((packed));
+
+/*
+ * UD Header
+ */
+struct ud_hdr {
+	struct base_hdr		hdr;
+	__be32			msg_id;
+	__be32			msg_length;
+	__be32			msg_offset;
+	u8			grh[40];
+} __attribute__ ((packed));
+
+/*
+ * Send Header
+ */
+struct send_hdr {
+	struct base_hdr		hdr;
+	__be32			msg_id;
+	__be32			msg_length;
+	__be32			msg_offset;
+	__be32			immed_data;
+	__be32			rma_id;		/* RMA */
+	__be32			num_rma_addrs;	/* RMA */
+	struct rma_addr		rma_addrs[0];	/* RMA */
+} __attribute__ ((packed));
+
+/*
+ * RDMA Write Header
+ */
+struct write_hdr {
+	struct base_hdr		hdr;
+	__be64			rdma_address;
+	__be32			rdma_key;
+	__be32			immed_data;
+	__be32			msg_id;
+	__be32			rma_length;	/* RMA */
+	__be32			rma_id;		/* RMA */
+	__be32			num_rma_addrs;	/* RMA */
+	struct rma_addr		rma_addrs[0];	/* RMA */
+} __attribute__ ((packed));
+
+/*
+ * RDMA Read Request Header
+ */
+struct read_req_hdr {
+	struct base_hdr		hdr;
+	__be64			rdma_address;
+	__be32			rdma_key;
+	__be32			rdma_length;	/* shared with RMA */
+	__be32			rdma_id;	/* shared with RMA */
+	__be32			num_rma_addrs;	/* RMA */
+	struct rma_addr		rma_addrs[0];	/* RMA */
+} __attribute__ ((packed));
+
+/*
+ * RDMA Read Response Header
+ */
+struct read_rsp_hdr {
+	struct base_hdr		hdr;
+	__be32			rdma_offset;
+	__be32			rdma_id;
+} __attribute__ ((packed));
+
+
+/*
+ * Atomic Compare and Swap Header
+ */
+struct comp_swap_hdr {
+	struct base_hdr		hdr;
+	__be64			atomic_address;
+	__be64			comp_data;
+	__be64			swap_data;
+	__be32			atomic_key;
+	__be32			atomic_id;
+	/* no pad needed */
+} __attribute__ ((packed));
+
+
+/*
+ * Atomic Fetch/Add Header
+ */
+struct fetch_add_hdr {
+	struct base_hdr		hdr;
+	__be64			atomic_address;
+	__be64			add_data;
+	__be32			atomic_key;
+	__be32			atomic_id;
+	/* no pad needed */
+} __attribute__ ((packed));
+
+/*
+ * Atomic Response Header
+ */
+struct atomic_rsp_hdr {
+	struct base_hdr		hdr;
+	__be64			orig_data;
+	__be32			atomic_id;
+} __attribute__ ((packed));
+
+/*
+ * ACK Header
+ */
+struct ack_hdr {
+	struct base_hdr		hdr;
+} __attribute__ ((packed));
+
+/*
+ * Disconnect Header
+ */
+struct disconnect_hdr {
+	struct base_hdr		hdr;
+	__be32			reason;
+} __attribute__ ((packed));
+
+/*
+ * RMA Response Header
+ */
+struct rma_rsp_hdr {
+	struct base_hdr		hdr;
+	__be32			rma_id;
+	__be32			xfer_length;
+	__be32			error;
+} __attribute__ ((packed));
+
+/*
+ * MR Reg/Dereg Info Header
+ */
+struct reg_hdr {
+	struct base_hdr		hdr;
+	__be64			scif_offset;
+	__be64			address;
+	__be32			length;
+	__be32			rkey;
+	__be32			access;
+} __attribute__ ((packed));
+
+/*
+ * SCIF endpoint close notiffication
+ */
+struct close_hdr {
+	struct base_hdr		hdr;
+} __attribute__ ((packed));
+
+
+#define IBSCIF_CM_REQ	1
+#define IBSCIF_CM_REP	2
+#define IBSCIF_CM_REJ	3
+#define IBSCIF_CM_RTU	4
+
+/*
+ * RDMA CM Header
+ */
+
+struct cm_hdr {
+	struct base_hdr		hdr;
+	__be64			req_ctx;
+	__be64			rep_ctx;
+	__be32			cmd;
+	__be32			port;
+	__be32			qpn;
+	__be32			status;
+	__be32			plen;
+	u8			pdata[0];
+} __attribute__ ((packed));
+
+enum ibscif_reason {	/* Set each value to simplify manual lookup. */
+
+	/* Local Events */
+	IBSCIF_REASON_USER_GENERATED	  = 0,
+	IBSCIF_REASON_CQ_COMPLETION	  = 1,
+	IBSCIF_REASON_NIC_FATAL		  = 2,
+	IBSCIF_REASON_NIC_REMOVED	  = 3,
+
+	/* Disconnect Event */
+	IBSCIF_REASON_DISCONNECT		  = 4,
+
+	/* CQ Error */
+	IBSCIF_REASON_CQ_OVERRUN		  = 5,
+	IBSCIF_REASON_CQ_FATAL		  = 6,
+
+	/* QP Errors */
+	IBSCIF_REASON_QP_SQ_ERROR	  = 7,
+	IBSCIF_REASON_QP_RQ_ERROR	  = 8,
+	IBSCIF_REASON_QP_DESTROYED	  = 9,
+	IBSCIF_REASON_QP_ERROR		  = 10,
+	IBSCIF_REASON_QP_FATAL		  = 11,
+
+	/* Operation Errors */
+	IBSCIF_REASON_INVALID_OPCODE	  = 12,
+	IBSCIF_REASON_INVALID_LENGTH	  = 13,
+	IBSCIF_REASON_INVALID_QP		  = 14,
+	IBSCIF_REASON_INVALID_MSG_ID	  = 15,
+	IBSCIF_REASON_INVALID_LKEY	  = 16,
+	IBSCIF_REASON_INVALID_RDMA_RKEY	  = 17,
+	IBSCIF_REASON_INVALID_RDMA_ID	  = 18,
+	IBSCIF_REASON_INVALID_ATOMIC_RKEY  = 19,
+	IBSCIF_REASON_INVALID_ATOMIC_ID	  = 20,
+	IBSCIF_REASON_MAX_IR_EXCEEDED	  = 21,
+	IBSCIF_REASON_ACK_TIMEOUT	  = 22,
+
+	/* Protection Errors */
+	IBSCIF_REASON_PROTECTION_VIOLATION = 23,
+	IBSCIF_REASON_BOUNDS_VIOLATION	  = 24,
+	IBSCIF_REASON_ACCESS_VIOLATION	  = 25,
+	IBSCIF_REASON_WRAP_ERROR		  = 26
+};
+
+union ibscif_pdu {
+	struct base_hdr		hdr;
+	struct ud_hdr		ud;
+	struct send_hdr 	send;
+	struct write_hdr	write;
+	struct read_req_hdr	read_req;
+	struct read_rsp_hdr	read_rsp;
+	struct comp_swap_hdr	comp_swap;
+	struct fetch_add_hdr	fetch_add;
+	struct atomic_rsp_hdr	atomic_rsp;
+	struct ack_hdr		ack;
+	struct disconnect_hdr	disconnect;
+	struct rma_rsp_hdr	rma_rsp;
+	struct reg_hdr		reg;
+	struct close_hdr	close;
+	struct cm_hdr		cm;
+};
+
+struct ibscif_full_frame {
+	union ibscif_pdu	ibscif;
+};
+
+static inline int seq_before(u32 seq1, u32 seq2)
+{
+	return (s32)(seq1 - seq2) < 0;
+}
+
+static inline int seq_after(u32 seq1, u32 seq2)
+{
+	return (s32)(seq2 - seq1) < 0;
+}
+
+static inline int seq_between(u32 seq_target, u32 seq_low, u32 seq_high)
+{
+	return seq_high - seq_low >= seq_target - seq_low;
+}
+
+static inline u32 seq_window(u32 earlier, u32 later)
+{
+	return earlier > later ? ((u32)~0 - earlier) + later : later - earlier;
+}
+
+#define ibscif_tx_unacked_window(tx)	seq_window((tx)->last_ack_seq_recvd, (tx)->next_seq - 1)
+
+#define ibscif_rx_window(rx)		seq_window((rx)->last_seq_acked, (rx)->last_in_seq)
+
+#define ibscif_tx_window(tx)		((u32)window_size - ibscif_tx_unacked_window(tx))
+
+#endif /* IBSCIF_PROTOCOL_H */
diff -ruN a/drivers/infiniband/hw/scif/ibscif_provider.c b/drivers/infiniband/hw/scif/ibscif_provider.c
--- a/drivers/infiniband/hw/scif/ibscif_provider.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_provider.c	2016-04-14 13:33:08.866411081 -0700
@@ -0,0 +1,410 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+static int ibscif_query_device(struct ib_device *ibdev, struct ib_device_attr *attr)
+{
+	memset(attr, 0, sizeof *attr);
+
+	attr->vendor_id           = VENDOR_ID;
+	attr->vendor_part_id      = DEVICE_ID;
+	attr->hw_ver              = HW_REV;
+	attr->fw_ver              = FW_REV;
+	attr->device_cap_flags    = IB_DEVICE_PORT_ACTIVE_EVENT;
+	attr->max_mr_size         = MAX_MR_SIZE;
+	attr->page_size_cap       = PAGE_SIZE;
+	attr->max_qp              = MAX_QPS;
+	attr->max_qp_wr           = MAX_QP_SIZE;
+	attr->max_sge             = MAX_SGES;
+	attr->max_cq              = MAX_CQS;
+	attr->max_cqe             = MAX_CQ_SIZE;
+	attr->max_mr              = MAX_MRS;
+	attr->max_pd              = MAX_PDS;
+	attr->max_qp_rd_atom      = MAX_IR>255 ? 255 : MAX_IR;
+	attr->max_qp_init_rd_atom = MAX_OR>255 ? 255 : MAX_OR;
+	attr->max_res_rd_atom     = MAX_IR>255 ? 255 : MAX_IR;
+	attr->atomic_cap          = IB_ATOMIC_HCA;
+	attr->sys_image_guid	  = ibdev->node_guid;
+
+	return 0;
+}
+
+static int ibscif_query_port(struct ib_device *ibdev, u8 port, struct ib_port_attr *attr)
+{
+	struct ibscif_dev *dev = to_dev(ibdev);
+
+	memset(attr, 0, sizeof *attr);
+
+	/* See IB Spec r1.2 Table 145 for physical port state values. */
+	attr->lid	   = IBSCIF_NODE_ID_TO_LID(dev->node_id);
+	attr->sm_lid	   = 1;
+	attr->gid_tbl_len  = 1;
+	attr->pkey_tbl_len = 1;
+	attr->max_msg_sz   = MAX_MR_SIZE;
+	attr->phys_state   = 5; /* LinkUp */ 
+	attr->state	   = IB_PORT_ACTIVE;
+	attr->max_mtu	   = IB_MTU_4096;
+	attr->active_mtu   = IB_MTU_4096;
+	attr->active_width = IB_WIDTH_4X;
+	attr->active_speed = 4;
+	attr->max_vl_num   = 1;
+	attr->port_cap_flags = IB_PORT_SM_DISABLED;
+
+	return 0;
+}
+
+static int ibscif_query_pkey(struct ib_device *ibdev, u8 port, u16 index, u16 *pkey)
+{
+	*pkey = 0xffff;	/* IB_DEFAULT_PKEY_FULL */
+	return 0;
+}
+
+static int ibscif_query_gid(struct ib_device *ibdev, u8 port, int index, union ib_gid *ibgid)
+{
+	struct ibscif_dev *dev = to_dev(ibdev);
+
+	memcpy(ibgid, &dev->gid, sizeof(*ibgid));
+	return 0;
+}
+
+static struct ib_ucontext *ibscif_alloc_ucontext(struct ib_device *ibdev, struct ib_udata *udata)
+{
+	struct ib_ucontext *context = kzalloc(sizeof *context, GFP_KERNEL);
+	return (!context) ? ERR_PTR(-ENOMEM) : context;
+}
+
+static int ibscif_dealloc_ucontext(struct ib_ucontext *context)
+{
+	kfree(context);
+	return 0;
+}
+
+static void ibscif_generate_eui64(struct ibscif_dev *dev, u8 *eui64)
+{
+	memcpy(eui64, dev->netdev->dev_addr, 3);
+	eui64[3] = 0xFF;
+	eui64[4] = 0xFE;
+	memcpy(eui64+5, dev->netdev->dev_addr+3, 3);
+}
+
+static int ibscif_register_device(struct ibscif_dev *dev)
+{
+	strncpy(dev->ibdev.node_desc, DRV_SIGNON, sizeof dev->ibdev.node_desc);
+	ibscif_generate_eui64(dev, (u8 *)&dev->ibdev.node_guid);
+	dev->ibdev.owner		= THIS_MODULE;
+	dev->ibdev.uverbs_abi_ver	= UVERBS_ABI_VER;
+	dev->ibdev.uverbs_cmd_mask	=
+		(1ull << IB_USER_VERBS_CMD_GET_CONTEXT)		|
+		(1ull << IB_USER_VERBS_CMD_QUERY_DEVICE)	|
+		(1ull << IB_USER_VERBS_CMD_QUERY_PORT)		|
+		(1ull << IB_USER_VERBS_CMD_ALLOC_PD)		|
+		(1ull << IB_USER_VERBS_CMD_DEALLOC_PD)		|
+		(1ull << IB_USER_VERBS_CMD_CREATE_AH)		|
+		(1ull << IB_USER_VERBS_CMD_DESTROY_AH)		|
+		(1ull << IB_USER_VERBS_CMD_CREATE_QP)		|
+		(1ull << IB_USER_VERBS_CMD_QUERY_QP)		|
+		(1ull << IB_USER_VERBS_CMD_MODIFY_QP)		|
+		(1ull << IB_USER_VERBS_CMD_DESTROY_QP)		|
+		(1ull << IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL)	|
+		(1ull << IB_USER_VERBS_CMD_CREATE_CQ)		|
+		(1ull << IB_USER_VERBS_CMD_RESIZE_CQ)		|
+		(1ull << IB_USER_VERBS_CMD_DESTROY_CQ)		|
+		(1ull << IB_USER_VERBS_CMD_POLL_CQ)		|
+		(1ull << IB_USER_VERBS_CMD_REQ_NOTIFY_CQ)	|
+		(1ull << IB_USER_VERBS_CMD_REG_MR)		|
+		(1ull << IB_USER_VERBS_CMD_DEREG_MR)		|
+		(1ull << IB_USER_VERBS_CMD_POST_SEND)		|
+		(1ull << IB_USER_VERBS_CMD_POST_RECV);		
+#if	defined(MOFED) && !defined(MOFED_2_1)
+	dev->ibdev.node_type		= new_ib_type ? RDMA_EXP_NODE_MIC : RDMA_NODE_RNIC;
+#else
+	dev->ibdev.node_type		= new_ib_type ? RDMA_NODE_MIC : RDMA_NODE_RNIC;
+#endif
+	dev->ibdev.phys_port_cnt	= 1;
+
+	dev->ibdev.query_device		= ibscif_query_device;		// Mandatory
+	dev->ibdev.num_comp_vectors     = 1;				// Mandatory
+	dev->ibdev.query_port		= ibscif_query_port;		// Mandatory
+	dev->ibdev.query_pkey		= ibscif_query_pkey;		// Mandatory
+	dev->ibdev.query_gid		= ibscif_query_gid;		// Mandatory
+	dev->ibdev.alloc_ucontext	= ibscif_alloc_ucontext;	// Required
+	dev->ibdev.dealloc_ucontext	= ibscif_dealloc_ucontext;	// Required
+	dev->ibdev.alloc_pd		= ibscif_alloc_pd;		// Mandatory
+	dev->ibdev.dealloc_pd		= ibscif_dealloc_pd;		// Mandatory
+	dev->ibdev.create_ah		= ibscif_create_ah;		// Mandatory
+	dev->ibdev.destroy_ah		= ibscif_destroy_ah;		// Mandatory
+	dev->ibdev.create_qp		= ibscif_create_qp;		// Mandatory
+	dev->ibdev.query_qp		= ibscif_query_qp;		// Optional
+	dev->ibdev.modify_qp		= ibscif_modify_qp;		// Mandatory
+	dev->ibdev.destroy_qp		= ibscif_destroy_qp;		// Mandatory
+	dev->ibdev.create_cq		= ibscif_create_cq;		// Mandatory
+	dev->ibdev.resize_cq		= ibscif_resize_cq;		// Optional
+	dev->ibdev.destroy_cq		= ibscif_destroy_cq;		// Mandatory
+	dev->ibdev.poll_cq		= ibscif_poll_cq;		// Mandatory
+	dev->ibdev.req_notify_cq	= ibscif_arm_cq;		// Mandatory
+	dev->ibdev.get_dma_mr		= ibscif_get_dma_mr;		// Mandatory
+	dev->ibdev.reg_phys_mr		= ibscif_reg_phys_mr;		// Required
+	dev->ibdev.reg_user_mr		= ibscif_reg_user_mr;		// Required
+	dev->ibdev.dereg_mr		= ibscif_dereg_mr;		// Mandatory
+	dev->ibdev.post_send		= ibscif_post_send;		// Mandatory
+	dev->ibdev.post_recv		= ibscif_post_receive;		// Mandatory
+	dev->ibdev.dma_ops              = &ibscif_dma_mapping_ops;	// ??
+
+	dev->ibdev.iwcm = kzalloc(sizeof(struct iw_cm_verbs), GFP_KERNEL);
+	if (!dev->ibdev.iwcm)
+		return -ENOMEM;
+
+	dev->ibdev.iwcm->connect = ibscif_cm_connect;
+	dev->ibdev.iwcm->accept = ibscif_cm_accept;
+	dev->ibdev.iwcm->reject = ibscif_cm_reject;
+	dev->ibdev.iwcm->create_listen = ibscif_cm_create_listen;
+	dev->ibdev.iwcm->destroy_listen = ibscif_cm_destroy_listen;
+	dev->ibdev.iwcm->add_ref = ibscif_cm_add_ref;
+	dev->ibdev.iwcm->rem_ref = ibscif_cm_rem_ref;
+	dev->ibdev.iwcm->get_qp = ibscif_cm_get_qp;
+
+	return ib_register_device(&dev->ibdev, NULL);
+}
+
+static void ibscif_dev_release(struct device *dev)
+{
+	kfree(dev);
+}
+
+/*
+ * Hold devlist_mutex during this call for synchronization as needed.
+ * Upon return, dev is invalid.
+ */
+static void ibscif_remove_dev(struct ibscif_dev *dev)
+{
+	struct ibscif_conn *conn, *next;
+
+	if (dev->ibdev.reg_state == IB_DEV_REGISTERED)
+		ib_unregister_device(&dev->ibdev);
+
+	WARN_ON(!list_empty(&dev->wq_list));
+
+	down(&devlist_mutex);
+	list_del(&dev->entry);
+	up(&devlist_mutex);
+
+	ibscif_refresh_pollep_list();
+
+	down(&dev->mutex); 
+	list_for_each_entry_safe(conn, next, &dev->conn_list, entry) {
+		scif_close(conn->ep);
+		list_del(&conn->entry);
+		kfree(conn);
+	}
+	up(&dev->mutex);
+
+	if (dev->listen_ep)
+		scif_close(dev->listen_ep);
+	ibscif_procfs_remove_dev(dev);
+
+	dev_put(dev->netdev);
+	device_unregister(dev->ibdev.dma_device);
+	ib_dealloc_device(&dev->ibdev);
+}
+
+static void ibscif_remove_one(struct net_device *netdev)
+{
+        struct ibscif_dev *dev, *next;
+ 
+        list_for_each_entry_safe(dev, next, &devlist, entry) {
+                if (netdev == dev->netdev) {
+                        ibscif_remove_dev(dev); 
+                        break;
+                }
+        }
+}
+
+static int node_cnt;
+static uint16_t node_ids[IBSCIF_MAX_DEVICES];
+static uint16_t my_node_id;
+
+static void ibscif_add_one(struct net_device *netdev)
+{
+	static int dev_cnt;
+	static dma_addr_t dma_mask = -1;
+	struct ibscif_dev *dev;
+	int ret;
+
+	dev = (struct ibscif_dev *)ib_alloc_device(sizeof *dev);
+	if (!dev) {
+		printk(KERN_ALERT PFX "%s: fail to allocate ib_device\n", __func__);
+		return;
+	}
+
+	INIT_LIST_HEAD(&dev->conn_list);
+	INIT_LIST_HEAD(&dev->mr_list);
+	init_MUTEX(&dev->mr_list_mutex);
+	init_MUTEX(&dev->mutex);
+	spin_lock_init(&dev->atomic_op);
+	INIT_LIST_HEAD(&dev->wq_list);
+	atomic_set(&dev->available, 256); /* FIXME */
+
+	dev_hold(netdev);
+	dev->netdev = netdev;
+
+	/* use the MAC address of the netdev as the GID so that RDMA CM can
+	 * find the ibdev from the IP address associated with the netdev.
+	 */
+	memcpy(&dev->gid, dev->netdev->dev_addr, ETH_ALEN);
+
+	dev->ibdev.dma_device = kzalloc(sizeof *dev->ibdev.dma_device, GFP_KERNEL);
+	if (!dev->ibdev.dma_device) {
+		printk(KERN_ALERT PFX "%s: fail to allocate dma_device\n", __func__);
+		goto out_free_ibdev;
+	}
+
+	snprintf(dev->name, IBSCIF_NAME_SIZE, "scif_dma_%d", dev_cnt);
+	snprintf(dev->ibdev.name, IB_DEVICE_NAME_MAX, "scif%d", dev_cnt++);
+	dev->ibdev.dma_device->release = ibscif_dev_release;
+	dev->ibdev.dma_device->init_name = dev->name;
+	dev->ibdev.dma_device->dma_mask = &dma_mask;
+	ret = device_register(dev->ibdev.dma_device);
+	if (ret) {
+		printk(KERN_ALERT PFX "%s: fail to register dma_device, ret=%d\n", __func__, ret);
+		kfree(dev->ibdev.dma_device); 
+		goto out_free_ibdev;
+	}
+
+	/* Notice: set up listen ep before inserting to devlist */
+
+	dev->listen_ep = scif_open();
+	if (!dev->listen_ep || IS_ERR(dev->listen_ep)) {
+		printk(KERN_ALERT PFX "%s: scif_open returns %ld\n", __func__, PTR_ERR(dev->listen_ep));
+		goto out_unreg_dmadev ;
+	}
+
+	ret = scif_get_nodeIDs( node_ids, IBSCIF_MAX_DEVICES, &my_node_id);
+	if (ret < 0) {
+		printk(KERN_ALERT PFX "%s: scif_get_nodeIDS returns %d\n",
+			__func__, ret);
+		goto out_close_ep;
+	}
+
+	node_cnt = ret;
+	dev->node_id = my_node_id;
+	printk(KERN_ALERT PFX "%s: my node_id is %d\n", __func__, dev->node_id);
+
+	ret = scif_bind(dev->listen_ep, SCIF_OFED_PORT_0);
+	if (ret < 0) {
+		printk(KERN_ALERT PFX "%s: scif_bind returns %d, port=%d\n",
+			__func__, ret, SCIF_OFED_PORT_0);
+		goto out_close_ep;
+	}
+
+	ret = scif_listen(dev->listen_ep, IBSCIF_MAX_DEVICES);
+	if (ret < 0) {
+		printk(KERN_ALERT PFX "%s: scif_listen returns %d\n", __func__, ret);
+		goto out_close_ep;
+	}
+
+	down(&devlist_mutex);
+	list_add_tail(&dev->entry, &devlist);
+	up(&devlist_mutex);
+
+	if (ibscif_register_device(dev))
+		ibscif_remove_dev(dev);
+	else
+		ibscif_procfs_add_dev(dev);
+
+	ibscif_refresh_pollep_list();
+
+	return;
+
+out_close_ep:
+	scif_close(dev->listen_ep);
+
+out_unreg_dmadev:
+	device_unregister(dev->ibdev.dma_device); /* it will free the memory, too */
+
+out_free_ibdev:
+	ib_dealloc_device(&dev->ibdev);
+}
+
+static int ibscif_notifier(struct notifier_block *nb, unsigned long event, void *ptr)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,11,0)
+	struct net_device *netdev = ((struct netdev_notifier_info *) ptr)->dev;
+#else
+	struct net_device *netdev = (struct net_device *)ptr;
+#endif
+
+	if (strcmp(netdev->name, "mic0"))
+		return NOTIFY_DONE;
+
+	switch(event) {
+	  case NETDEV_REGISTER:
+		ibscif_add_one(netdev);
+		ibscif_protocol_init_post();
+		break;
+
+	  case NETDEV_UNREGISTER:
+		ibscif_remove_one(netdev);
+		break;
+
+	  default:
+		/* we only care about the MAC address, ignore other notifications */
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block ibscif_notifier_block = {
+	.notifier_call = ibscif_notifier,
+};
+
+int ibscif_dev_init(void)
+{
+	int err = 0;
+
+	ibscif_protocol_init_pre();
+
+	err = register_netdevice_notifier(&ibscif_notifier_block);
+	if (err) 
+		ibscif_protocol_cleanup();
+
+	return err;
+}
+
+void ibscif_dev_cleanup(void)
+{
+	struct ibscif_dev *dev, *next;
+
+	ibscif_protocol_cleanup();
+	unregister_netdevice_notifier(&ibscif_notifier_block);
+	list_for_each_entry_safe(dev, next, &devlist, entry)
+		ibscif_remove_dev(dev);
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_qp.c b/drivers/infiniband/hw/scif/ibscif_qp.c
--- a/drivers/infiniband/hw/scif/ibscif_qp.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_qp.c	2016-04-14 13:33:08.866411081 -0700
@@ -0,0 +1,868 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+static struct ibscif_wr *ibscif_alloc_wr(struct ibscif_wq *wq, int new_size, int bytes)
+{
+	if (new_size && (new_size != wq->size)) {
+		struct ibscif_wr *new_wr = vzalloc(bytes);
+		return new_wr ? new_wr : ERR_PTR(-ENOMEM);
+	}
+	return NULL;
+}
+
+static void ibscif_move_wr(struct ibscif_wq *wq, struct ibscif_wr *new_wr, int new_size)
+{
+	int i;
+
+	if (wq->size == new_size)
+		return;
+
+	for (i = 0; i < wq->depth; i++) {
+		memcpy(&new_wr[i], &wq->wr[wq->head], wq->wr_size);
+		wq->head = (wq->head + 1) % wq->size;
+	}
+
+	if (wq->wr) {
+		vfree(wq->wr);
+	}
+
+	wq->wr   = new_wr;
+	wq->head = 0;
+	wq->tail = wq->depth;
+	wq->size = new_size;
+}
+
+/* Caller must provide proper synchronization. */
+static int ibscif_resize_qp(struct ibscif_qp *qp, int sq_size, int rq_size, int iq_size)
+{
+	struct ibscif_wr *new_sq, *new_rq, *new_iq;
+	int sq_bytes, rq_bytes, iq_bytes;
+	int old_npages, new_npages, err;
+
+	sq_bytes = PAGE_ALIGN(sq_size * qp->sq.wr_size);
+	rq_bytes = PAGE_ALIGN(rq_size * qp->rq.wr_size);
+	iq_bytes = PAGE_ALIGN(iq_size * qp->iq.wr_size);
+
+	sq_size = sq_bytes / qp->sq.wr_size;
+	rq_size = rq_bytes / qp->rq.wr_size;
+	iq_size = iq_bytes / qp->iq.wr_size;
+
+	if ((sq_size == qp->sq.size) &&
+	    (rq_size == qp->rq.size) &&
+	    (iq_size == qp->iq.size))
+		return 0;
+
+	if ((sq_size < qp->sq.depth) ||
+	    (rq_size < qp->rq.depth) ||
+	    (iq_size < qp->iq.depth))
+		return -EINVAL;
+
+	/* Calculate the number of new pages required for this allocation. */
+	new_npages = (sq_bytes + rq_bytes + iq_bytes) >> PAGE_SHIFT;
+	old_npages = (PAGE_ALIGN(qp->sq.size * qp->sq.wr_size) +
+		      PAGE_ALIGN(qp->rq.size * qp->rq.wr_size) +
+		      PAGE_ALIGN(qp->iq.size * qp->iq.wr_size)) >> PAGE_SHIFT;
+	new_npages -= old_npages;
+
+	if (new_npages > 0) {
+		err = ibscif_reserve_quota(&new_npages);
+		if (err)
+			return err;
+	}
+
+	new_sq = ibscif_alloc_wr(&qp->sq, sq_size, sq_bytes);
+	new_rq = ibscif_alloc_wr(&qp->rq, rq_size, rq_bytes);
+	new_iq = ibscif_alloc_wr(&qp->iq, iq_size, iq_bytes);
+	if (IS_ERR(new_sq) || IS_ERR(new_rq) || IS_ERR(new_iq))
+		goto out;
+
+	ibscif_move_wr(&qp->sq, new_sq, sq_size);
+	ibscif_move_wr(&qp->rq, new_rq, rq_size);
+	ibscif_move_wr(&qp->iq, new_iq, iq_size);
+
+	if (new_npages < 0)
+		ibscif_release_quota(-new_npages);
+
+	return 0;
+out:
+	if (new_sq && !IS_ERR(new_sq))
+		vfree(new_sq);
+	if (new_rq && !IS_ERR(new_rq))
+		vfree(new_rq);
+	if (new_iq && !IS_ERR(new_iq))
+		vfree(new_iq);
+
+	return -ENOMEM;
+}
+
+static int ibscif_init_wqs(struct ibscif_qp *qp, struct ib_qp_init_attr *attr)
+{
+	spin_lock_init(&qp->sq.lock);
+	spin_lock_init(&qp->rq.lock);
+	spin_lock_init(&qp->iq.lock);
+
+	qp->sq.qp = qp;
+	qp->rq.qp = qp;
+	qp->iq.qp = qp;
+
+	qp->sq.wirestate = &qp->wire.sq;
+	qp->iq.wirestate = &qp->wire.iq;
+
+	qp->sq.max_sge = attr->cap.max_send_sge;
+	qp->rq.max_sge = attr->cap.max_recv_sge;
+	qp->iq.max_sge = 1;
+
+	qp->sq.wr_size = sizeof *qp->sq.wr + (sizeof *qp->sq.wr->ds_list * qp->sq.max_sge);
+	qp->rq.wr_size = sizeof *qp->rq.wr + (sizeof *qp->rq.wr->ds_list * qp->rq.max_sge);
+	qp->iq.wr_size = sizeof *qp->iq.wr + (sizeof *qp->iq.wr->ds_list * qp->iq.max_sge);
+
+	return ibscif_resize_qp(qp, attr->cap.max_send_wr, attr->cap.max_recv_wr, (rma_threshold==0x7FFFFFFF)?0:attr->cap.max_send_wr);
+}
+
+static void ibscif_reset_tx_state(struct ibscif_tx_state *tx)
+{
+	tx->next_seq	       = 1;
+	tx->last_ack_seq_recvd = 0;
+	tx->next_msg_id	       = 0;
+}
+
+static void ibscif_reset_rx_state(struct ibscif_rx_state *rx)
+{
+	rx->last_in_seq	       = 0;
+	rx->last_seq_acked     = 0;
+	rx->defer_in_process   = 0;
+}
+
+static void ibscif_reset_wirestate(struct ibscif_wirestate *wirestate)
+{
+	ibscif_reset_tx_state(&wirestate->tx);
+	ibscif_reset_rx_state(&wirestate->rx);
+}
+
+static void ibscif_reset_wire(struct ibscif_wire *wire)
+{
+	ibscif_reset_wirestate(&wire->sq);
+	ibscif_reset_wirestate(&wire->iq);
+}
+
+static void ibscif_init_wire(struct ibscif_wire *wire)
+{
+	ibscif_reset_wire(wire);
+}
+
+static void ibscif_query_qp_cap(struct ibscif_qp *qp, struct ib_qp_cap *cap)
+{
+	memset(cap, 0, sizeof *cap);
+	cap->max_send_wr  = qp->sq.size;
+	cap->max_recv_wr  = qp->rq.size;
+	cap->max_send_sge = qp->sq.max_sge;
+	cap->max_recv_sge = qp->rq.max_sge;
+}
+
+struct ib_qp *ibscif_create_qp(struct ib_pd *ibpd, struct ib_qp_init_attr *attr, struct ib_udata *udata)
+{
+	struct ibscif_dev *dev = to_dev(ibpd->device);
+	struct ibscif_qp *qp;
+	int err;
+
+	if ((attr->qp_type != IB_QPT_RC && attr->qp_type != IB_QPT_UD) ||
+	    (attr->cap.max_send_wr  > MAX_QP_SIZE)    ||
+	    (attr->cap.max_recv_wr  > MAX_QP_SIZE)    ||
+	    (attr->cap.max_send_sge > MAX_SGES)	      ||
+	    (attr->cap.max_recv_sge > MAX_SGES)	      ||
+	    (attr->cap.max_send_wr && !attr->send_cq) ||
+	    (attr->cap.max_recv_wr && !attr->recv_cq))
+		return ERR_PTR(-EINVAL);
+
+	if (!atomic_add_unless(&dev->qp_cnt, 1, MAX_QPS))
+		return ERR_PTR(-EAGAIN);
+
+	qp = kzalloc(sizeof *qp, GFP_KERNEL);
+	if (!qp) {
+		atomic_dec(&dev->qp_cnt);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	qp->local_node_id = dev->node_id;
+
+	kref_init(&qp->ref);
+	init_completion(&qp->done);
+	init_MUTEX(&qp->modify_mutex);
+	spin_lock_init(&qp->lock);
+	ibscif_init_wire(&qp->wire);
+	qp->sq_policy = attr->sq_sig_type;
+	qp->dev	      = dev;
+	qp->mtu	      = IBSCIF_MTU; /* FIXME */
+	qp->state     = QP_IDLE;
+
+	err = ibscif_init_wqs(qp, attr);
+	if (err)
+		goto out;
+
+	ibscif_query_qp_cap(qp, &attr->cap);
+
+	err = ibscif_wiremap_add(qp, &qp->ibqp.qp_num);
+	if (err)
+		goto out;
+
+	qp->magic = QP_MAGIC;
+
+	ibscif_scheduler_add_qp(qp);
+	qp->in_scheduler = 1;
+
+	return &qp->ibqp;
+out:
+	ibscif_destroy_qp(&qp->ibqp);
+	return ERR_PTR(err);
+}
+
+static inline enum ib_qp_state to_ib_qp_state(enum ibscif_qp_state state)
+{
+	switch (state) {
+	case QP_IDLE:		return IB_QPS_INIT;
+	case QP_CONNECTED:	return IB_QPS_RTS;
+	case QP_DISCONNECT:	return IB_QPS_SQD;
+	case QP_ERROR:		return IB_QPS_ERR;
+	case QP_RESET:		return IB_QPS_RESET;
+	default:		return -1;
+	}
+}
+
+static inline enum ibscif_qp_state to_ibscif_qp_state(enum ib_qp_state state)
+{
+	switch (state) {
+	case IB_QPS_INIT:	return QP_IDLE;
+	case IB_QPS_RTS:	return QP_CONNECTED;
+	case IB_QPS_SQD:	return QP_DISCONNECT;
+	case IB_QPS_ERR:	return QP_ERROR;
+	case IB_QPS_RESET:	return QP_RESET;
+	case IB_QPS_RTR:	return QP_IGNORE;
+	default:		return -1;
+	}
+}
+
+/* Caller must provide proper synchronization. */
+static void __ibscif_query_qp(struct ibscif_qp *qp, struct ib_qp_attr *attr, struct ib_qp_init_attr *init_attr)
+{
+	struct ib_qp_cap cap;
+
+	ibscif_query_qp_cap(qp, &cap);
+
+	if (attr) {
+		attr->qp_state		 = to_ib_qp_state(qp->state);
+		attr->cur_qp_state	 = attr->qp_state;
+		attr->port_num		 = 1;
+		attr->path_mtu		 = qp->mtu;
+		attr->dest_qp_num	 = qp->remote_qpn;
+		attr->qp_access_flags	 = qp->access;
+		attr->max_rd_atomic	 = qp->max_or;
+		attr->max_dest_rd_atomic = qp->iq.size;
+		attr->cap		 = cap;
+	}
+
+	if (init_attr) {
+		init_attr->qp_type	 = qp->ibqp.qp_type;
+		init_attr->sq_sig_type	 = qp->sq_policy;
+		init_attr->cap		 = cap;
+	}
+}
+
+int ibscif_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr, int attr_mask, struct ib_qp_init_attr *init_attr)
+{
+	struct ibscif_qp *qp = to_qp(ibqp);
+
+	memset(attr, 0, sizeof *attr);
+	memset(init_attr, 0, sizeof *init_attr);
+
+	spin_lock_bh(&qp->lock);
+	__ibscif_query_qp(qp, attr, init_attr);
+	spin_unlock_bh(&qp->lock);
+
+	return 0;
+}
+
+static int ibscif_flush_wq(struct ibscif_wq *wq, struct ibscif_cq *cq)
+{
+	struct ibscif_wr *wr;
+	struct ibscif_wc *wc;
+	int i, num_wr, err;
+
+	/* Prevent divide by zero traps on wrap math. */
+	if (!wq->size)
+		return 0;
+
+	spin_lock_bh(&wq->lock);
+	for (i = (wq->head + wq->completions) % wq->size, num_wr = 0;
+	     wq->depth && (wq->completions != wq->depth);
+	     i = (i + 1) % wq->size, num_wr++) {
+
+		wr = ibscif_get_wr(wq, i);
+
+		ibscif_clear_ds_refs(wr->ds_list, wr->num_ds);
+
+		if (!cq) {
+			wq->completions++;
+			continue;
+		}
+
+		err = ibscif_reserve_cqe(cq, &wc);
+		if (err) {
+			num_wr = err;
+			break;
+		}
+
+		wc->ibwc.qp	  = &wq->qp->ibqp;
+		wc->ibwc.src_qp	  = wq->qp->remote_qpn;
+		wc->ibwc.wr_id	  = wr->id;
+		wc->ibwc.opcode	  = is_rq(wq) ? IB_WC_RECV : to_ib_wc_opcode(wr->opcode);
+		wc->ibwc.status	  = IB_WC_WR_FLUSH_ERR;
+		wc->ibwc.ex.imm_data = 0;
+		wc->ibwc.byte_len = 0;
+		wc->ibwc.port_num = 1;
+
+		wc->wq	 = wq;
+		wc->reap = wq->reap + 1;
+		wq->reap = 0;
+		wq->completions++;
+
+		ibscif_append_cqe(cq, wc, 0);
+	}
+	spin_unlock_bh(&wq->lock);
+
+	if (num_wr && cq)
+		ibscif_notify_cq(cq);
+
+	return num_wr;
+}
+
+static void ibscif_flush_wqs(struct ibscif_qp *qp)
+{
+	int ret;
+
+	ret = ibscif_flush_wq(&qp->sq, to_cq(qp->ibqp.send_cq));
+	if (ret) /* A clean SQ flush should have done nothing. */
+		qp->state = QP_ERROR;
+
+	ret = ibscif_flush_wq(&qp->rq, to_cq(qp->ibqp.recv_cq));
+	if (ret < 0)
+		qp->state = QP_ERROR;
+
+	ibscif_flush_wq(&qp->iq, NULL);
+}
+
+static void ibscif_reset_wq(struct ibscif_wq *wq, struct ibscif_cq *cq)
+{
+	ibscif_clear_cqes(cq, wq);
+
+	wq->head	= 0;
+	wq->tail	= 0;
+	wq->depth	= 0;
+	wq->reap	= 0;
+	wq->next_wr	= 0;
+	wq->next_msg_id	= 0;
+	wq->completions = 0;
+}
+
+static void ibscif_reset_wqs(struct ibscif_qp *qp)
+{
+	ibscif_reset_wq(&qp->sq, to_cq(qp->ibqp.send_cq));
+	ibscif_reset_wq(&qp->rq, to_cq(qp->ibqp.recv_cq));
+	ibscif_reset_wq(&qp->iq, NULL);
+}
+
+static void ibscif_qp_event(struct ibscif_qp *qp, enum ib_event_type event)
+{
+	if (qp->ibqp.event_handler) {
+		struct ib_event record;
+		record.event	  = event;
+		record.device	  = qp->ibqp.device;
+		record.element.qp = &qp->ibqp;
+		qp->ibqp.event_handler(&record, qp->ibqp.qp_context);
+	}
+}
+
+/* Caller must provide proper synchronization. */
+static void ibscif_qp_error(struct ibscif_qp *qp)
+{
+	if (qp->state == QP_ERROR)
+		return;
+
+	if (qp->state == QP_CONNECTED)
+		ibscif_send_disconnect(qp, IBSCIF_REASON_DISCONNECT);
+
+	qp->state = QP_ERROR;
+
+	ibscif_flush_wqs(qp);
+
+	ibscif_cm_async_callback(qp->cm_context);
+	qp->cm_context = NULL;
+
+	/* don't generate the error event because transitioning to IB_QPS_ERR 
+           state is normal when a QP is disconnected */
+
+	//ibscif_qp_event(qp, IB_EVENT_QP_FATAL);
+}
+
+/* Caller must provide proper synchronization. */
+static void ibscif_qp_reset(struct ibscif_qp *qp)
+{
+	if (qp->state == QP_RESET)
+		return;
+
+	if (qp->state == QP_CONNECTED)
+		ibscif_send_disconnect(qp, IBSCIF_REASON_DISCONNECT);
+
+	ibscif_reset_wqs(qp);
+	ibscif_reset_wire(&qp->wire);
+
+	ibscif_cm_async_callback(qp->cm_context);
+	qp->cm_context = NULL;
+
+	qp->state = QP_RESET;
+}
+
+/* Caller must provide proper synchronization. */
+void ibscif_qp_idle(struct ibscif_qp *qp)
+{
+	if (qp->state == QP_IDLE)
+		return;
+
+	ibscif_reset_wqs(qp);
+	ibscif_reset_wire(&qp->wire);
+
+	qp->state = QP_IDLE;
+}
+
+/* Caller must provide proper synchronization. */
+static void ibscif_qp_connect(struct ibscif_qp *qp, enum ibscif_qp_state cur_state)
+{
+	if (cur_state == QP_CONNECTED)
+		return;
+
+	qp->loopback = (qp->ibqp.qp_type != IB_QPT_UD) && !scif_loopback && (qp->local_node_id == qp->remote_node_id);
+	qp->conn = NULL;
+
+	qp->state = QP_CONNECTED;
+}
+
+/* Caller must provide proper synchronization. */
+static void ibscif_qp_local_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason)
+{
+	if (qp->state != QP_CONNECTED)
+		return;
+
+	if (reason != IBSCIF_REASON_DISCONNECT)
+		printk(KERN_NOTICE PFX "QP %u sending abnormal disconnect %d\n",
+		       qp->ibqp.qp_num, reason);
+
+	qp->state = QP_DISCONNECT;
+	ibscif_send_disconnect(qp, reason);
+
+	ibscif_flush_wqs(qp);
+
+	ibscif_cm_async_callback(qp->cm_context);
+	qp->cm_context = NULL;
+
+	if (reason != IBSCIF_REASON_DISCONNECT) {
+		qp->state = QP_ERROR;
+		ibscif_qp_event(qp, IB_EVENT_QP_FATAL);
+	} else
+		ibscif_qp_idle(qp);
+}
+
+void ibscif_qp_internal_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason)
+{
+	spin_lock_bh(&qp->lock);
+	ibscif_qp_local_disconnect(qp, reason);
+	spin_unlock_bh(&qp->lock);
+}
+
+void ibscif_qp_remote_disconnect(struct ibscif_qp *qp, enum ibscif_reason reason)
+{
+	if (reason != IBSCIF_REASON_DISCONNECT)
+		printk(KERN_NOTICE PFX "QP %u received abnormal disconnect %d\n",
+		       qp->ibqp.qp_num, reason);
+
+	if (qp->loopback) {
+		/*
+		 * Prevent simultaneous loopback QP disconnect deadlocks.
+		 * This is no worse than dropping a disconnect packet.
+		 */
+		if (!spin_trylock_bh(&qp->lock))
+			return;
+	} else
+		spin_lock_bh(&qp->lock);
+
+	if (qp->state != QP_CONNECTED) {
+		spin_unlock_bh(&qp->lock);
+		return;
+	}
+
+	ibscif_flush_wqs(qp);
+
+	ibscif_cm_async_callback(qp->cm_context);
+	qp->cm_context = NULL;
+
+	if (reason != IBSCIF_REASON_DISCONNECT) {
+		qp->state = QP_ERROR;
+		ibscif_qp_event(qp, IB_EVENT_QP_FATAL);
+	} else
+		qp->state = QP_IDLE;
+
+	spin_unlock_bh(&qp->lock);
+}
+
+#define	MODIFY_ALLOWED					1
+#define	MODIFY_INVALID					0
+#define	VALID_TRANSITION(next_state, modify_allowed)	{ 1, modify_allowed },
+#define	INVAL_TRANSITION(next_state)			{ 0, MODIFY_INVALID },
+#define	START_STATE(current_state)			{
+#define	CEASE_STATE(current_state)			},
+
+static const struct {
+
+	int valid;
+	int modify_allowed;
+
+} qp_transition[NR_QP_STATES][NR_QP_STATES] = {
+
+	START_STATE(QP_IDLE)
+		VALID_TRANSITION( QP_IDLE,	 MODIFY_ALLOWED	)	
+		VALID_TRANSITION( QP_CONNECTED,	 MODIFY_ALLOWED	)
+		INVAL_TRANSITION( QP_DISCONNECT			)
+		VALID_TRANSITION( QP_ERROR,	 MODIFY_INVALID	)
+		VALID_TRANSITION( QP_RESET,	 MODIFY_INVALID	)	
+		VALID_TRANSITION( QP_IGNORE,	 MODIFY_ALLOWED	)	
+	CEASE_STATE(QP_IDLE)
+
+	START_STATE(QP_CONNECTED)
+		INVAL_TRANSITION( QP_IDLE			)
+		VALID_TRANSITION( QP_CONNECTED,	 MODIFY_INVALID	)
+		VALID_TRANSITION( QP_DISCONNECT, MODIFY_INVALID	)
+		VALID_TRANSITION( QP_ERROR,	 MODIFY_INVALID	)
+		VALID_TRANSITION( QP_RESET,	 MODIFY_INVALID	)	
+		VALID_TRANSITION( QP_IGNORE,	 MODIFY_ALLOWED	)	
+	CEASE_STATE(QP_CONNECTED)
+
+	START_STATE(QP_DISCONNECT) /* Automatic transition to IDLE */
+		INVAL_TRANSITION( QP_IDLE			)
+		INVAL_TRANSITION( QP_CONNECTED			)
+		INVAL_TRANSITION( QP_DISCONNECT			)
+		INVAL_TRANSITION( QP_ERROR			)
+		INVAL_TRANSITION( QP_RESET			)	
+		INVAL_TRANSITION( QP_IGNORE			)	
+	CEASE_STATE(QP_DISCONNECT)
+
+	START_STATE(QP_ERROR)
+		VALID_TRANSITION( QP_IDLE,	 MODIFY_INVALID	)
+		INVAL_TRANSITION( QP_CONNECTED			)
+		INVAL_TRANSITION( QP_DISCONNECT			)
+		VALID_TRANSITION( QP_ERROR,	 MODIFY_INVALID	)
+		VALID_TRANSITION( QP_RESET,	 MODIFY_INVALID	)	
+		VALID_TRANSITION( QP_IGNORE,	 MODIFY_ALLOWED	)	
+	CEASE_STATE(QP_ERROR)
+
+	START_STATE(QP_RESET)
+		VALID_TRANSITION( QP_IDLE,	 MODIFY_ALLOWED	)
+		INVAL_TRANSITION( QP_CONNECTED			)
+		INVAL_TRANSITION( QP_DISCONNECT			)
+		VALID_TRANSITION( QP_ERROR,	 MODIFY_INVALID	)
+		VALID_TRANSITION( QP_RESET,	 MODIFY_INVALID	)	
+		VALID_TRANSITION( QP_IGNORE,	 MODIFY_ALLOWED	)	
+	CEASE_STATE(QP_RESET)
+};
+
+int ibscif_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr, int attr_mask, struct ib_udata *udata)
+{
+	struct ibscif_qp *qp = to_qp(ibqp);
+	enum ibscif_qp_state cur_state, new_state;
+	int sq_size, rq_size, max_or, max_ir;
+	int err = -EINVAL;
+
+	/*
+	 * Mutex prevents simultaneous user-mode QP modifies.
+	 */
+	down(&qp->modify_mutex);
+
+	cur_state = qp->state;
+
+	if ((attr_mask & IB_QP_CUR_STATE) && (to_ibscif_qp_state(attr->cur_qp_state) != cur_state))
+		goto out;
+	if ((attr_mask & IB_QP_PORT) && (attr->port_num == 0 || attr->port_num > 1))
+		goto out;
+
+	/* Validate any state transition. */
+	if (attr_mask & IB_QP_STATE) {
+		new_state = to_ibscif_qp_state(attr->qp_state);
+		if (new_state < 0 || new_state >= NR_QP_STATES)
+			goto out;
+
+		if (!qp_transition[cur_state][new_state].valid)
+			goto out;
+	} else
+		new_state = cur_state;
+
+	/* Validate any attribute modify request. */
+	if (attr_mask & (IB_QP_AV		  |
+			 IB_QP_CAP		  |
+			 IB_QP_DEST_QPN		  |
+			 IB_QP_ACCESS_FLAGS	  |
+			 IB_QP_MAX_QP_RD_ATOMIC	  |
+			 IB_QP_MAX_DEST_RD_ATOMIC)) {
+
+		if (!qp_transition[cur_state][new_state].modify_allowed)
+			goto out;
+
+		if ((attr_mask & IB_QP_AV)  && (attr->ah_attr.ah_flags & IB_AH_GRH) && check_grh) {
+			int remote_node_id = IBSCIF_LID_TO_NODE_ID(attr->ah_attr.dlid);
+			struct ibscif_conn *conn;
+			union ib_gid *dgid;
+
+			if (verbose)
+				printk(KERN_INFO PFX "%s: %d-->%d, DGID=%llx:%llx\n",
+					__func__, qp->local_node_id, remote_node_id,
+					__be64_to_cpu(attr->ah_attr.grh.dgid.global.subnet_prefix),
+					__be64_to_cpu(attr->ah_attr.grh.dgid.global.interface_id));
+
+			if (remote_node_id == qp->local_node_id) {
+				dgid = &qp->dev->gid;
+			}
+			else {
+				spin_lock(&qp->lock);
+				conn = ibscif_get_conn(qp->local_node_id, remote_node_id, 0);
+				spin_unlock(&qp->lock);
+				if (!conn) {
+					if (verbose)
+						printk(KERN_INFO PFX "%s: failed to make SCIF connection %d-->%d.\n",
+							__func__, qp->local_node_id, remote_node_id);
+					goto out;
+				}
+				dgid = &conn->remote_gid;
+				ibscif_put_conn(conn);
+			}
+
+			if (verbose)
+				printk(KERN_INFO PFX "%s: local GID[%d]=%llx:%llx\n",
+					__func__, remote_node_id,
+					__be64_to_cpu(dgid->global.subnet_prefix),
+					__be64_to_cpu(dgid->global.interface_id));
+
+			if (memcmp(dgid, &attr->ah_attr.grh.dgid, sizeof(*dgid))) {
+				if (verbose)
+					printk(KERN_INFO PFX "%s: connecting to DGID outside the box is unsupported.\n",
+						__func__);
+				goto out;
+			}
+		}
+
+		if (attr_mask & IB_QP_CAP) {
+			sq_size = attr->cap.max_send_wr;
+			rq_size = attr->cap.max_recv_wr;
+			if ((sq_size > MAX_QP_SIZE) || (rq_size > MAX_QP_SIZE))
+				goto out;
+		} else {
+			sq_size = qp->sq.size;
+			rq_size = qp->rq.size;
+		}
+		if ((sq_size && !qp->ibqp.send_cq) || (rq_size && !qp->ibqp.recv_cq))
+			goto out;
+
+		max_or = (attr_mask & IB_QP_MAX_QP_RD_ATOMIC) ?
+			  attr->max_rd_atomic : qp->max_or;
+		max_ir = (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) ?
+			  attr->max_dest_rd_atomic : qp->iq.size;
+
+		if (rma_threshold<0x7FFFFFFF && max_ir>MAX_IR && max_ir>=qp->sq.size)
+			max_ir -= qp->sq.size;
+
+		if ((max_or > MAX_OR) || (max_ir > MAX_IR))
+			goto out;
+
+		/* Validation successful; resize the QP as needed. */
+		err = ibscif_resize_qp(qp, sq_size, rq_size, max_ir + ((rma_threshold==0x7FFFFFFFF)?0:sq_size));
+		if (err)
+			goto out;
+
+		/* No failure paths below the QP resize. */
+
+		qp->max_or = max_or;
+
+		if (attr_mask & IB_QP_ACCESS_FLAGS)
+			qp->access = attr->qp_access_flags;
+
+		if (attr_mask & IB_QP_DEST_QPN)
+			qp->remote_qpn = attr->dest_qp_num;
+
+		if (attr_mask & IB_QP_AV)
+			qp->remote_node_id = IBSCIF_LID_TO_NODE_ID(attr->ah_attr.dlid);
+	}
+
+	err = 0;
+	if (attr_mask & IB_QP_STATE) {
+
+		/* Perform state change processing. */
+		spin_lock_bh(&qp->lock);
+		switch (new_state) {
+		case QP_IDLE:
+			ibscif_qp_idle(qp);
+			break;
+		case QP_CONNECTED:
+			ibscif_qp_connect(qp, cur_state);
+			break;
+		case QP_DISCONNECT:
+			ibscif_qp_local_disconnect(qp, IBSCIF_REASON_DISCONNECT);
+			break;
+		case QP_ERROR:
+			ibscif_qp_error(qp);
+			break;
+		case QP_RESET:
+			ibscif_qp_reset(qp);
+			break;
+		default:
+			break;
+		}
+		spin_unlock_bh(&qp->lock);
+
+		/* scif_connect() can not be called with spin_lock_bh() held */
+		if (ibqp->qp_type != IB_QPT_UD && new_state == QP_CONNECTED && !qp->loopback) {
+			int flag = (qp->ibqp.qp_num > qp->remote_qpn);
+			spin_lock(&qp->lock);
+			qp->conn = ibscif_get_conn( qp->local_node_id, qp->remote_node_id, flag );
+			spin_unlock(&qp->lock);
+		}
+	}
+
+	__ibscif_query_qp(qp, attr, NULL);
+out:
+	up(&qp->modify_mutex);
+	return err;
+}
+
+void ibscif_complete_qp(struct kref *ref)
+{
+	struct ibscif_qp *qp = container_of(ref, struct ibscif_qp, ref);
+	complete(&qp->done);
+}
+
+int ibscif_destroy_qp(struct ib_qp *ibqp)
+{
+	struct ibscif_qp *qp = to_qp(ibqp);
+	struct ibscif_dev *dev = qp->dev;
+	int i, j;
+	struct ibscif_conn *conn[IBSCIF_MAX_DEVICES];
+
+	if (qp->cm_context) {
+		ibscif_cm_async_callback(qp->cm_context);
+		qp->cm_context = NULL;
+	}
+
+	if (ibqp->qp_num)
+		ibscif_wiremap_del(ibqp->qp_num);
+
+	if (qp->in_scheduler)
+		ibscif_scheduler_remove_qp(qp);
+
+	spin_lock_bh(&qp->lock);
+	if (qp->state == QP_CONNECTED)
+		ibscif_send_disconnect(qp, IBSCIF_REASON_DISCONNECT);
+	spin_unlock_bh(&qp->lock);
+
+	ibscif_put_qp(qp);
+	wait_for_completion(&qp->done);
+
+	ibscif_flush_wqs(qp);
+	ibscif_reset_wqs(qp);
+	ibscif_reset_wire(&qp->wire);
+
+	if (qp->sq.wr)
+		vfree(qp->sq.wr);
+	if (qp->rq.wr)
+		vfree(qp->rq.wr);
+	if (qp->iq.wr)
+		vfree(qp->iq.wr);
+
+	ibscif_release_quota((PAGE_ALIGN(qp->sq.size * qp->sq.wr_size) +
+			     PAGE_ALIGN(qp->rq.size * qp->rq.wr_size) +
+			     PAGE_ALIGN(qp->iq.size * qp->iq.wr_size)) >> PAGE_SHIFT);
+
+	atomic_dec(&dev->qp_cnt);
+
+	ibscif_put_conn(qp->conn);
+
+	if (qp->ibqp.qp_type == IB_QPT_UD) {
+		spin_lock_bh(&qp->lock);
+		for (i=0, j=0; i<IBSCIF_MAX_DEVICES; i++) {
+			if (qp->ud_conn[i]) {
+				conn[j++] = qp->ud_conn[i];
+				qp->ud_conn[i] = NULL;
+			}
+		}
+		spin_unlock_bh(&qp->lock);
+
+		/* ibscif_put_conn() may call scif_unregister(), should not hold a lock */
+		for (i=0; i<j; i++)
+			ibscif_put_conn(conn[i]);
+	}
+
+	kfree(qp);
+	return 0;
+}
+
+void ibscif_qp_add_ud_conn(struct ibscif_qp *qp, struct ibscif_conn *conn)
+{
+	int i;
+
+	if (!qp || !conn)
+		return;
+
+	if (qp->ibqp.qp_type != IB_QPT_UD)
+		return;
+
+	
+	spin_lock_bh(&qp->lock);
+
+	for (i=0; i<IBSCIF_MAX_DEVICES; i++) {
+		if (qp->ud_conn[i] == conn)
+			goto done;
+	}
+
+	for (i=0; i<IBSCIF_MAX_DEVICES; i++) {
+		if (qp->ud_conn[i] == NULL) {
+			atomic_inc(&conn->refcnt);
+			qp->ud_conn[i] = conn;
+			break;
+		}
+	}
+done:
+	spin_unlock_bh(&qp->lock);
+}
+
diff -ruN a/drivers/infiniband/hw/scif/ibscif_scheduler.c b/drivers/infiniband/hw/scif/ibscif_scheduler.c
--- a/drivers/infiniband/hw/scif/ibscif_scheduler.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_scheduler.c	2016-04-14 13:33:08.867411057 -0700
@@ -0,0 +1,195 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+static int ibscif_schedule_tx(struct ibscif_wq *wq, int max_send)
+{
+	struct ibscif_tx_state *tx = &wq->wirestate->tx;
+	struct ibscif_qp *qp = wq->qp;
+	struct ibscif_wr *wr;
+	int index, sent = 0;
+
+	while ((wq->next_wr != wq->tail) && ibscif_tx_window(tx) && max_send) {
+
+		index = wq->next_wr;
+		wr = ibscif_get_wr(wq, index);
+
+		/*
+		 * Ack processing can reschedule a WR that is in retry; only process
+		 * it if we are all caught up.  Also, do not start a fenced WR until
+		 * all prior RDMA read and atomic operations have completed.
+		 */
+		if ((wr->flags & IB_SEND_FENCE) && atomic_read(&qp->or_depth) &&
+		    (wr->state == WR_WAITING))
+			break;
+
+		switch (wr->opcode) {
+		case WR_RDMA_READ:
+		case WR_ATOMIC_CMP_AND_SWP:
+		case WR_ATOMIC_FETCH_AND_ADD:
+			/* Throttle IQ stream requests if needed. */
+			if (wr->state == WR_WAITING) {
+				if (atomic_read(&qp->or_depth) == qp->max_or)
+					return 0;
+				atomic_inc(&qp->or_depth);
+			}
+			/* Fall through. */
+		case WR_SEND:
+		case WR_SEND_WITH_IMM:
+		case WR_RDMA_WRITE:
+		case WR_RDMA_WRITE_WITH_IMM:
+		case WR_RDMA_READ_RSP:
+		case WR_ATOMIC_RSP:
+		case WR_RMA_RSP:
+			sent = ibscif_xmit_wr(wq, wr, min((u32)max_send, ibscif_tx_window(tx)),
+					     0, tx->next_seq, &tx->next_seq);
+			break;
+		case WR_UD:
+			sent = ibscif_xmit_wr(wq, wr, min((u32)max_send, ibscif_tx_window(tx)),
+					     0, 0, NULL);
+			break;
+		default:
+			printk(KERN_ERR PFX "%s() botch: found opcode %d on work queue\n",
+			       __func__, wr->opcode);
+			return -EOPNOTSUPP;
+		}
+
+		/* If an IQ stream request did not get started we need to back off or_depth. */
+		if ((wr->state == WR_WAITING) &&
+		    ((wr->opcode == WR_RDMA_READ) ||
+		     (wr->opcode == WR_ATOMIC_CMP_AND_SWP) || (wr->opcode == WR_ATOMIC_FETCH_AND_ADD)))
+			atomic_dec(&qp->or_depth);
+
+		if (sent < 0)
+			return sent;
+
+		 max_send -= sent;
+
+		/*
+		 * The tx engine bumps next_wr when finished sending a whole WR.
+		 * Bail if it didn't this time around.
+		 */
+		if (wq->next_wr == index)
+			break;
+	}
+
+	return 0;
+}
+
+static int ibscif_schedule_wq(struct ibscif_wq *wq)
+{
+	int max_send, err = 0;
+	int need_call_sq_completions = 0;
+
+	/* Ignore loopback QPs that may be scheduled by retry processing. */
+	if (wq->qp->loopback)
+		return 0;
+
+	if (!(max_send = atomic_read(&wq->qp->dev->available)))
+		return -EBUSY;
+
+	spin_lock(&wq->lock);
+	err = ibscif_schedule_tx(wq, max_send);
+	need_call_sq_completions = wq->fast_rdma_completions;
+	wq->fast_rdma_completions = 0;
+	spin_unlock(&wq->lock);
+
+	if (unlikely(err))
+		ibscif_qp_internal_disconnect(wq->qp, IBSCIF_REASON_QP_FATAL);
+
+	if (fast_rdma && need_call_sq_completions) 
+		ibscif_process_sq_completions(wq->qp);
+		
+	return err;
+}
+
+void ibscif_schedule(struct ibscif_wq *wq)
+{
+	struct ibscif_dev *dev;
+	struct list_head processed;
+
+	if (wq->qp->loopback) {
+		ibscif_loopback(wq);
+		return;
+	}
+	dev = wq->qp->dev;
+
+	if (!ibscif_schedule_wq(wq))
+		goto out;
+
+	while (atomic_xchg(&dev->was_new, 0)) {
+		/* Bail if the device is busy. */
+		if (down_trylock(&dev->mutex))
+			goto out;
+
+		/*
+		 * Schedule each WQ on the device and move it to the processed list.
+		 * When complete, append the processed list to the device WQ list.
+		 */
+		INIT_LIST_HEAD(&processed);
+		while (!list_empty(&dev->wq_list)) {
+			wq = list_entry(dev->wq_list.next, typeof(*wq), entry);
+			if (!ibscif_schedule_wq(wq)) {
+				DEV_STAT(dev, sched_exhaust++);
+				list_splice(&processed, dev->wq_list.prev);
+				up(&dev->mutex);
+				goto out;
+			}
+			list_move_tail(&wq->entry, &processed);
+		}
+		list_splice(&processed, dev->wq_list.prev);
+
+		up(&dev->mutex);
+	}
+	return;
+out:
+	atomic_inc(&dev->was_new);
+}
+
+void ibscif_scheduler_add_qp(struct ibscif_qp *qp)
+{
+	struct ibscif_dev *dev = qp->dev;
+
+	down(&dev->mutex);
+	list_add_tail(&qp->sq.entry, &dev->wq_list);
+	list_add_tail(&qp->iq.entry, &dev->wq_list);
+	up(&dev->mutex);
+}
+
+void ibscif_scheduler_remove_qp(struct ibscif_qp *qp)
+{
+	struct ibscif_dev *dev = qp->dev;
+
+	down(&dev->mutex);
+	list_del(&qp->sq.entry);
+	list_del(&qp->iq.entry);
+	up(&dev->mutex);
+}
diff -ruN a/drivers/infiniband/hw/scif/ibscif_util.c b/drivers/infiniband/hw/scif/ibscif_util.c
--- a/drivers/infiniband/hw/scif/ibscif_util.c	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/ibscif_util.c	2016-04-14 13:33:08.867411057 -0700
@@ -0,0 +1,623 @@
+/*
+ * Copyright (c) 2008 Intel Corporation.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the
+ * GNU General Public License (GPL) Version 2, available from the
+ * file COPYING in the main directory of this source tree, or the
+ * OpenFabrics.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above copyright
+ *        notice, this list of conditions and the following disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "ibscif_driver.h"
+
+#define IBSCIF_CONN_IDLE 0
+#define IBSCIF_CONN_REQ_SENT 1
+#define IBSCIF_CONN_REQ_RCVD 2
+#define IBSCIF_CONN_ESTABLISHED 3
+#define IBSCIF_CONN_ACTIVE 4
+
+DEFINE_SPINLOCK(conn_state_lock);
+static int conn_state[IBSCIF_MAX_DEVICES][IBSCIF_MAX_DEVICES];
+
+#define IBSCIF_CONN_REP 1
+#define IBSCIF_CONN_REJ 2
+#define IBSCIF_CONN_ERR 3
+
+struct ibscif_conn_resp {
+	int cmd;
+	union ib_gid gid;
+};
+
+void ibscif_do_accept(struct ibscif_dev *dev)
+{
+	struct scif_portID peer;
+	scif_epd_t ep;
+	struct ibscif_conn *conn;
+	int ret;
+	struct ibscif_conn_resp resp;
+	int resp_size;
+
+	if (check_grh)
+		resp_size = sizeof(resp);
+	else
+		resp_size = sizeof(int);
+
+	ret = scif_accept(dev->listen_ep, &peer, &ep, SCIF_ACCEPT_SYNC);
+	if (ret) {
+		printk(KERN_ALERT PFX "%s: scif_accept returns %ld\n", __func__, PTR_ERR(ep));
+		return;
+	}
+
+	if (verbose)
+		printk(KERN_INFO PFX "%s: %d<--%d\n", __func__, dev->node_id, peer.node);
+
+	if (check_grh)
+		memcpy(&resp.gid, &dev->gid, sizeof(resp.gid));
+
+	spin_lock(&conn_state_lock);
+	switch (conn_state[dev->node_id][peer.node]) {
+	  case IBSCIF_CONN_IDLE:
+		conn_state[dev->node_id][peer.node] = IBSCIF_CONN_REQ_RCVD;
+		resp.cmd = IBSCIF_CONN_REP;
+		if (verbose)
+			printk(KERN_INFO PFX "%s: no double connection, accepting\n", __func__);
+		break;
+
+	  case IBSCIF_CONN_REQ_SENT:
+		/* A connection request has been sent, but no response yet. Node id is used to
+ 		 * break the tie when both side send the connection request. One side is allowed
+ 		 * to accept the request and its own request will be rejected by the peer.
+ 		 */
+		if (dev->node_id > peer.node) {
+			resp.cmd = IBSCIF_CONN_REJ;
+			if (verbose)
+				printk(KERN_INFO PFX "%s: double connection, rejecting (peer will accept)\n", __func__);
+		}
+		else if (dev->node_id == peer.node) {
+			conn_state[dev->node_id][peer.node] = IBSCIF_CONN_REQ_RCVD;
+			resp.cmd = IBSCIF_CONN_REP;
+			if (verbose)
+				printk(KERN_INFO PFX "%s: loopback connection, accepting\n", __func__);
+		}
+		else {
+			conn_state[dev->node_id][peer.node] = IBSCIF_CONN_REQ_RCVD;
+			resp.cmd = IBSCIF_CONN_REP;
+			if (verbose)
+				printk(KERN_INFO PFX "%s: double connection, accepting (peer will reject)\n", __func__);
+		}
+		break;
+
+	  case IBSCIF_CONN_REQ_RCVD:
+		if (verbose)
+			printk(KERN_INFO PFX "%s: duplicated connection request, rejecting\n", __func__);
+		resp.cmd = IBSCIF_CONN_REJ;
+		break;
+
+	  case IBSCIF_CONN_ESTABLISHED:
+	  case IBSCIF_CONN_ACTIVE:
+		if (verbose)
+			printk(KERN_INFO PFX "%s: already connected, rejecting\n", __func__);
+		resp.cmd = IBSCIF_CONN_REJ;
+		break;
+
+	  default:
+		if (verbose)
+			printk(KERN_INFO PFX "%s: invalid state: %d\n", __func__, conn_state[dev->node_id][peer.node]);
+		resp.cmd = IBSCIF_CONN_ERR;
+		break;
+	}
+	spin_unlock(&conn_state_lock);
+
+	ret = scif_send(ep, &resp, resp_size, SCIF_SEND_BLOCK);
+	if (ret < 0) {
+		printk(KERN_ALERT PFX "%s: scif_send returns %d\n", __func__, ret);
+		scif_close(ep);
+		return;
+	}
+
+	if (resp.cmd != IBSCIF_CONN_REP) {
+		/* one additional hand shaking to prevent the previous send from being trashed by ep closing */
+		scif_recv(ep, &resp, resp_size, SCIF_RECV_BLOCK);
+		scif_close(ep);
+		return;
+	}
+
+	if (check_grh) {
+		ret = scif_recv(ep, &resp, resp_size, SCIF_RECV_BLOCK);
+		if (ret < 0) {
+			printk(KERN_ALERT PFX "%s: scif_recv returns %d\n", __func__, ret);
+			scif_close(ep);
+			spin_lock(&conn_state_lock);
+			conn_state[dev->node_id][peer.node] = IBSCIF_CONN_IDLE;
+			spin_unlock(&conn_state_lock);
+			return;
+		}
+	}
+
+	conn = kzalloc(sizeof (*conn), GFP_KERNEL);
+	if (!conn) {
+		printk(KERN_ALERT PFX "%s: cannot allocate connection context.\n", __func__);
+		scif_close(ep);
+		spin_lock(&conn_state_lock);
+		conn_state[dev->node_id][peer.node] = IBSCIF_CONN_IDLE;
+		spin_unlock(&conn_state_lock);
+		return;
+	}
+
+	conn->ep = ep;
+	conn->remote_node_id = peer.node;
+	if (check_grh)
+		memcpy(&conn->remote_gid, &resp.gid, sizeof(conn->remote_gid));
+	conn->dev = dev;
+	atomic_set(&conn->refcnt, 0);
+
+	spin_lock(&conn_state_lock);
+	conn_state[dev->node_id][peer.node] = IBSCIF_CONN_ESTABLISHED;
+	spin_unlock(&conn_state_lock);
+
+	if (verbose)
+		printk(KERN_INFO PFX "%s: connection established. ep=%p\n", __func__, ep);
+
+	ibscif_refresh_mreg(conn);
+
+	/* one addition sync to ensure the MRs are registered with the new ep at both side */
+	scif_send(ep, &resp, resp_size, SCIF_SEND_BLOCK);
+	scif_recv(ep, &resp, resp_size, SCIF_RECV_BLOCK);
+
+	list_add(&conn->entry, &dev->conn_list); 
+	ibscif_refresh_pollep_list();
+
+	spin_lock(&conn_state_lock);
+	conn_state[dev->node_id][peer.node] = IBSCIF_CONN_ACTIVE;
+	spin_unlock(&conn_state_lock);
+}
+
+struct ibscif_conn *ibscif_do_connect(struct ibscif_dev *dev, int remote_node_id)
+{
+	struct scif_portID dest;
+	struct ibscif_conn *conn = NULL;
+	int ret;
+	scif_epd_t ep;
+	struct ibscif_conn_resp resp;
+	union ib_gid peer_gid;
+	int resp_size;
+
+	if (check_grh)
+		resp_size = sizeof(resp);
+	else
+		resp_size = sizeof(int);
+
+	if (verbose)
+		printk(KERN_INFO PFX "%s: %d-->%d\n", __func__, dev->node_id, remote_node_id);
+
+	/* Validate remote_node_id for conn_state array check */
+	if ((remote_node_id < 0) || (remote_node_id >= IBSCIF_MAX_DEVICES))
+		return ERR_PTR(-EINVAL);
+
+	spin_lock(&conn_state_lock);
+	if (conn_state[dev->node_id][remote_node_id] != IBSCIF_CONN_IDLE) {
+		spin_unlock(&conn_state_lock);
+		if (verbose)
+			printk(KERN_INFO PFX "%s: connection already in progress, retry\n", __func__);
+		return ERR_PTR(-EAGAIN);
+	}
+	conn_state[dev->node_id][remote_node_id] = IBSCIF_CONN_REQ_SENT;
+	spin_unlock(&conn_state_lock);
+
+	ep = scif_open();
+	if (!ep) /* SCIF API semantics */
+		goto out_state; 
+
+	if (IS_ERR(ep)) /* SCIF emulator semantics */
+		goto out_state;
+
+	dest.node = remote_node_id;
+	dest.port = SCIF_OFED_PORT_0;
+
+	ret = scif_connect(ep, &dest);
+	if (ret < 0) 
+		goto out_close;
+
+	/* Now ret is the port number ep is bound to */
+
+	ret = scif_recv(ep, &resp, resp_size, SCIF_RECV_BLOCK);
+	if (ret < 0) {
+		printk(KERN_ALERT PFX "%s: scif_recv returns %d\n", __func__, ret);
+		goto out_close;
+	}
+
+	if (resp.cmd != IBSCIF_CONN_REP) {
+		scif_send(ep, &resp, resp_size, SCIF_SEND_BLOCK);
+		/* the peer has issued the connection request */
+		if (resp.cmd == IBSCIF_CONN_REJ) {
+			if (verbose)
+				printk(KERN_INFO PFX "%s: rejected by peer due to double connection\n", __func__);
+			scif_close(ep);
+			/* don't reset the state becasue it's used for checking connection state */
+			return ERR_PTR(-EAGAIN);
+		}
+		else {
+			if (verbose)
+				printk(KERN_INFO PFX "%s: rejected by peer due to invalid state\n", __func__);
+			goto out_close;
+		}
+	}
+
+	if (check_grh) {
+		memcpy(&peer_gid, &resp.gid, sizeof(peer_gid));
+		memcpy(&resp.gid, &dev->gid, sizeof(resp.gid));
+		ret = scif_send(ep, &resp, resp_size, SCIF_SEND_BLOCK);
+		if (ret < 0) {
+			printk(KERN_ALERT PFX "%s: scif_send returns %d\n", __func__, ret);
+			goto out_close;
+		}
+	}
+
+	if (verbose)
+		printk(KERN_INFO PFX "%s: connection established. ep=%p\n", __func__, ep);
+
+	spin_lock(&conn_state_lock);
+	conn_state[dev->node_id][remote_node_id] = IBSCIF_CONN_ESTABLISHED;
+	spin_unlock(&conn_state_lock);
+
+	conn = kzalloc(sizeof *conn, GFP_KERNEL);
+	if (!conn) {
+		printk(KERN_ALERT PFX "%s: failed to allocate connection object\n", __func__);
+		goto out_close;
+	}
+
+	conn->ep = ep;
+	conn->remote_node_id = remote_node_id;
+	if (check_grh)
+		memcpy(&conn->remote_gid, &peer_gid, sizeof(conn->remote_gid));
+	conn->dev = dev;
+	atomic_set(&conn->refcnt, 0);
+
+	ibscif_refresh_mreg(conn);
+
+	/* one addition sync to ensure the MRs are registered with the new ep at both side */
+	scif_send(ep, &resp, resp_size, SCIF_SEND_BLOCK);
+	scif_recv(ep, &resp, resp_size, SCIF_RECV_BLOCK);
+
+	list_add_tail(&conn->entry, &dev->conn_list);
+	ibscif_refresh_pollep_list();
+
+	spin_lock(&conn_state_lock);
+	conn_state[dev->node_id][remote_node_id] = IBSCIF_CONN_ACTIVE;
+	spin_unlock(&conn_state_lock);
+
+	return conn;
+
+out_close:
+	scif_close(ep);
+
+out_state:
+	spin_lock(&conn_state_lock);
+	if (conn_state[dev->node_id][remote_node_id] == IBSCIF_CONN_REQ_SENT)
+		conn_state[dev->node_id][remote_node_id] = IBSCIF_CONN_IDLE;
+	spin_unlock(&conn_state_lock);
+	return conn;
+}
+
+struct ibscif_conn *ibscif_get_conn(int node_id, int remote_node_id, int find_local_peer)
+{
+	struct ibscif_dev *cur, *next, *dev = NULL;
+	struct ibscif_conn *conn, *conn1, *conn2;
+	int done=0, err=0, connect_tried=0;
+
+	down(&devlist_mutex);
+	list_for_each_entry_safe(cur, next, &devlist, entry) {
+		if (cur->node_id == node_id) {
+			dev = cur;
+			break;
+		}
+	}
+	up(&devlist_mutex);
+
+	if (!dev)
+		return NULL;
+
+again:
+	conn1 = NULL;
+	conn2 = NULL;
+	down(&dev->mutex);
+	list_for_each_entry(conn, &dev->conn_list, entry)
+	{
+		if (conn->remote_node_id == remote_node_id) {
+			if (node_id == remote_node_id) {
+				if (!conn1) {
+					conn1 = conn;
+					continue;
+				}
+				else {
+					conn2 = conn;
+					break;
+				}
+			}
+			up(&dev->mutex);
+			atomic_inc(&conn->refcnt);
+			if (conn->local_close) {
+				conn->local_close = 0;
+				ibscif_send_reopen(conn);
+			}
+			return conn;
+		}
+	}
+	up(&dev->mutex);
+
+	/* for loopback connections, we must wait for both endpoints be in the list to ensure that
+ 	 * different endpoints are assigned to the two sides
+ 	 */
+	if (node_id == remote_node_id) {
+		if (conn1 && conn2) {
+			conn = find_local_peer ? conn2 : conn1;
+			atomic_inc(&conn->refcnt);
+			if (conn->local_close) {
+				conn->local_close = 0;
+				ibscif_send_reopen(conn);
+			}
+			return conn;
+		}
+		else if (conn1) {
+			schedule();
+			goto again;
+		}
+	}
+
+	if (connect_tried) {
+		printk(KERN_ALERT PFX "%s: ERROR: cannot get connection (%d-->%d) after waiting, state=%d\n", 
+				__func__, dev->node_id, remote_node_id, err-1);
+		return NULL;
+	}
+
+	conn = ibscif_do_connect(dev, remote_node_id);
+
+	/* If a connection is in progress, wait for its finish */
+	if (conn == ERR_PTR(-EAGAIN)) {
+	    while (!done && !err) {
+		spin_lock(&conn_state_lock);
+		switch (conn_state[node_id][remote_node_id]) {
+		  case IBSCIF_CONN_REQ_SENT:
+		  case IBSCIF_CONN_REQ_RCVD:
+		  case IBSCIF_CONN_ESTABLISHED:
+			break;
+		  case IBSCIF_CONN_ACTIVE:
+			done = 1;
+			break;
+		  default:
+			err = 1 + conn_state[node_id][remote_node_id];
+			break;
+		}
+		spin_unlock(&conn_state_lock);
+		schedule();
+	    }
+	}
+
+	connect_tried = 1;
+	goto again;
+}
+
+void ibscif_put_conn(struct ibscif_conn *conn)
+{
+	if (!conn)
+		return;
+
+	if (atomic_dec_and_test(&conn->refcnt)) {
+		// printk(KERN_INFO PFX "%s: local_close, conn=%p, remote_close=%d\n", __func__, conn, conn->remote_close);
+		ibscif_send_close(conn);
+		conn->local_close = 1;
+	}
+}
+
+void ibscif_get_pollep_list(struct scif_pollepd *polleps,
+			  struct ibscif_dev **devs, int *types, struct ibscif_conn **conns, int *count)
+{
+	struct ibscif_dev *dev;
+	struct ibscif_conn *conn;
+	int i = 0;
+	int max = *count;
+
+	down(&devlist_mutex);
+	list_for_each_entry(dev, &devlist, entry) {
+		if (i >= max)
+			break;
+
+		polleps[i].epd = dev->listen_ep;
+		polleps[i].events = POLLIN;
+		polleps[i].revents = 0;
+		devs[i] = dev;
+		types[i] = IBSCIF_EP_TYPE_LISTEN; 
+		conns[i] = NULL;
+		i++;
+		if (verbose)
+			printk(KERN_INFO PFX "%s: ep=%p (%d:listen)\n", __func__, dev->listen_ep, dev->node_id);
+
+		down(&dev->mutex);
+		list_for_each_entry(conn, &dev->conn_list, entry)
+		{
+			if (i >= max)
+				break;
+			polleps[i].epd = conn->ep;
+			polleps[i].events = POLLIN;
+			polleps[i].revents = 0;
+			devs[i] = dev;
+			types[i] = IBSCIF_EP_TYPE_COMM; 
+			conns[i] = conn;
+			i++;
+			if (verbose)
+				printk(KERN_INFO PFX "%s: ep=%p (%d<--->%d)\n", __func__, conn->ep, dev->node_id, conn->remote_node_id);
+		}
+		up(&dev->mutex);
+	}
+	up(&devlist_mutex);
+
+	if (verbose)
+		printk(KERN_INFO PFX "%s: count=%d\n", __func__, i);
+	*count = i;
+}
+
+void ibscif_get_ep_list(scif_epd_t *eps, int *count)
+{
+	struct ibscif_dev *dev;
+	struct ibscif_conn *conn;
+	int i = 0;
+	int max = *count;
+
+	down(&devlist_mutex);
+	list_for_each_entry(dev, &devlist, entry) {
+		if (i >= max)
+			break;
+
+		down(&dev->mutex);
+		list_for_each_entry(conn, &dev->conn_list, entry)
+		{
+			if (i >= max)
+				break;
+			eps[i] = conn->ep;
+			i++;
+		}
+		up(&dev->mutex);
+	}
+	up(&devlist_mutex);
+
+	*count = i;
+}
+
+void ibscif_remove_ep(struct ibscif_dev *dev, scif_epd_t ep)
+{
+	struct ibscif_conn *conn, *next;
+	down(&dev->mutex);
+	list_for_each_entry_safe(conn, next, &dev->conn_list, entry)
+	{
+		if (conn->ep == ep) {
+			spin_lock(&conn_state_lock);
+			conn_state[conn->dev->node_id][conn->remote_node_id] = IBSCIF_CONN_IDLE;
+			spin_unlock(&conn_state_lock);
+			list_del(&conn->entry);
+		}
+	}
+	up(&dev->mutex);
+}
+
+
+void ibscif_free_conn(struct ibscif_conn *conn)
+{
+	scif_close(conn->ep);
+	kfree(conn);
+}
+
+int ibscif_cleanup_idle_conn(void)
+{
+	struct ibscif_dev *dev;
+	struct ibscif_conn *conn, *next;
+	struct ibscif_conn *idle_conns[IBSCIF_MAX_DEVICES];
+	int i, n=0;
+
+	down(&devlist_mutex);
+	list_for_each_entry(dev, &devlist, entry) {
+		down(&dev->mutex);
+		list_for_each_entry_safe(conn, next, &dev->conn_list, entry)
+		{
+			if (conn->local_close && conn->remote_close) {
+				spin_lock(&conn_state_lock);
+				conn_state[conn->dev->node_id][conn->remote_node_id] = IBSCIF_CONN_IDLE;
+				spin_unlock(&conn_state_lock);
+				list_del(&conn->entry);
+				idle_conns[n++] = conn;
+			}
+		}
+		up(&dev->mutex);
+	}
+	up(&devlist_mutex);
+
+	for (i=0; i<n; i++)
+		ibscif_free_conn(idle_conns[i]);
+
+	if (n && verbose)
+		printk(KERN_ALERT PFX "%s: n=%d\n", __func__, n);
+
+	return n;
+}
+
+/*
+ * Simple routines to support performance profiling
+ */
+
+#include <linux/time.h>
+
+static uint32_t ibscif_time_passed(void)
+{
+	static int first = 1;
+	static struct timeval t0;
+	static struct timeval t;
+	uint32_t usec;
+	
+	if (first) {
+		do_gettimeofday(&t0);
+		first = 0;
+		return 0;
+	}
+
+	do_gettimeofday(&t);
+	usec = (t.tv_sec - t0.tv_sec) * 1000000UL;
+	if (t.tv_usec >= t0.tv_usec) 
+		usec += (t.tv_usec - t0.tv_usec);
+	else
+		usec -= (t0.tv_usec - t.tv_usec);
+
+	t0 = t;
+	return usec;
+}
+
+#define IBSCIF_PERF_MAX_SAMPLES		100
+#define IBSCIF_PERF_MAX_COUNTERS	10
+
+void ibscif_perf_sample(int counter, int next)
+{
+	static uint32_t T[IBSCIF_PERF_MAX_SAMPLES][IBSCIF_PERF_MAX_COUNTERS];
+	static int T_idx=0;
+	int i, j, sum;
+
+	if (counter>=0 && counter<IBSCIF_PERF_MAX_COUNTERS)
+		T[T_idx][counter] = ibscif_time_passed();
+
+	if (next) {
+		if (++T_idx < IBSCIF_PERF_MAX_SAMPLES)
+			return;
+
+		T_idx = 0;
+
+		/* batch output to minimize the impact on higher level timing */
+		for (i=0; i<IBSCIF_PERF_MAX_SAMPLES; i++) {
+			sum = 0;
+			printk(KERN_INFO PFX "%d: ", i);
+			for (j=0; j<IBSCIF_PERF_MAX_COUNTERS; j++) {
+				printk("T%d=%u ", j, T[i][j]);
+				if (j>0)
+					sum += T[i][j];
+			}
+			printk("SUM(T1..T%d)=%u\n", IBSCIF_PERF_MAX_COUNTERS-1, sum);
+		}
+	}
+}
+
diff -ruN a/drivers/infiniband/hw/scif/Kconfig b/drivers/infiniband/hw/scif/Kconfig
--- a/drivers/infiniband/hw/scif/Kconfig	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/Kconfig	2016-04-14 13:33:08.868411034 -0700
@@ -0,0 +1,4 @@
+config INFINIBAND_SCIF
+	tristate "SCIF RDMA driver support"
+	---help---
+	  RDMA over SCIF driver.
diff -ruN a/drivers/infiniband/hw/scif/Makefile b/drivers/infiniband/hw/scif/Makefile
--- a/drivers/infiniband/hw/scif/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ b/drivers/infiniband/hw/scif/Makefile	2016-04-14 13:33:08.868411034 -0700
@@ -0,0 +1,39 @@
+KERNEL_V := $(shell uname -r)
+
+KDIR ?= /lib/modules/$(KERNEL_V)/build
+
+SCIF_INCL := /usr/src/kernels/$(KERNEL_V)/include/modules/
+SCIF_SYMS := /lib/modules/$(KERNEL_V)/scif.symvers
+
+ccflags-y += -I$(SCIF_INCL)
+EXTRA_LDFLAGS += -L$(SCIF_SYMS)
+
+obj-$(CONFIG_INFINIBAND_SCIF) += ibscif.o
+
+ibscif-y :=	ibscif_main.o		\
+		ibscif_ah.o		\
+		ibscif_pd.o		\
+		ibscif_cq.o		\
+		ibscif_qp.o		\
+		ibscif_mr.o		\
+		ibscif_cm.o		\
+		ibscif_post.o		\
+		ibscif_procfs.o		\
+		ibscif_loopback.o	\
+		ibscif_provider.o	\
+		ibscif_protocol.o	\
+		ibscif_scheduler.o	\
+		ibscif_util.o
+
+default:
+	$(MAKE) -C $(KDIR) M=`pwd`
+
+modules_install:
+	$(MAKE) -C $(KDIR) M=`pwd` modules_install
+
+clean:
+	rm -rf *.ko *.o .*.ko.cmd .*.o.cmd *.mod.c Module.* modules.order .tmp_versions
+
+unix:
+	dos2unix *.[ch] Kconfig Makefile
+
